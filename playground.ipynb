{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "colored-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "data_path = '../datasets/'\n",
    "\n",
    "from models import ConvBlock, Flatten, LeNet, MnistConvNet, RandomGame, RandomGamePos\n",
    "# from backpack import backpack, extend\n",
    "# from backpack.extensions import (\n",
    "#     GGNMP,\n",
    "#     HMP,\n",
    "#     KFAC,\n",
    "#     KFLR,\n",
    "#     KFRA,\n",
    "#     PCHMP,\n",
    "#     BatchGrad,\n",
    "#     BatchL2Grad,\n",
    "#     DiagGGNExact,\n",
    "#     DiagGGNMC,\n",
    "#     DiagHessian,\n",
    "#     SumGradSquared,\n",
    "#     Variance,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regional-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCDataset(Dataset):\n",
    "    \n",
    "    \"\"\"MNIST-C dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root, train, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            self.data = torch.tensor(np.load(root + '/train_images.npy'))\n",
    "            self.targets = torch.tensor(np.load(root + '/train_labels.npy'))\n",
    "        else:\n",
    "            self.data = torch.tensor(np.load(root + '/test_images.npy'))\n",
    "            self.targets = torch.tensor(np.load(root + '/test_labels.npy'))\n",
    "            \n",
    "        self.train=train\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index][:,:,0], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = ([transforms.ToTensor()])\n",
    "trans = transforms.Compose(trans)\n",
    "\n",
    "fulltrainset = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=trans)\n",
    "train_set, valset = torch.utils.data.random_split(fulltrainset, [55000, 5000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "validloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=False,\n",
    "                                          num_workers=4, pin_memory=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=trans)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "nb_classes = 10\n",
    "\n",
    "rand_labels_raw = torch.randint(0, 10, fulltrainset.targets.shape)\n",
    "rand_labels_train, rand_labels_val = torch.utils.data.random_split(rand_labels_raw, [55000, 5000])\n",
    "\n",
    "rand_labels_train = torch.utils.data.DataLoader(rand_labels_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "rand_labels_val = torch.utils.data.DataLoader(rand_labels_val, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_c_bright_trainset = MNISTCDataset(root='../datasets/mnist_c/fog', train=False, transform=trans)\n",
    "mnist_c_bright_trainloader = torch.utils.data.DataLoader(mnist_c_bright_trainset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "talented-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(mnist_c_bright_trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smooth-disaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1ef3ee9a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_batch[0][0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-nomination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "apparent-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, save=False, epoch=0, bn_eval=True):\n",
    "    \n",
    "    if bn_eval: # forward prop data twice to update BN running averages\n",
    "        model.train()\n",
    "        for _ in range(2):\n",
    "            for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                _ = (model(inputs))\n",
    "\n",
    "    model.eval()\n",
    "    correct, total, total_loss = 0,0,0\n",
    "    tot_iters = len(loader)\n",
    "#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\n",
    "#         inputs, targets = next(iter(loader)) \n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = (model(inputs, True))\n",
    "\n",
    "            _, predicted = torch.max(nn.Softmax(dim=1)(outputs).data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += torch.sum(predicted.eq(targets.data)).cpu()\n",
    "            total_loss += model.loss_fn(outputs, targets)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*float(correct)/float(total)\n",
    "    loss = total_loss/tot_iters\n",
    "#     if save and acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         print('Saving best model..')\n",
    "#         state = {\n",
    "#             'model': model0,\n",
    "#             'epoch': epoch\n",
    "#         }\n",
    "#         with open(args.save_dir + '/best_model.pt', 'wb') as f:\n",
    "#             torch.save(state, f)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bound-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGame(nn.Module):\n",
    "    def __init__(self, model_func, num_class, multi_rand_layer=[], loss_type = 'softmax'):\n",
    "        super(RandomGame, self).__init__()\n",
    "        self.feature    = model_func()\n",
    "        \n",
    "        self.true_classifier = nn.Linear(self.feature.final_feat_dim, num_class, bias=True)\n",
    "\n",
    "        self.multi_rand_layer = multi_rand_layer\n",
    "        rand_trunk = []\n",
    "        if len(multi_rand_layer) == 0:\n",
    "            rand_trunk.append(nn.Linear(self.feature.final_feat_dim, num_class, bias=True))\n",
    "        else:\n",
    "            prev_layer_dim=self.feature.final_feat_dim\n",
    "            for i in self.multi_rand_layer:\n",
    "                rand_trunk.append(nn.Linear(prev_layer_dim, i, bias = True))\n",
    "                rand_trunk.append(nn.ReLU(inplace=True))\n",
    "                prev_layer_dim = i\n",
    "            rand_trunk.append(nn.Linear(prev_layer_dim, num_class, bias = True))\n",
    "            \n",
    "        self.rand_classifier = nn.Sequential(*rand_trunk)\n",
    "        \n",
    "        self.num_class = num_class\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, true_flag):\n",
    "        embedding  = self.feature.forward(x)\n",
    "        \n",
    "        if true_flag:\n",
    "            out = self.true_classifier.forward(embedding)\n",
    "        else:\n",
    "            out = self.rand_classifier.forward(embedding)\n",
    "        return out, embedding\n",
    "    \n",
    "    def train_rand(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    def train_true(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def train_true_classifier(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'true' not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    def train_encoder(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "portable-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomGame(model_func=MnistConvNet, num_class = 10)#, multi_rand_layer=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "settled-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "brutal-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = model(test_batch[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "periodic-maria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "urban-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_weights=[]\n",
    "for name, param in model.named_parameters():\n",
    "    if 'true' in name:\n",
    "        cur_weights.append(param.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "criminal-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature.fc1.weight\n",
      "torch.Size([300, 784])\n",
      "feature.fc1.bias\n",
      "torch.Size([300])\n",
      "feature.fc2.weight\n",
      "torch.Size([100, 300])\n",
      "feature.fc2.bias\n",
      "torch.Size([100])\n",
      "true_classifier.weight\n",
      "torch.Size([10, 100])\n",
      "true_classifier.bias\n",
      "torch.Size([10])\n",
      "rand_classifier.log_weight\n",
      "torch.Size([10, 100])\n",
      "tensor([[0.9383, 1.2137, 1.0354, 0.8536, 0.8437, 0.9379, 1.0042, 0.9595, 1.0116,\n",
      "         0.9611, 0.9303, 0.9120, 1.1674, 1.2485, 1.0649, 1.0517, 1.0865, 1.1391,\n",
      "         0.9346, 0.7919, 0.9608, 1.0218, 1.0246, 0.9564, 1.0333, 1.0969, 1.2617,\n",
      "         1.0815, 0.9507, 0.9167, 1.0379, 1.2106, 0.9011, 0.9173, 1.2503, 1.0218,\n",
      "         1.1057, 0.8229, 0.8413, 0.8407, 1.1059, 0.8897, 0.9460, 1.1984, 0.9072,\n",
      "         0.8324, 0.8737, 0.8006, 1.1244, 0.8725, 1.2410, 1.0298, 0.9960, 1.0302,\n",
      "         0.9734, 0.9821, 1.2270, 0.9615, 1.1599, 0.8679, 1.0722, 0.8294, 0.8655,\n",
      "         0.9241, 1.1052, 0.9204, 0.9277, 1.1525, 1.2116, 0.9156, 0.8224, 1.1718,\n",
      "         0.8743, 1.0451, 0.9725, 1.0436, 1.0082, 1.2123, 0.8932, 1.0935, 0.8077,\n",
      "         0.7927, 1.1459, 0.8938, 1.2124, 1.2310, 1.1762, 1.1168, 0.8552, 0.9893,\n",
      "         0.8836, 1.0165, 0.8516, 1.0871, 0.9602, 1.1089, 1.0330, 0.8018, 0.8795,\n",
      "         1.1125],\n",
      "        [1.2249, 1.1795, 1.1207, 1.2627, 1.0661, 0.7955, 1.1526, 0.8941, 1.0729,\n",
      "         0.9372, 0.8093, 0.8711, 1.0258, 0.8596, 1.0609, 0.7933, 0.9849, 0.8550,\n",
      "         1.1640, 1.2180, 0.9382, 1.1235, 1.0214, 1.0140, 1.0706, 1.0450, 0.9758,\n",
      "         1.0016, 1.0725, 1.1218, 0.9603, 1.2020, 0.8831, 1.2209, 1.1501, 1.0039,\n",
      "         0.9019, 0.8868, 1.1477, 0.9471, 0.8636, 0.9571, 1.1527, 0.9456, 1.1748,\n",
      "         1.2477, 1.0270, 1.2017, 0.9807, 1.1851, 0.8994, 0.9190, 0.8942, 0.8439,\n",
      "         0.9108, 1.0027, 0.9128, 1.2018, 1.0483, 0.8374, 0.8141, 1.2502, 0.7947,\n",
      "         1.1431, 1.0899, 1.2159, 0.8011, 1.0254, 1.1113, 1.0366, 1.0806, 0.8853,\n",
      "         1.0764, 1.1241, 0.9406, 1.0332, 1.0519, 0.8605, 0.8501, 0.9854, 1.1082,\n",
      "         0.8573, 1.2245, 1.0471, 1.0801, 1.2421, 0.7976, 0.9148, 0.9119, 0.9356,\n",
      "         0.9648, 1.1191, 1.0995, 0.8637, 0.8895, 0.9139, 1.2000, 1.2103, 0.9588,\n",
      "         0.9631],\n",
      "        [1.0976, 1.2324, 0.9624, 0.8211, 0.8081, 1.1514, 0.8323, 1.2209, 1.0008,\n",
      "         1.1991, 1.2552, 0.9473, 0.8699, 0.9748, 0.7989, 0.8174, 1.0790, 1.1802,\n",
      "         1.2262, 1.2247, 1.0694, 0.8177, 1.0034, 0.9480, 1.2287, 0.9961, 1.2589,\n",
      "         1.2576, 1.1261, 0.9638, 1.2527, 0.8122, 0.8546, 1.0354, 0.8859, 1.1434,\n",
      "         1.1958, 0.8568, 1.0225, 1.1103, 1.1801, 1.0295, 1.1776, 1.1072, 1.0723,\n",
      "         1.1953, 0.8702, 0.8869, 0.8133, 0.7971, 1.1421, 1.0369, 0.9155, 1.1044,\n",
      "         0.8279, 1.1431, 1.0020, 1.1845, 1.0095, 1.0868, 0.7993, 0.9825, 1.1121,\n",
      "         1.1731, 1.1583, 1.0686, 1.1662, 1.1755, 0.8986, 1.0738, 0.9536, 0.9270,\n",
      "         1.0517, 0.8822, 0.9284, 0.8849, 1.0320, 1.0277, 0.8634, 1.1729, 1.2429,\n",
      "         1.0296, 0.9036, 0.9136, 1.2283, 1.0107, 1.1928, 0.8426, 0.8022, 1.0727,\n",
      "         1.1606, 1.1690, 0.9396, 0.9061, 0.8244, 0.7933, 0.8595, 1.2087, 1.1170,\n",
      "         0.8918],\n",
      "        [0.8432, 0.9276, 0.9351, 0.8863, 0.8902, 0.9224, 1.2382, 0.9573, 0.9314,\n",
      "         1.0031, 1.1734, 1.1076, 1.2416, 1.1726, 0.9878, 0.9980, 0.9663, 0.9497,\n",
      "         1.0985, 1.0017, 1.0828, 0.8190, 0.9797, 0.8480, 0.8960, 0.9053, 0.9032,\n",
      "         1.1064, 1.2053, 0.8532, 0.7985, 0.8879, 0.8005, 1.2115, 1.1667, 1.0961,\n",
      "         1.1343, 0.9286, 0.9303, 0.8154, 0.7981, 1.0327, 0.8632, 1.0661, 0.9340,\n",
      "         0.8827, 0.8161, 1.1112, 1.0820, 1.2252, 1.0538, 0.9315, 0.9578, 1.2318,\n",
      "         1.0393, 0.9662, 1.0217, 0.9758, 0.9734, 1.1211, 1.1364, 0.8064, 1.0674,\n",
      "         1.0228, 0.8459, 1.0508, 1.2628, 0.9957, 0.8019, 1.2471, 0.8663, 0.9690,\n",
      "         0.8252, 1.2267, 0.9712, 1.0146, 1.1052, 0.9974, 0.9180, 0.8822, 1.0208,\n",
      "         0.8047, 1.0622, 1.1584, 0.9620, 1.2129, 1.2622, 0.9216, 0.8349, 0.8416,\n",
      "         0.8349, 0.8344, 1.0582, 1.0763, 1.1507, 1.2069, 1.0796, 1.1576, 1.0897,\n",
      "         1.0840],\n",
      "        [0.9499, 1.0581, 0.8000, 0.8384, 1.0182, 1.0821, 0.8860, 0.8524, 0.9208,\n",
      "         1.2605, 0.9222, 0.9751, 0.8425, 0.9903, 1.0485, 1.0795, 0.8561, 0.8281,\n",
      "         0.8852, 1.0857, 1.0837, 0.8227, 1.0574, 1.1939, 1.1239, 1.1155, 1.0714,\n",
      "         0.9611, 1.1089, 1.2571, 0.8284, 0.9286, 0.9602, 1.0678, 1.1435, 1.0843,\n",
      "         0.8109, 1.0689, 1.0671, 0.8916, 0.9054, 0.9351, 0.9116, 0.8365, 0.9438,\n",
      "         1.1486, 0.9496, 1.1385, 0.9518, 1.1618, 1.1224, 0.8613, 1.1425, 1.1778,\n",
      "         1.0208, 1.1944, 1.2401, 1.1945, 0.8757, 1.1223, 0.8881, 1.0338, 0.8859,\n",
      "         1.2303, 1.1361, 1.0519, 0.9245, 0.9347, 0.9783, 1.1419, 1.0704, 0.8657,\n",
      "         1.1659, 0.9923, 1.1851, 0.8904, 1.2208, 0.9671, 1.0790, 1.1484, 1.2580,\n",
      "         0.9495, 1.0563, 1.1112, 1.2530, 1.2544, 0.9011, 1.1641, 0.8055, 0.8758,\n",
      "         0.8735, 1.1172, 1.2330, 1.2038, 1.1465, 1.1406, 0.9707, 0.8490, 1.0233,\n",
      "         1.0886],\n",
      "        [1.0903, 1.0049, 1.1770, 1.1917, 1.1817, 0.8076, 0.8091, 0.8735, 1.0491,\n",
      "         1.1091, 0.8280, 1.2095, 0.8322, 1.0187, 0.8528, 1.0183, 0.8045, 1.0879,\n",
      "         0.9054, 0.7930, 1.1036, 1.1420, 1.2405, 0.8295, 1.0515, 1.0708, 1.1042,\n",
      "         1.1077, 0.8584, 0.9739, 0.9285, 0.9985, 0.8504, 0.8653, 0.8000, 0.8271,\n",
      "         1.2064, 1.1085, 1.1302, 1.0655, 1.2418, 1.2116, 1.1800, 1.2361, 0.8589,\n",
      "         0.8719, 1.2171, 0.8218, 1.1140, 0.8703, 0.9435, 1.2298, 1.1409, 1.1310,\n",
      "         0.8103, 1.1451, 1.0240, 1.1992, 0.9196, 1.0374, 1.0162, 1.0115, 0.8758,\n",
      "         0.9569, 1.1416, 1.0586, 1.2135, 1.0516, 0.8836, 1.0840, 0.8558, 0.8881,\n",
      "         0.8963, 0.9129, 0.9895, 0.9100, 1.2375, 0.8463, 0.8322, 0.7944, 0.8248,\n",
      "         0.8100, 0.8326, 1.0386, 0.8677, 0.8234, 0.9261, 1.0833, 1.1754, 1.0737,\n",
      "         1.0829, 1.2007, 1.0882, 0.9482, 0.8350, 0.8042, 0.8897, 0.9895, 0.9898,\n",
      "         1.0215],\n",
      "        [1.2591, 1.1613, 1.1823, 0.8266, 1.0407, 0.8739, 1.0033, 1.0994, 0.8522,\n",
      "         0.8124, 1.0714, 0.8340, 1.2096, 1.1355, 0.9275, 1.1010, 0.9069, 0.9062,\n",
      "         1.1340, 1.2384, 0.8271, 1.0299, 0.9325, 1.0610, 1.0598, 1.2095, 1.1552,\n",
      "         1.0932, 1.1704, 1.0293, 1.1087, 1.0133, 1.1579, 0.9089, 0.8075, 1.0269,\n",
      "         0.9098, 0.9017, 1.2328, 0.9147, 0.9846, 1.1331, 1.0553, 0.9525, 1.0171,\n",
      "         0.8179, 1.2488, 1.1671, 0.9198, 0.8886, 1.0268, 1.0880, 1.1950, 0.8021,\n",
      "         1.1303, 0.8830, 0.8620, 0.8541, 1.1323, 0.9573, 0.8364, 0.8776, 1.1407,\n",
      "         0.8331, 0.8311, 1.1074, 0.9681, 1.1495, 0.8248, 1.0750, 0.9782, 1.2619,\n",
      "         0.9410, 0.8501, 0.9615, 1.0276, 1.0109, 1.2545, 0.8713, 1.0889, 0.9110,\n",
      "         0.8799, 1.2500, 1.2152, 0.9322, 0.8013, 0.9118, 1.0384, 0.9834, 0.8332,\n",
      "         0.9161, 1.1351, 1.0559, 0.9483, 1.0460, 0.8563, 1.2020, 0.8109, 1.1288,\n",
      "         1.1355],\n",
      "        [1.0154, 1.1203, 0.8090, 0.9529, 1.1914, 1.0567, 0.9349, 0.9208, 1.2316,\n",
      "         1.0477, 1.0909, 0.8139, 0.8254, 1.0680, 1.2628, 1.1198, 1.2589, 1.0601,\n",
      "         1.2014, 0.8485, 1.0104, 1.1122, 0.9570, 1.1641, 0.8217, 1.0291, 0.8552,\n",
      "         1.0772, 0.9090, 0.9517, 0.9873, 1.0693, 0.8469, 1.0103, 1.2480, 1.1082,\n",
      "         1.0972, 0.8323, 1.2214, 1.2264, 0.8740, 0.7925, 1.0384, 1.0786, 0.8267,\n",
      "         0.9422, 1.0549, 0.8781, 1.2364, 1.1872, 0.8765, 0.8964, 0.9766, 0.9245,\n",
      "         1.2597, 0.8840, 1.2508, 1.2152, 0.9970, 1.2109, 0.7923, 1.0374, 1.0789,\n",
      "         1.0231, 1.1564, 1.1437, 1.2288, 0.8337, 0.8867, 0.9170, 1.0843, 0.8599,\n",
      "         1.2565, 1.0386, 1.2190, 0.7944, 0.9469, 1.2139, 0.9276, 1.1173, 0.8455,\n",
      "         0.9582, 0.8086, 0.9472, 1.0177, 1.1430, 0.8560, 1.0985, 0.8451, 0.9509,\n",
      "         1.1495, 1.2066, 0.8544, 1.0548, 0.9619, 1.0723, 0.8374, 1.1211, 1.0713,\n",
      "         0.9702],\n",
      "        [0.8793, 1.0989, 1.1603, 0.8461, 0.8701, 1.1318, 0.9257, 0.8816, 0.8488,\n",
      "         1.0401, 1.2476, 0.9154, 0.8512, 0.9198, 0.8466, 1.2026, 1.1416, 1.0404,\n",
      "         1.0906, 0.9765, 1.1111, 0.9768, 1.2107, 1.2332, 1.0150, 1.2196, 0.8359,\n",
      "         1.2510, 1.0856, 1.2102, 0.9441, 1.2167, 0.8047, 1.1495, 0.9495, 1.0566,\n",
      "         0.9541, 1.2166, 1.1122, 1.2117, 1.1165, 0.9507, 1.0571, 1.1791, 0.8731,\n",
      "         1.2047, 1.2193, 1.0934, 1.0591, 0.8979, 1.1897, 1.2548, 0.9159, 1.0522,\n",
      "         0.9511, 0.8418, 0.8288, 0.8589, 0.9479, 1.1630, 0.9776, 1.0904, 1.1084,\n",
      "         0.8199, 0.7935, 1.0104, 0.8826, 1.0653, 1.0051, 1.1322, 1.1376, 0.8132,\n",
      "         1.1047, 0.8799, 1.0128, 1.1208, 1.1182, 1.1752, 1.0145, 0.9050, 1.2097,\n",
      "         1.1889, 1.1585, 1.0168, 0.9977, 1.1825, 1.2412, 0.9038, 0.9189, 1.2111,\n",
      "         0.9339, 1.1679, 1.0077, 1.0002, 0.9128, 0.8082, 1.0469, 1.0110, 0.9231,\n",
      "         0.9442],\n",
      "        [1.0965, 0.8029, 0.9774, 1.2327, 0.8040, 0.8846, 1.0467, 1.0263, 1.0619,\n",
      "         1.0566, 0.8469, 1.1620, 0.9788, 0.9747, 0.9491, 0.9829, 1.0097, 0.8952,\n",
      "         1.1055, 1.1278, 0.9992, 0.8559, 0.9461, 0.8385, 1.0103, 0.9515, 0.8250,\n",
      "         0.8136, 1.1515, 0.8623, 1.0552, 0.8830, 0.8991, 0.9700, 0.9256, 0.8772,\n",
      "         1.0671, 0.8161, 1.0160, 1.1688, 0.7990, 0.8638, 0.9535, 1.2093, 0.9019,\n",
      "         1.1837, 0.8088, 1.0150, 0.9466, 0.9930, 0.7970, 0.8106, 0.8987, 0.9191,\n",
      "         1.0306, 0.8227, 1.0513, 1.1365, 0.8528, 0.8872, 1.0499, 1.1362, 1.1838,\n",
      "         0.8934, 0.9085, 1.1355, 1.1113, 1.0363, 1.1470, 1.0387, 0.8421, 0.7967,\n",
      "         0.7944, 0.9890, 1.1414, 1.0269, 0.9880, 1.0391, 0.9743, 0.9136, 1.2003,\n",
      "         1.0583, 0.8773, 1.1069, 0.9330, 0.8045, 0.9781, 0.9300, 1.0540, 1.0406,\n",
      "         0.8446, 1.0076, 0.8562, 0.9409, 0.8147, 1.2446, 1.0645, 1.1080, 0.8079,\n",
      "         0.8496]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n",
    "    if 'rand' in name:\n",
    "        print(param.exp())\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-fighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "stuffed-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "mse_loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "democratic-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = list(model.feature.parameters()) + list(model.true_classifier.parameters())\n",
    "\n",
    "# Optimizers\n",
    "optimizer_T = torch.optim.SGD(true_parameters, lr = 0.1, weight_decay=0.0)\n",
    "optimizer_R = torch.optim.SGD(model.rand_classifier.parameters(), lr=0.1, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "designed-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_E = torch.optim.SGD(model.feature.parameters(), lr = 0.1, weight_decay=0)\n",
    "optimizer_TC = torch.optim.SGD(model.true_classifier.parameters(), lr = 0.1, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fitting-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cardiovascular-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_loss(model, outputs_true, outputs_rand, labels, rand_labels, alpha, gamma, mse_loss_func=None):\n",
    "    \n",
    "    true_loss=model.loss_fn(outputs_true, labels)\n",
    "    if mse_loss_func is not None:\n",
    "        rand_loss=mse_loss_func(outputs_rand, rand_labels)\n",
    "    else:\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "    loss = alpha*true_loss - gamma*rand_loss\n",
    "    \n",
    "    return true_loss, rand_loss, loss\n",
    "\n",
    "\n",
    "def train_true_loop(model, optimizer_T, inputs, labels, rand_labels, alpha, gamma, mse_loss_func=None):\n",
    "    # zero the parameter gradients\n",
    "    optimizer_T.zero_grad()        \n",
    "    model.train_true()\n",
    "    outputs_true = model(inputs, True)\n",
    "    outputs_rand = model(inputs, False)\n",
    "    \n",
    "    true_loss, rand_loss, loss = get_all_loss(model, outputs_true, outputs_rand, labels, rand_labels, \n",
    "                                              alpha, gamma, mse_loss_func)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "    loss.backward()\n",
    "    optimizer_T.step()\n",
    "#     return true_loss, rand_loss, loss\n",
    "    \n",
    "def init_rand_weights(model, init_type='prev'):\n",
    "    if init_type == 'true':\n",
    "        cur_weights=[]\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'true' in name:\n",
    "                cur_weights.append(param.data.clone())\n",
    "                \n",
    "        t=0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' in name:\n",
    "                param.data=cur_weights[t]\n",
    "                t+=1\n",
    "    elif init_type == 'reinit':\n",
    "        if isinstance(model, RandomGamePos):\n",
    "            for name, layer in model.named_children():\n",
    "                if 'rand' in name:\n",
    "                    layer.reset_parameters()\n",
    "        else:\n",
    "            for name, layer in model.named_children():\n",
    "                if 'rand' in name:\n",
    "                    layer[0].reset_parameters()\n",
    "                    \n",
    "                    \n",
    "def train_rand_loop(model, optimizer, inputs, rand_labels, pos_weight=False,\n",
    "                    reinit=False, init_type='prev', num_iter_rand_sb=1, mse_loss_func=None):\n",
    "    \n",
    "    if reinit:\n",
    "        init_rand_weights(model, init_type=init_type)\n",
    "        \n",
    "    model.train_rand()\n",
    "    for k in range(num_iter_rand_sb):\n",
    "        optimizer.zero_grad()\n",
    "        outputs_rand = model(inputs, False)\n",
    "        if mse_loss_func is not None:\n",
    "            rand_loss=mse_loss_func(outputs_rand, rand_labels)\n",
    "        else:\n",
    "            rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        rand_loss.backward()\n",
    "        optimizer.step()\n",
    "        if pos_weight:\n",
    "            for p in model.rand_classifier.parameters():\n",
    "                p.data.clamp_(0)\n",
    "#     return rand_loss\n",
    "\n",
    "def train_true_classifier_loop(model, optimizer, inputs, labels, \n",
    "                    init_type='prev', num_iter_rand_sb=1):\n",
    "    \n",
    "    if init_type == 'reinit':\n",
    "        for name, layer in model.named_children():\n",
    "            if 'true' in name:\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "    model.train_true_classifier()\n",
    "    for k in range(num_iter_rand_sb):\n",
    "        optimizer.zero_grad()\n",
    "        outputs_true, _ = model(inputs, True)\n",
    "        loss=model.loss_fn(outputs_true, labels)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def train_encoder_loop(model, optimizer, inputs, labels, rand_labels, alpha, gamma, mse_loss_func=None):\n",
    "    # zero the parameter gradients\n",
    "    model.train_encoder()\n",
    "    optimizer.zero_grad()        \n",
    "    \n",
    "    outputs_true = model(inputs, True)\n",
    "    outputs_rand = model(inputs, False)\n",
    "    \n",
    "    true_loss, rand_loss, loss = get_all_loss(model, outputs_true, outputs_rand, labels, rand_labels, \n",
    "                                              alpha, gamma, mse_loss_func)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#     return true_loss, rand_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "isolated-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "coordinated-ticket",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exp_logged/mnist_mnist_c/brightness/_convnet_trainfirst_iterT1_iterR1_iterRsb1_gam0.003_alf1.0_lrT0.01_lrR0.01_dgm0.95per300_ep15_simt_sep_rpb_seed0_inittypereinit_fromfresh/15.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-6d10250a52e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exp_logged/mnist_mnist_c/brightness/_convnet_trainfirst_iterT1_iterR1_iterRsb1_gam0.003_alf1.0_lrT0.01_lrR0.01_dgm0.95per300_ep15_simt_sep_rpb_seed0_inittypereinit_fromfresh/15.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# init_rand_weights(model, 'true')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exp_logged/mnist_mnist_c/brightness/_convnet_trainfirst_iterT1_iterR1_iterRsb1_gam0.003_alf1.0_lrT0.01_lrR0.01_dgm0.95per300_ep15_simt_sep_rpb_seed0_inittypereinit_fromfresh/15.tar'"
     ]
    }
   ],
   "source": [
    "checkpoint=torch.load('exp_logged/mnist_mnist_c/brightness/_convnet_trainfirst_iterT1_iterR1_iterRsb1_gam0.003_alf1.0_lrT0.01_lrR0.01_dgm0.95per300_ep15_simt_sep_rpb_seed0_inittypereinit_fromfresh/15.tar')\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# init_rand_weights(model, 'true')\n",
    "\n",
    "outputs_true, emb_true = model(inputs, True)\n",
    "outputs_rand, emb_rand = model(inputs, False)\n",
    "\n",
    "all_embedding.append(emb_rand.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "labeled-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding_stack = np.stack(all_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "brazilian-monte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.707626 , 1.2886751, 1.6251273, 1.5803208, 1.6432005, 1.7805538,\n",
       "       1.7775501, 1.842689 , 1.7846127, 2.0102408, 1.9541191, 2.0461721,\n",
       "       1.9932562, 2.0669484, 2.1290536, 2.2324197], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embedding_stack[:,0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-proceeding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-commercial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-circulation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "permanent-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_classifier_loop(model, optimizer_TC, inputs, labels,\n",
    "                           init_type='prev', num_iter_rand_sb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "hidden-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0813, -0.3591,  0.1400,  ...,  0.4027,  0.1349, -0.1287],\n",
       "        [ 0.2843,  0.2435,  0.1626,  ...,  0.3280,  0.2451,  0.0100],\n",
       "        [ 0.4577, -0.3580, -0.0257,  ...,  0.1060,  0.4364, -0.2737],\n",
       "        ...,\n",
       "        [ 0.1670, -0.2877,  0.2048,  ...,  0.4731, -0.0852, -0.0558],\n",
       "        [ 0.3060, -0.3253, -0.0624,  ..., -0.0842,  0.3183, -0.2497],\n",
       "        [ 0.0940,  0.3608,  0.3324,  ...,  0.1302,  0.3209, -0.4377]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "monthly-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0395, -0.3267,  0.1457,  ...,  0.3531,  0.1920, -0.0370],\n",
       "        [ 0.2668,  0.2190,  0.2317,  ...,  0.2688,  0.2767,  0.1356],\n",
       "        [ 0.4349, -0.3591, -0.0394,  ...,  0.0683,  0.4957, -0.1550],\n",
       "        ...,\n",
       "        [ 0.1454, -0.2678,  0.2171,  ...,  0.4171, -0.0356,  0.0491],\n",
       "        [ 0.2424, -0.3175, -0.1383,  ..., -0.0998,  0.3879, -0.1112],\n",
       "        [ 0.0830,  0.3054,  0.3162,  ...,  0.0986,  0.3741, -0.3022]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "unsigned-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "total = labels.size(0)\n",
    "correct = predicted.eq(labels.data).cpu().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "difficult-partition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1328)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "            _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "            correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "impressed-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true = True\n",
    "iter_counter = 0\n",
    "changer_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "hundred-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "validloss_all = []\n",
    "validacc_all = []\n",
    "testloss_all = []\n",
    "testacc_all = []\n",
    "trainloss_all = []\n",
    "trainacc_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "intelligent-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "total=0\n",
    "correct_rand=0\n",
    "validacc=0\n",
    "testacc=0\n",
    "validloss=0\n",
    "testloss=0\n",
    "print_every=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "introductory-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pointed-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomGame(\n",
       "  (feature): LeNet(\n",
       "    (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  )\n",
       "  (true_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (rand_classifier): PositiveLinear()\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "gothic-permission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.sum(model.true_classifier.weight)\n",
    "\n",
    "# model.true_classifier.bias\n",
    "\n",
    "# torch.sum(model.feature.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "intellectual-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "frank-ordinary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomGame(\n",
       "  (feature): LeNet(\n",
       "    (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  )\n",
       "  (true_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (rand_classifier): PositiveLinear()\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-christianity",
   "metadata": {},
   "source": [
    "## ce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monetary-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2%1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "novel-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.314, true loss: 2.229, loss: 2.541, train acc: 39.312, rand_train_acc: 9.469, test acc: 11.120, train_true is False, gamma is 0.25\n",
      "[1,   100] rand loss: 2.359, true loss: 1.689, loss: 2.159, train acc: 64.766, rand_train_acc: 9.641, test acc: 9.590, train_true is False, gamma is 0.25\n",
      "[1,   150] rand loss: 2.368, true loss: 0.961, loss: 1.609, train acc: 75.969, rand_train_acc: 11.016, test acc: 10.010, train_true is False, gamma is 0.25\n",
      "[1,   200] rand loss: 2.355, true loss: 0.674, loss: 1.393, train acc: 81.984, rand_train_acc: 11.688, test acc: 9.480, train_true is False, gamma is 0.25\n",
      "[1,   250] rand loss: 2.362, true loss: 0.585, loss: 1.330, train acc: 84.656, rand_train_acc: 11.922, test acc: 9.920, train_true is False, gamma is 0.25\n",
      "[1,   300] rand loss: 2.346, true loss: 0.506, loss: 1.269, train acc: 86.297, rand_train_acc: 11.453, test acc: 10.120, train_true is False, gamma is 0.25\n",
      "[1,   350] rand loss: 2.348, true loss: 0.496, loss: 1.256, train acc: 86.172, rand_train_acc: 12.250, test acc: 10.660, train_true is False, gamma is 0.25\n",
      "[1,   400] rand loss: 2.359, true loss: 0.434, loss: 1.219, train acc: 89.031, rand_train_acc: 12.547, test acc: 11.670, train_true is False, gamma is 0.25\n",
      "[2,    50] rand loss: 2.356, true loss: 0.423, loss: 1.203, train acc: 89.147, rand_train_acc: 12.608, test acc: 11.470, train_true is False, gamma is 0.25\n",
      "[2,   100] rand loss: 2.344, true loss: 0.399, loss: 1.179, train acc: 90.203, rand_train_acc: 12.781, test acc: 11.920, train_true is False, gamma is 0.25\n",
      "[2,   150] rand loss: 2.340, true loss: 0.395, loss: 1.177, train acc: 90.125, rand_train_acc: 11.953, test acc: 12.210, train_true is False, gamma is 0.25\n",
      "[2,   200] rand loss: 2.335, true loss: 0.335, loss: 1.135, train acc: 91.578, rand_train_acc: 13.125, test acc: 12.480, train_true is False, gamma is 0.25\n",
      "[2,   250] rand loss: 2.358, true loss: 0.358, loss: 1.150, train acc: 91.125, rand_train_acc: 13.016, test acc: 12.910, train_true is False, gamma is 0.25\n",
      "[2,   300] rand loss: 2.335, true loss: 0.349, loss: 1.144, train acc: 91.516, rand_train_acc: 12.719, test acc: 11.980, train_true is False, gamma is 0.25\n",
      "[2,   350] rand loss: 2.339, true loss: 0.350, loss: 1.142, train acc: 90.891, rand_train_acc: 13.062, test acc: 11.700, train_true is False, gamma is 0.25\n",
      "[2,   400] rand loss: 2.340, true loss: 0.318, loss: 1.118, train acc: 91.906, rand_train_acc: 13.266, test acc: 11.810, train_true is False, gamma is 0.25\n",
      "[3,    50] rand loss: 2.347, true loss: 0.308, loss: 1.110, train acc: 91.882, rand_train_acc: 12.755, test acc: 13.040, train_true is False, gamma is 0.25\n",
      "[3,   100] rand loss: 2.336, true loss: 0.319, loss: 1.120, train acc: 92.203, rand_train_acc: 12.672, test acc: 12.160, train_true is False, gamma is 0.25\n",
      "[3,   150] rand loss: 2.351, true loss: 0.345, loss: 1.135, train acc: 91.609, rand_train_acc: 12.906, test acc: 12.620, train_true is False, gamma is 0.25\n",
      "[3,   200] rand loss: 2.342, true loss: 0.309, loss: 1.102, train acc: 92.625, rand_train_acc: 13.219, test acc: 13.760, train_true is False, gamma is 0.25\n",
      "[3,   250] rand loss: 2.344, true loss: 0.291, loss: 1.095, train acc: 93.000, rand_train_acc: 12.484, test acc: 13.190, train_true is False, gamma is 0.25\n",
      "[3,   300] rand loss: 2.340, true loss: 0.309, loss: 1.113, train acc: 92.203, rand_train_acc: 12.672, test acc: 13.030, train_true is False, gamma is 0.25\n",
      "[3,   350] rand loss: 2.340, true loss: 0.282, loss: 1.085, train acc: 92.969, rand_train_acc: 12.609, test acc: 13.640, train_true is False, gamma is 0.25\n",
      "[3,   400] rand loss: 2.341, true loss: 0.299, loss: 1.100, train acc: 92.422, rand_train_acc: 12.781, test acc: 13.430, train_true is False, gamma is 0.25\n",
      "[4,    50] rand loss: 2.338, true loss: 0.262, loss: 1.076, train acc: 92.941, rand_train_acc: 12.765, test acc: 13.190, train_true is False, gamma is 0.25\n",
      "[4,   100] rand loss: 2.332, true loss: 0.304, loss: 1.106, train acc: 92.984, rand_train_acc: 13.297, test acc: 13.400, train_true is False, gamma is 0.25\n",
      "[4,   150] rand loss: 2.336, true loss: 0.267, loss: 1.076, train acc: 93.422, rand_train_acc: 13.000, test acc: 14.080, train_true is False, gamma is 0.25\n",
      "[4,   200] rand loss: 2.340, true loss: 0.242, loss: 1.057, train acc: 94.062, rand_train_acc: 13.172, test acc: 13.390, train_true is False, gamma is 0.25\n",
      "[4,   250] rand loss: 2.337, true loss: 0.280, loss: 1.086, train acc: 93.469, rand_train_acc: 13.750, test acc: 14.060, train_true is False, gamma is 0.25\n",
      "[4,   300] rand loss: 2.326, true loss: 0.301, loss: 1.103, train acc: 93.609, rand_train_acc: 13.078, test acc: 13.750, train_true is False, gamma is 0.25\n",
      "[4,   350] rand loss: 2.330, true loss: 0.243, loss: 1.059, train acc: 93.641, rand_train_acc: 13.125, test acc: 13.710, train_true is False, gamma is 0.25\n",
      "[4,   400] rand loss: 2.338, true loss: 0.255, loss: 1.062, train acc: 93.812, rand_train_acc: 12.656, test acc: 13.740, train_true is False, gamma is 0.25\n",
      "[5,    50] rand loss: 2.322, true loss: 0.236, loss: 1.050, train acc: 93.971, rand_train_acc: 12.814, test acc: 14.110, train_true is False, gamma is 0.25\n",
      "[5,   100] rand loss: 2.336, true loss: 0.256, loss: 1.065, train acc: 94.000, rand_train_acc: 12.750, test acc: 14.050, train_true is False, gamma is 0.25\n",
      "[5,   150] rand loss: 2.333, true loss: 0.256, loss: 1.069, train acc: 94.266, rand_train_acc: 12.844, test acc: 15.040, train_true is False, gamma is 0.25\n",
      "[5,   200] rand loss: 2.331, true loss: 0.235, loss: 1.051, train acc: 94.234, rand_train_acc: 13.266, test acc: 13.850, train_true is False, gamma is 0.25\n",
      "[5,   250] rand loss: 2.335, true loss: 0.245, loss: 1.060, train acc: 94.250, rand_train_acc: 13.281, test acc: 14.200, train_true is False, gamma is 0.25\n",
      "[5,   300] rand loss: 2.331, true loss: 0.243, loss: 1.060, train acc: 94.578, rand_train_acc: 13.203, test acc: 14.220, train_true is False, gamma is 0.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-fb6a026fa3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moutputs_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutputs_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-26d31cf87f91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, true_flag)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0membedding\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrue_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-93878935b2fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         x = self.fc3(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_true=True\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if train_true:\n",
    "            # zero the parameter gradients\n",
    "            optimizer_T.zero_grad()        \n",
    "            model.train_true()\n",
    "            outputs_true = model(inputs, True)\n",
    "            outputs_rand = model(inputs, False)\n",
    "\n",
    "            true_loss=ce_loss_fn(outputs_true, labels)\n",
    "            rand_loss=ce_loss_fn(outputs_rand, rand_labels)\n",
    "            loss = 1.5*true_loss - gamma*rand_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_T.step()\n",
    "        \n",
    "        \n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "#             for k in range(20):\n",
    "            optimizer_R.zero_grad()\n",
    "            outputs_rand = model(inputs, False)\n",
    "            loss = ce_loss_fn(outputs_rand, rand_labels)\n",
    "#             print(loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "#                 train_true=True\n",
    "\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "        \n",
    "        if iter_counter >= 1 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "#         if i % 50 == 49:\n",
    "#             gamma *= 1.1\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "alternate-hammer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "going-charger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.298, true loss: 1.917, loss: 2.645, train acc: 51.828, rand_train_acc: 10.859, test acc: 19.920, train_true is True, gamma is 0.10\n",
      "[1,   100] rand loss: 2.264, true loss: 0.746, loss: 0.893, train acc: 78.375, rand_train_acc: 15.391, test acc: 23.990, train_true is True, gamma is 0.10\n",
      "[1,   150] rand loss: 2.247, true loss: 0.494, loss: 0.539, train acc: 85.188, rand_train_acc: 15.672, test acc: 42.090, train_true is True, gamma is 0.09\n",
      "[1,   200] rand loss: 2.249, true loss: 0.413, loss: 0.417, train acc: 88.141, rand_train_acc: 15.984, test acc: 33.610, train_true is True, gamma is 0.09\n",
      "[1,   250] rand loss: 2.246, true loss: 0.364, loss: 0.364, train acc: 89.531, rand_train_acc: 16.344, test acc: 42.690, train_true is True, gamma is 0.08\n",
      "[1,   300] rand loss: 2.248, true loss: 0.338, loss: 0.325, train acc: 89.844, rand_train_acc: 15.859, test acc: 40.320, train_true is True, gamma is 0.08\n",
      "[1,   350] rand loss: 2.248, true loss: 0.316, loss: 0.311, train acc: 90.719, rand_train_acc: 15.234, test acc: 38.470, train_true is True, gamma is 0.07\n",
      "[1,   400] rand loss: 2.247, true loss: 0.291, loss: 0.273, train acc: 91.594, rand_train_acc: 16.281, test acc: 42.230, train_true is True, gamma is 0.07\n",
      "[2,    50] rand loss: 2.246, true loss: 0.255, loss: 0.236, train acc: 92.480, rand_train_acc: 16.176, test acc: 37.000, train_true is True, gamma is 0.07\n",
      "[2,   100] rand loss: 2.249, true loss: 0.239, loss: 0.212, train acc: 93.109, rand_train_acc: 16.234, test acc: 28.530, train_true is True, gamma is 0.07\n",
      "[2,   150] rand loss: 2.255, true loss: 0.250, loss: 0.241, train acc: 92.750, rand_train_acc: 15.859, test acc: 36.140, train_true is True, gamma is 0.06\n",
      "[2,   200] rand loss: 2.252, true loss: 0.233, loss: 0.217, train acc: 93.438, rand_train_acc: 16.531, test acc: 34.740, train_true is True, gamma is 0.06\n",
      "[2,   250] rand loss: 2.256, true loss: 0.222, loss: 0.213, train acc: 93.812, rand_train_acc: 15.516, test acc: 35.490, train_true is True, gamma is 0.05\n",
      "[2,   300] rand loss: 2.254, true loss: 0.205, loss: 0.188, train acc: 93.891, rand_train_acc: 15.672, test acc: 20.250, train_true is True, gamma is 0.05\n",
      "[2,   350] rand loss: 2.251, true loss: 0.189, loss: 0.175, train acc: 94.469, rand_train_acc: 16.266, test acc: 27.750, train_true is True, gamma is 0.05\n",
      "[2,   400] rand loss: 2.255, true loss: 0.199, loss: 0.191, train acc: 94.281, rand_train_acc: 16.000, test acc: 30.930, train_true is True, gamma is 0.05\n",
      "[3,    50] rand loss: 2.251, true loss: 0.171, loss: 0.159, train acc: 94.588, rand_train_acc: 16.196, test acc: 37.780, train_true is True, gamma is 0.04\n",
      "[3,   100] rand loss: 2.254, true loss: 0.163, loss: 0.147, train acc: 95.266, rand_train_acc: 15.922, test acc: 27.730, train_true is True, gamma is 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-586-92b7c2bf168c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#             validloss, validacc = test(validloader, model, bn_eval=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mtestloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_c_bright_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#             validloss_all.append(validloss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e968ef14151e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, save, epoch, bn_eval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         inputs, targets = next(iter(loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyreg_dispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_reducers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_true=False\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "            for k in range(1):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = ce_loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "                train_true=True\n",
    "        \n",
    "        model.train_true()\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=ce_loss_fn(outputs_true, labels)\n",
    "        rand_loss=ce_loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 1.5*true_loss - gamma*rand_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            gamma *= 0.9\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "interested-squad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-marriage",
   "metadata": {},
   "source": [
    "## train without game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "endangered-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.187, train acc: 43.438, test acc: 12.080, train_true is True, gamma is 0.10\n",
      "[1,   100] loss: 1.297, train acc: 72.594, test acc: 10.610, train_true is True, gamma is 0.10\n",
      "[1,   150] loss: 0.639, train acc: 82.391, test acc: 9.250, train_true is True, gamma is 0.10\n",
      "[1,   200] loss: 0.474, train acc: 86.656, test acc: 10.200, train_true is True, gamma is 0.10\n",
      "[1,   250] loss: 0.424, train acc: 87.922, test acc: 10.110, train_true is True, gamma is 0.10\n",
      "[1,   300] loss: 0.391, train acc: 88.578, test acc: 11.690, train_true is True, gamma is 0.10\n",
      "[1,   350] loss: 0.381, train acc: 89.078, test acc: 11.770, train_true is True, gamma is 0.10\n",
      "[1,   400] loss: 0.354, train acc: 89.703, test acc: 12.020, train_true is True, gamma is 0.10\n",
      "[2,    50] loss: 0.319, train acc: 91.147, test acc: 12.540, train_true is True, gamma is 0.10\n",
      "[2,   100] loss: 0.312, train acc: 91.031, test acc: 12.600, train_true is True, gamma is 0.10\n",
      "[2,   150] loss: 0.304, train acc: 91.422, test acc: 13.510, train_true is True, gamma is 0.10\n",
      "[2,   200] loss: 0.286, train acc: 91.797, test acc: 13.560, train_true is True, gamma is 0.10\n",
      "[2,   250] loss: 0.292, train acc: 91.734, test acc: 14.220, train_true is True, gamma is 0.10\n",
      "[2,   300] loss: 0.270, train acc: 91.984, test acc: 14.000, train_true is True, gamma is 0.10\n",
      "[2,   350] loss: 0.255, train acc: 92.422, test acc: 14.220, train_true is True, gamma is 0.10\n",
      "[2,   400] loss: 0.252, train acc: 92.500, test acc: 14.620, train_true is True, gamma is 0.10\n",
      "[3,    50] loss: 0.230, train acc: 93.284, test acc: 14.980, train_true is True, gamma is 0.10\n",
      "[3,   100] loss: 0.235, train acc: 93.250, test acc: 14.700, train_true is True, gamma is 0.10\n",
      "[3,   150] loss: 0.217, train acc: 93.656, test acc: 15.360, train_true is True, gamma is 0.10\n",
      "[3,   200] loss: 0.221, train acc: 93.391, test acc: 15.520, train_true is True, gamma is 0.10\n",
      "[3,   250] loss: 0.198, train acc: 94.062, test acc: 15.910, train_true is True, gamma is 0.10\n",
      "[3,   300] loss: 0.215, train acc: 94.047, test acc: 15.880, train_true is True, gamma is 0.10\n",
      "[3,   350] loss: 0.196, train acc: 94.094, test acc: 15.900, train_true is True, gamma is 0.10\n",
      "[3,   400] loss: 0.203, train acc: 94.094, test acc: 16.400, train_true is True, gamma is 0.10\n",
      "[4,    50] loss: 0.177, train acc: 94.559, test acc: 17.380, train_true is True, gamma is 0.10\n",
      "[4,   100] loss: 0.163, train acc: 95.125, test acc: 16.850, train_true is True, gamma is 0.10\n",
      "[4,   150] loss: 0.176, train acc: 95.281, test acc: 17.330, train_true is True, gamma is 0.10\n",
      "[4,   200] loss: 0.171, train acc: 95.000, test acc: 16.940, train_true is True, gamma is 0.10\n",
      "[4,   250] loss: 0.174, train acc: 95.047, test acc: 16.870, train_true is True, gamma is 0.10\n",
      "[4,   300] loss: 0.154, train acc: 95.484, test acc: 17.850, train_true is True, gamma is 0.10\n",
      "[4,   350] loss: 0.168, train acc: 95.109, test acc: 17.580, train_true is True, gamma is 0.10\n",
      "[4,   400] loss: 0.159, train acc: 95.344, test acc: 18.000, train_true is True, gamma is 0.10\n",
      "[5,    50] loss: 0.133, train acc: 95.961, test acc: 18.270, train_true is True, gamma is 0.10\n",
      "[5,   100] loss: 0.140, train acc: 95.875, test acc: 18.650, train_true is True, gamma is 0.10\n",
      "[5,   150] loss: 0.137, train acc: 96.109, test acc: 18.730, train_true is True, gamma is 0.10\n",
      "[5,   200] loss: 0.136, train acc: 95.922, test acc: 18.890, train_true is True, gamma is 0.10\n",
      "[5,   250] loss: 0.139, train acc: 96.172, test acc: 18.600, train_true is True, gamma is 0.10\n",
      "[5,   300] loss: 0.126, train acc: 96.391, test acc: 18.490, train_true is True, gamma is 0.10\n",
      "[5,   350] loss: 0.136, train acc: 96.156, test acc: 18.280, train_true is True, gamma is 0.10\n",
      "[5,   400] loss: 0.133, train acc: 96.094, test acc: 19.280, train_true is True, gamma is 0.10\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model.train_true()\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        outputs_true = model(inputs, True)\n",
    "        \n",
    "        loss=ce_loss_fn(outputs_true, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] loss: %.3f, train acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 1 and train_true:\n",
    "#             train_true = False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "#         if i % 100 == 99:\n",
    "#             gamma *= 1.1\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-webmaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "bronze-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "selected-configuration",
   "metadata": {},
   "source": [
    "## mse version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "smart-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 0.105, true loss: 1.858, loss: 2.682, train acc: 52.141, rand_train_acc: 10.562, test acc: 29.930, train_true is True, gamma is 1.00\n",
      "[1,   100] rand loss: 0.100, true loss: 0.752, loss: 1.029, train acc: 77.422, rand_train_acc: 12.078, test acc: 34.550, train_true is True, gamma is 1.00\n",
      "[1,   150] rand loss: 94.333, true loss: 6.696, loss: -74.855, train acc: 53.484, rand_train_acc: 10.391, test acc: 17.630, train_true is True, gamma is 0.90\n",
      "[1,   200] rand loss: 4932.527, true loss: 92.846, loss: -4300.005, train acc: 19.750, rand_train_acc: 11.344, test acc: 9.800, train_true is True, gamma is 0.90\n",
      "[1,   250] rand loss: 44288.547, true loss: 363.182, loss: -35328.950, train acc: 11.594, rand_train_acc: 10.703, test acc: 9.820, train_true is True, gamma is 0.81\n",
      "[1,   300] rand loss: 185581.833, true loss: 797.359, loss: -149125.247, train acc: 10.688, rand_train_acc: 10.094, test acc: 9.580, train_true is True, gamma is 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-570-8132297249fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#             validloss, validacc = test(validloader, model, bn_eval=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtestloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_c_bright_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#             validloss_all.append(validloss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e968ef14151e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, save, epoch, bn_eval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         inputs, targets = next(iter(loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels, rand_labels_oh = inputs.cuda(), labels.cuda(), rand_labels.cuda(), rand_labels_oh.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "            for k in range(1):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = mse_loss_fn(outputs_rand, rand_labels_oh)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "                train_true=True\n",
    "        \n",
    "        model.train_true()\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=ce_loss_fn(outputs_true, labels)\n",
    "        rand_loss=mse_loss_fn(outputs_rand, rand_labels_oh)\n",
    "        loss = 1.5*true_loss - gamma*rand_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            gamma *= 0.9\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "tamil-fishing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "impaired-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-george",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-stephen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-captain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-layer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-rogers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 2.5*true_loss - gamma*(rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-shopper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-charlotte",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-claim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-fossil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-bread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-tournament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-healing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-investor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-bidder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "biological-palmer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.376, true loss: 1.603, loss: 1.633, train acc: 54.719, rand_train_acc: 12.016, train_true is False, gamma is 1.00\n",
      "[1,   100] rand loss: 2.593, true loss: 0.681, loss: -0.891, train acc: 77.750, rand_train_acc: 14.016, train_true is False, gamma is 1.00\n",
      "[1,   150] rand loss: 2.909, true loss: 0.607, loss: -1.392, train acc: 82.625, rand_train_acc: 14.016, train_true is False, gamma is 1.00\n",
      "[1,   200] rand loss: 3.747, true loss: 1.716, loss: 0.544, train acc: 76.094, rand_train_acc: 13.062, train_true is False, gamma is 1.00\n",
      "[1,   250] rand loss: 3.135, true loss: 0.891, loss: -0.908, train acc: 82.094, rand_train_acc: 13.594, train_true is False, gamma is 1.00\n",
      "[1,   300] rand loss: 2.755, true loss: 0.418, loss: -1.709, train acc: 87.891, rand_train_acc: 14.734, train_true is False, gamma is 1.00\n",
      "[1,   350] rand loss: 4.828, true loss: 2.030, loss: 0.488, train acc: 78.219, rand_train_acc: 14.141, train_true is False, gamma is 0.95\n",
      "[1,   400] rand loss: 2.674, true loss: 0.444, loss: -1.431, train acc: 87.781, rand_train_acc: 13.906, train_true is False, gamma is 0.95\n",
      "[2,    50] rand loss: 3.198, true loss: 0.805, loss: -1.025, train acc: 86.461, rand_train_acc: 14.520, train_true is False, gamma is 0.95\n",
      "[2,   100] rand loss: 3.062, true loss: 0.439, loss: -1.811, train acc: 88.906, rand_train_acc: 14.578, train_true is False, gamma is 0.95\n",
      "[2,   150] rand loss: 4.355, true loss: 1.320, loss: -0.836, train acc: 84.219, rand_train_acc: 14.453, train_true is False, gamma is 0.95\n",
      "[2,   200] rand loss: 6.064, true loss: 2.331, loss: 0.068, train acc: 83.359, rand_train_acc: 13.906, train_true is False, gamma is 0.95\n",
      "[2,   250] rand loss: 2.765, true loss: 0.471, loss: -1.450, train acc: 88.516, rand_train_acc: 14.656, train_true is False, gamma is 0.95\n",
      "[2,   300] rand loss: 2.873, true loss: 0.365, loss: -1.817, train acc: 91.312, rand_train_acc: 15.250, train_true is False, gamma is 0.95\n",
      "[2,   350] rand loss: 5.584, true loss: 3.077, loss: 2.654, train acc: 80.844, rand_train_acc: 14.281, train_true is False, gamma is 0.90\n",
      "[2,   400] rand loss: 2.845, true loss: 0.470, loss: -1.393, train acc: 87.953, rand_train_acc: 13.672, train_true is False, gamma is 0.90\n",
      "[3,    50] rand loss: 2.939, true loss: 0.427, loss: -1.584, train acc: 90.755, rand_train_acc: 15.000, train_true is False, gamma is 0.90\n",
      "[3,   100] rand loss: 4.265, true loss: 0.773, loss: -1.918, train acc: 88.234, rand_train_acc: 13.969, train_true is False, gamma is 0.90\n",
      "[3,   150] rand loss: 4.063, true loss: 0.600, loss: -2.168, train acc: 90.203, rand_train_acc: 14.578, train_true is False, gamma is 0.90\n",
      "[3,   200] rand loss: 7.477, true loss: 1.509, loss: -2.977, train acc: 84.688, rand_train_acc: 13.188, train_true is False, gamma is 0.90\n",
      "[3,   250] rand loss: 9.488, true loss: 2.401, loss: -2.559, train acc: 86.594, rand_train_acc: 15.016, train_true is False, gamma is 0.90\n",
      "[3,   300] rand loss: 12.698, true loss: 3.332, loss: -3.131, train acc: 81.500, rand_train_acc: 14.141, train_true is False, gamma is 0.90\n",
      "[3,   350] rand loss: 6.292, true loss: 1.935, loss: -0.556, train acc: 82.438, rand_train_acc: 14.406, train_true is False, gamma is 0.86\n",
      "[3,   400] rand loss: 3.303, true loss: 0.459, loss: -1.684, train acc: 91.078, rand_train_acc: 16.312, train_true is False, gamma is 0.86\n",
      "[4,    50] rand loss: 19.926, true loss: 7.648, loss: 2.036, train acc: 81.176, rand_train_acc: 14.225, train_true is False, gamma is 0.86\n",
      "[4,   100] rand loss: 22.956, true loss: 5.270, loss: -6.507, train acc: 80.047, rand_train_acc: 14.438, train_true is False, gamma is 0.86\n",
      "[4,   150] rand loss: 4.113, true loss: 0.626, loss: -1.962, train acc: 89.078, rand_train_acc: 15.844, train_true is False, gamma is 0.86\n",
      "[4,   200] rand loss: 25.407, true loss: 13.358, loss: 11.610, train acc: 78.062, rand_train_acc: 14.578, train_true is False, gamma is 0.86\n",
      "[4,   250] rand loss: 66.027, true loss: 43.958, loss: 53.285, train acc: 41.328, rand_train_acc: 12.312, train_true is False, gamma is 0.86\n",
      "[4,   300] rand loss: 3.024, true loss: 0.842, loss: -0.487, train acc: 73.109, rand_train_acc: 13.203, train_true is False, gamma is 0.86\n",
      "[4,   350] rand loss: 3.034, true loss: 0.584, loss: -1.012, train acc: 83.953, rand_train_acc: 14.875, train_true is False, gamma is 0.81\n",
      "[4,   400] rand loss: 3.104, true loss: 0.545, loss: -1.165, train acc: 86.016, rand_train_acc: 15.391, train_true is False, gamma is 0.81\n",
      "[5,    50] rand loss: 2.458, true loss: 1.809, loss: 2.519, train acc: 31.471, rand_train_acc: 11.255, train_true is False, gamma is 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-de1532c3544d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer_R\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0moutputs_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#             print(loss.cpu().detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-67da3d9a33d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, true_flag)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 2.5*true_loss - gamma*(rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "likely-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "flush-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.316, true loss: 2.167, loss: 2.167, train acc: 44.484, rand_train_acc: 11.719, train_true is False, gamma is 1.00\n",
      "[1,   100] rand loss: 2.420, true loss: 1.231, loss: 1.231, train acc: 69.891, rand_train_acc: 12.703, train_true is False, gamma is 1.00\n",
      "[1,   150] rand loss: 2.442, true loss: 0.647, loss: 0.647, train acc: 80.812, rand_train_acc: 13.125, train_true is False, gamma is 1.00\n",
      "[1,   200] rand loss: 2.504, true loss: 0.490, loss: 0.490, train acc: 85.234, rand_train_acc: 12.531, train_true is False, gamma is 1.00\n",
      "[1,   250] rand loss: 2.473, true loss: 0.433, loss: 0.433, train acc: 87.203, rand_train_acc: 13.781, train_true is False, gamma is 1.00\n",
      "[1,   300] rand loss: 2.527, true loss: 0.395, loss: 0.395, train acc: 88.547, rand_train_acc: 13.406, train_true is False, gamma is 1.00\n",
      "[1,   350] rand loss: 2.512, true loss: 0.367, loss: 0.367, train acc: 89.750, rand_train_acc: 13.828, train_true is False, gamma is 1.10\n",
      "[1,   400] rand loss: 2.509, true loss: 0.360, loss: 0.360, train acc: 89.750, rand_train_acc: 13.703, train_true is False, gamma is 1.10\n",
      "[2,    50] rand loss: 2.538, true loss: 0.332, loss: 0.332, train acc: 89.990, rand_train_acc: 13.745, train_true is False, gamma is 1.10\n",
      "[2,   100] rand loss: 2.514, true loss: 0.313, loss: 0.313, train acc: 90.578, rand_train_acc: 13.344, train_true is False, gamma is 1.10\n",
      "[2,   150] rand loss: 2.508, true loss: 0.279, loss: 0.279, train acc: 91.734, rand_train_acc: 14.062, train_true is False, gamma is 1.10\n",
      "[2,   200] rand loss: 2.563, true loss: 0.281, loss: 0.281, train acc: 91.844, rand_train_acc: 14.891, train_true is False, gamma is 1.10\n",
      "[2,   250] rand loss: 2.506, true loss: 0.275, loss: 0.275, train acc: 91.922, rand_train_acc: 14.078, train_true is False, gamma is 1.10\n",
      "[2,   300] rand loss: 2.521, true loss: 0.258, loss: 0.258, train acc: 92.297, rand_train_acc: 14.203, train_true is False, gamma is 1.10\n",
      "[2,   350] rand loss: 2.540, true loss: 0.255, loss: 0.255, train acc: 93.031, rand_train_acc: 13.750, train_true is False, gamma is 1.21\n",
      "[2,   400] rand loss: 2.544, true loss: 0.255, loss: 0.255, train acc: 92.469, rand_train_acc: 13.531, train_true is False, gamma is 1.21\n",
      "[3,    50] rand loss: 2.543, true loss: 0.227, loss: 0.227, train acc: 93.343, rand_train_acc: 13.931, train_true is False, gamma is 1.21\n",
      "[3,   100] rand loss: 2.554, true loss: 0.218, loss: 0.218, train acc: 93.547, rand_train_acc: 13.875, train_true is False, gamma is 1.21\n",
      "[3,   150] rand loss: 2.524, true loss: 0.201, loss: 0.201, train acc: 94.266, rand_train_acc: 14.719, train_true is False, gamma is 1.21\n",
      "[3,   200] rand loss: 2.520, true loss: 0.215, loss: 0.215, train acc: 93.953, rand_train_acc: 14.109, train_true is False, gamma is 1.21\n",
      "[3,   250] rand loss: 2.561, true loss: 0.213, loss: 0.213, train acc: 93.844, rand_train_acc: 14.703, train_true is False, gamma is 1.21\n",
      "[3,   300] rand loss: 2.546, true loss: 0.185, loss: 0.185, train acc: 94.656, rand_train_acc: 14.172, train_true is False, gamma is 1.21\n",
      "[3,   350] rand loss: 2.520, true loss: 0.213, loss: 0.213, train acc: 93.281, rand_train_acc: 14.422, train_true is False, gamma is 1.33\n",
      "[3,   400] rand loss: 2.551, true loss: 0.188, loss: 0.188, train acc: 94.719, rand_train_acc: 14.547, train_true is False, gamma is 1.33\n",
      "[4,    50] rand loss: 2.521, true loss: 0.172, loss: 0.172, train acc: 94.961, rand_train_acc: 14.696, train_true is False, gamma is 1.33\n",
      "[4,   100] rand loss: 2.510, true loss: 0.189, loss: 0.189, train acc: 94.516, rand_train_acc: 14.500, train_true is False, gamma is 1.33\n",
      "[4,   150] rand loss: 2.552, true loss: 0.176, loss: 0.176, train acc: 94.812, rand_train_acc: 15.125, train_true is False, gamma is 1.33\n",
      "[4,   200] rand loss: 2.526, true loss: 0.160, loss: 0.160, train acc: 95.203, rand_train_acc: 13.453, train_true is False, gamma is 1.33\n",
      "[4,   250] rand loss: 2.543, true loss: 0.152, loss: 0.152, train acc: 95.125, rand_train_acc: 14.125, train_true is False, gamma is 1.33\n",
      "[4,   300] rand loss: 2.571, true loss: 0.150, loss: 0.150, train acc: 95.953, rand_train_acc: 13.922, train_true is False, gamma is 1.33\n",
      "[4,   350] rand loss: 2.553, true loss: 0.160, loss: 0.160, train acc: 95.016, rand_train_acc: 14.156, train_true is False, gamma is 1.46\n",
      "[4,   400] rand loss: 2.519, true loss: 0.146, loss: 0.146, train acc: 95.953, rand_train_acc: 14.484, train_true is False, gamma is 1.46\n",
      "[5,    50] rand loss: 2.554, true loss: 0.143, loss: 0.143, train acc: 95.735, rand_train_acc: 14.510, train_true is False, gamma is 1.46\n",
      "[5,   100] rand loss: 2.543, true loss: 0.127, loss: 0.127, train acc: 96.422, rand_train_acc: 14.734, train_true is False, gamma is 1.46\n",
      "[5,   150] rand loss: 2.534, true loss: 0.134, loss: 0.134, train acc: 96.375, rand_train_acc: 14.391, train_true is False, gamma is 1.46\n",
      "[5,   200] rand loss: 2.556, true loss: 0.134, loss: 0.134, train acc: 95.969, rand_train_acc: 14.422, train_true is False, gamma is 1.46\n",
      "[5,   250] rand loss: 2.522, true loss: 0.134, loss: 0.134, train acc: 96.297, rand_train_acc: 14.938, train_true is False, gamma is 1.46\n",
      "[5,   300] rand loss: 2.544, true loss: 0.137, loss: 0.137, train acc: 95.969, rand_train_acc: 14.922, train_true is False, gamma is 1.46\n",
      "[5,   350] rand loss: 2.566, true loss: 0.137, loss: 0.137, train acc: 96.062, rand_train_acc: 14.328, train_true is False, gamma is 1.61\n",
      "[5,   400] rand loss: 2.570, true loss: 0.131, loss: 0.131, train acc: 96.219, rand_train_acc: 15.078, train_true is False, gamma is 1.61\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 1*true_loss# - gamma*(2.3-rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 1.1\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "chemical-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] train loss: 2.348, train acc: 8.094, rand_train_acc: 10.484, train_true is False, gamma is 0.50\n",
      "[1,   100] train loss: 2.304, train acc: 8.484, rand_train_acc: 9.938, train_true is False, gamma is 0.50\n",
      "[1,   150] train loss: 2.303, train acc: 9.172, rand_train_acc: 9.484, train_true is False, gamma is 0.50\n",
      "[1,   200] train loss: 2.349, train acc: 7.672, rand_train_acc: 10.188, train_true is False, gamma is 0.50\n",
      "[1,   250] train loss: 2.303, train acc: 8.891, rand_train_acc: 9.953, train_true is False, gamma is 0.50\n",
      "[1,   300] train loss: 2.303, train acc: 9.297, rand_train_acc: 9.391, train_true is False, gamma is 0.50\n",
      "[1,   350] train loss: 2.349, train acc: 8.203, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[1,   400] train loss: 2.303, train acc: 9.078, rand_train_acc: 10.312, train_true is False, gamma is 0.50\n",
      "[2,    50] train loss: 2.349, train acc: 8.490, rand_train_acc: 9.833, train_true is False, gamma is 0.50\n",
      "[2,   100] train loss: 2.303, train acc: 8.641, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[2,   150] train loss: 2.303, train acc: 8.844, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[2,   200] train loss: 2.349, train acc: 8.703, rand_train_acc: 10.062, train_true is False, gamma is 0.50\n",
      "[2,   250] train loss: 2.303, train acc: 7.922, rand_train_acc: 10.531, train_true is False, gamma is 0.50\n",
      "[2,   300] train loss: 2.303, train acc: 8.734, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[2,   350] train loss: 2.349, train acc: 8.422, rand_train_acc: 10.422, train_true is False, gamma is 0.50\n",
      "[2,   400] train loss: 2.303, train acc: 8.875, rand_train_acc: 10.484, train_true is False, gamma is 0.50\n",
      "[3,    50] train loss: 2.303, train acc: 8.520, rand_train_acc: 10.157, train_true is False, gamma is 0.50\n",
      "[3,   100] train loss: 2.349, train acc: 8.188, rand_train_acc: 10.578, train_true is False, gamma is 0.50\n",
      "[3,   150] train loss: 2.303, train acc: 8.625, rand_train_acc: 9.719, train_true is False, gamma is 0.50\n",
      "[3,   200] train loss: 2.303, train acc: 8.750, rand_train_acc: 9.516, train_true is False, gamma is 0.50\n",
      "[3,   250] train loss: 2.348, train acc: 8.750, rand_train_acc: 10.375, train_true is False, gamma is 0.50\n",
      "[3,   300] train loss: 2.302, train acc: 8.844, rand_train_acc: 10.562, train_true is False, gamma is 0.50\n",
      "[3,   350] train loss: 2.303, train acc: 8.875, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[3,   400] train loss: 2.349, train acc: 8.422, rand_train_acc: 9.812, train_true is False, gamma is 0.50\n",
      "[4,    50] train loss: 2.303, train acc: 8.373, rand_train_acc: 9.804, train_true is False, gamma is 0.50\n",
      "[4,   100] train loss: 2.349, train acc: 8.750, rand_train_acc: 10.047, train_true is False, gamma is 0.50\n",
      "[4,   150] train loss: 2.303, train acc: 8.922, rand_train_acc: 9.375, train_true is False, gamma is 0.50\n",
      "[4,   200] train loss: 2.303, train acc: 8.641, rand_train_acc: 9.828, train_true is False, gamma is 0.50\n",
      "[4,   250] train loss: 2.349, train acc: 8.281, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[4,   300] train loss: 2.303, train acc: 8.375, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[4,   350] train loss: 2.303, train acc: 8.391, rand_train_acc: 9.516, train_true is False, gamma is 0.50\n",
      "[4,   400] train loss: 2.348, train acc: 9.047, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[5,    50] train loss: 2.303, train acc: 8.980, rand_train_acc: 9.961, train_true is False, gamma is 0.50\n",
      "[5,   100] train loss: 2.303, train acc: 8.203, rand_train_acc: 9.641, train_true is False, gamma is 0.50\n",
      "[5,   150] train loss: 2.349, train acc: 8.906, rand_train_acc: 10.078, train_true is False, gamma is 0.50\n",
      "[5,   200] train loss: 2.303, train acc: 8.391, rand_train_acc: 10.750, train_true is False, gamma is 0.50\n",
      "[5,   250] train loss: 2.303, train acc: 8.406, rand_train_acc: 9.828, train_true is False, gamma is 0.50\n",
      "[5,   300] train loss: 2.349, train acc: 8.172, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[5,   350] train loss: 2.303, train acc: 8.828, rand_train_acc: 9.672, train_true is False, gamma is 0.50\n",
      "[5,   400] train loss: 2.303, train acc: 8.797, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[6,    50] train loss: 2.303, train acc: 8.480, rand_train_acc: 10.412, train_true is False, gamma is 0.50\n",
      "[6,   100] train loss: 2.302, train acc: 9.000, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[6,   150] train loss: 2.348, train acc: 8.688, rand_train_acc: 10.906, train_true is False, gamma is 0.50\n",
      "[6,   200] train loss: 2.303, train acc: 8.031, rand_train_acc: 9.750, train_true is False, gamma is 0.50\n",
      "[6,   250] train loss: 2.303, train acc: 9.188, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[6,   300] train loss: 2.349, train acc: 8.594, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[6,   350] train loss: 2.303, train acc: 8.812, rand_train_acc: 9.906, train_true is False, gamma is 0.50\n",
      "[6,   400] train loss: 2.303, train acc: 8.391, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[7,    50] train loss: 2.349, train acc: 8.314, rand_train_acc: 10.353, train_true is False, gamma is 0.50\n",
      "[7,   100] train loss: 2.303, train acc: 8.766, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[7,   150] train loss: 2.303, train acc: 8.656, rand_train_acc: 10.266, train_true is False, gamma is 0.50\n",
      "[7,   200] train loss: 2.348, train acc: 8.656, rand_train_acc: 10.109, train_true is False, gamma is 0.50\n",
      "[7,   250] train loss: 2.303, train acc: 8.422, rand_train_acc: 10.141, train_true is False, gamma is 0.50\n",
      "[7,   300] train loss: 2.303, train acc: 8.609, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[7,   350] train loss: 2.349, train acc: 8.703, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[7,   400] train loss: 2.303, train acc: 8.234, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[8,    50] train loss: 2.349, train acc: 8.745, rand_train_acc: 10.265, train_true is False, gamma is 0.50\n",
      "[8,   100] train loss: 2.303, train acc: 8.328, rand_train_acc: 10.406, train_true is False, gamma is 0.50\n",
      "[8,   150] train loss: 2.303, train acc: 8.594, rand_train_acc: 9.812, train_true is False, gamma is 0.50\n",
      "[8,   200] train loss: 2.349, train acc: 8.453, rand_train_acc: 10.375, train_true is False, gamma is 0.50\n",
      "[8,   250] train loss: 2.304, train acc: 8.672, rand_train_acc: 9.375, train_true is False, gamma is 0.50\n",
      "[8,   300] train loss: 2.303, train acc: 8.547, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[8,   350] train loss: 2.349, train acc: 8.531, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[8,   400] train loss: 2.303, train acc: 9.047, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[9,    50] train loss: 2.303, train acc: 8.353, rand_train_acc: 9.745, train_true is False, gamma is 0.50\n",
      "[9,   100] train loss: 2.349, train acc: 8.953, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[9,   150] train loss: 2.303, train acc: 8.828, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[9,   200] train loss: 2.303, train acc: 8.078, rand_train_acc: 10.328, train_true is False, gamma is 0.50\n",
      "[9,   250] train loss: 2.349, train acc: 8.719, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[9,   300] train loss: 2.303, train acc: 8.719, rand_train_acc: 9.984, train_true is False, gamma is 0.50\n",
      "[9,   350] train loss: 2.303, train acc: 8.375, rand_train_acc: 9.703, train_true is False, gamma is 0.50\n",
      "[9,   400] train loss: 2.349, train acc: 8.891, rand_train_acc: 9.391, train_true is False, gamma is 0.50\n",
      "[10,    50] train loss: 2.303, train acc: 8.343, rand_train_acc: 9.696, train_true is False, gamma is 0.50\n",
      "[10,   100] train loss: 2.349, train acc: 8.344, rand_train_acc: 10.656, train_true is False, gamma is 0.50\n",
      "[10,   150] train loss: 2.303, train acc: 8.469, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[10,   200] train loss: 2.303, train acc: 8.625, rand_train_acc: 9.844, train_true is False, gamma is 0.50\n",
      "[10,   250] train loss: 2.349, train acc: 9.062, rand_train_acc: 10.203, train_true is False, gamma is 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,   300] train loss: 2.303, train acc: 8.469, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[10,   350] train loss: 2.303, train acc: 8.922, rand_train_acc: 10.516, train_true is False, gamma is 0.50\n",
      "[10,   400] train loss: 2.349, train acc: 8.797, rand_train_acc: 9.891, train_true is False, gamma is 0.50\n",
      "[11,    50] train loss: 2.302, train acc: 8.255, rand_train_acc: 10.373, train_true is False, gamma is 0.50\n",
      "[11,   100] train loss: 2.303, train acc: 8.594, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[11,   150] train loss: 2.349, train acc: 9.000, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[11,   200] train loss: 2.303, train acc: 8.484, rand_train_acc: 9.984, train_true is False, gamma is 0.50\n",
      "[11,   250] train loss: 2.303, train acc: 8.266, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[11,   300] train loss: 2.349, train acc: 8.984, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[11,   350] train loss: 2.303, train acc: 8.641, rand_train_acc: 9.797, train_true is False, gamma is 0.50\n",
      "[11,   400] train loss: 2.303, train acc: 8.547, rand_train_acc: 9.781, train_true is False, gamma is 0.50\n",
      "[12,    50] train loss: 2.303, train acc: 8.696, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[12,   100] train loss: 2.302, train acc: 8.156, rand_train_acc: 10.516, train_true is False, gamma is 0.50\n",
      "[12,   150] train loss: 2.349, train acc: 8.141, rand_train_acc: 9.953, train_true is False, gamma is 0.50\n",
      "[12,   200] train loss: 2.303, train acc: 8.703, rand_train_acc: 9.438, train_true is False, gamma is 0.50\n",
      "[12,   250] train loss: 2.303, train acc: 8.891, rand_train_acc: 9.422, train_true is False, gamma is 0.50\n",
      "[12,   300] train loss: 2.349, train acc: 8.453, rand_train_acc: 10.328, train_true is False, gamma is 0.50\n",
      "[12,   350] train loss: 2.303, train acc: 8.266, rand_train_acc: 10.078, train_true is False, gamma is 0.50\n",
      "[12,   400] train loss: 2.303, train acc: 8.719, rand_train_acc: 10.062, train_true is False, gamma is 0.50\n",
      "[13,    50] train loss: 2.349, train acc: 8.853, rand_train_acc: 10.657, train_true is False, gamma is 0.50\n",
      "[13,   100] train loss: 2.303, train acc: 8.250, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[13,   150] train loss: 2.303, train acc: 9.031, rand_train_acc: 10.875, train_true is False, gamma is 0.50\n",
      "[13,   200] train loss: 2.348, train acc: 8.562, rand_train_acc: 10.812, train_true is False, gamma is 0.50\n",
      "[13,   250] train loss: 2.303, train acc: 8.266, rand_train_acc: 10.312, train_true is False, gamma is 0.50\n",
      "[13,   300] train loss: 2.302, train acc: 8.375, rand_train_acc: 10.703, train_true is False, gamma is 0.50\n",
      "[13,   350] train loss: 2.349, train acc: 9.281, rand_train_acc: 10.250, train_true is False, gamma is 0.50\n",
      "[13,   400] train loss: 2.303, train acc: 8.859, rand_train_acc: 10.656, train_true is False, gamma is 0.50\n",
      "[14,    50] train loss: 2.349, train acc: 8.402, rand_train_acc: 10.431, train_true is False, gamma is 0.50\n",
      "[14,   100] train loss: 2.303, train acc: 8.484, rand_train_acc: 9.875, train_true is False, gamma is 0.50\n",
      "[14,   150] train loss: 2.303, train acc: 8.438, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[14,   200] train loss: 2.350, train acc: 8.578, rand_train_acc: 10.047, train_true is False, gamma is 0.50\n",
      "[14,   250] train loss: 2.303, train acc: 8.844, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[14,   300] train loss: 2.303, train acc: 8.547, rand_train_acc: 9.891, train_true is False, gamma is 0.50\n",
      "[14,   350] train loss: 2.349, train acc: 8.266, rand_train_acc: 10.031, train_true is False, gamma is 0.50\n",
      "[14,   400] train loss: 2.303, train acc: 9.031, rand_train_acc: 10.266, train_true is False, gamma is 0.50\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(14):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        optimizer_R.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        if train_true:\n",
    "            loss = 2*model.loss_fn(outputs_true, labels) - gamma*model.loss_fn(outputs_rand, rand_labels)\n",
    "        else:\n",
    "            loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if train_true:\n",
    "            optimizer_T.step()\n",
    "        else:\n",
    "            optimizer_R.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] train loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 2 and train_true:\n",
    "            train_true=False\n",
    "            iter_counter = 0\n",
    "            \n",
    "        if iter_counter >= 150 and not train_true:\n",
    "            train_true=True\n",
    "            iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 500 == 499:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "difficult-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_T.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "third-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_R.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aerial-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "circular-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.cuda()\n",
    "labels=labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "operating-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward + backward + optimize\n",
    "outputs = model(inputs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "undefined-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2884e-02, -1.3882e-02,  1.2817e-02, -9.4555e-04,  3.0092e-02,\n",
       "         1.0486e-02, -3.0941e-02, -4.4655e-03, -3.3369e-02, -1.8193e-02,\n",
       "        -9.4091e-03, -1.3727e-03, -2.6099e-02, -5.4899e-03,  2.5781e-02,\n",
       "         1.6908e-02,  1.7572e-02,  3.4798e-03,  2.0891e-02,  2.8016e-02,\n",
       "        -2.0708e-02,  2.2318e-03, -2.9543e-02,  2.4213e-02, -6.9216e-03,\n",
       "        -1.2713e-02, -3.1164e-02, -3.1157e-02, -2.5164e-02,  1.8522e-02,\n",
       "        -2.3435e-02,  7.9300e-03, -2.5209e-02, -2.6475e-02, -3.1741e-02,\n",
       "        -1.6724e-02, -8.0129e-03,  1.8957e-02, -7.8422e-04, -1.4845e-02,\n",
       "        -1.9399e-02,  3.2636e-02,  3.7357e-03, -5.1018e-03, -1.9835e-02,\n",
       "         6.9270e-03, -1.9509e-02, -3.5025e-02,  2.1834e-02, -1.0204e-02,\n",
       "        -2.1467e-02,  3.0699e-02, -1.5840e-02,  1.4424e-02,  2.7146e-02,\n",
       "         1.8442e-02, -1.6263e-02, -1.9906e-03,  3.1360e-02,  2.9571e-02,\n",
       "         9.1939e-03,  2.5311e-02, -3.4110e-03,  1.5451e-02, -9.0994e-03,\n",
       "        -1.9657e-03,  3.5384e-02,  5.8600e-03,  2.1896e-02,  1.5586e-02,\n",
       "         1.0265e-02,  1.7733e-02, -2.3972e-02,  2.8380e-02, -5.0356e-03,\n",
       "         3.3850e-02, -2.4760e-02,  4.3374e-03, -1.6485e-02, -1.0450e-02,\n",
       "        -2.8341e-02, -2.2571e-02, -2.1294e-02, -2.6533e-02, -2.7138e-02,\n",
       "        -2.0946e-02, -3.4908e-02, -2.0826e-02,  1.5211e-02,  2.6404e-02,\n",
       "        -1.2574e-02,  3.6687e-02,  1.8213e-04, -9.4334e-03, -1.5012e-02,\n",
       "        -1.9071e-02,  5.4779e-03,  4.4809e-02, -3.0874e-03, -1.3790e-03,\n",
       "         1.7485e-02,  1.1420e-02,  2.9561e-02,  1.1621e-02, -3.4348e-03,\n",
       "        -2.0989e-02,  9.8980e-03, -5.3693e-03,  2.6606e-02,  3.3313e-02,\n",
       "         2.7127e-02, -2.1401e-02, -1.9578e-02,  1.9718e-02,  5.0845e-03,\n",
       "         1.7148e-02, -2.8259e-02,  2.9030e-02, -2.8951e-02,  2.6423e-02,\n",
       "        -1.7914e-02,  4.0643e-02,  6.1854e-02,  6.6175e-02,  3.8729e-02,\n",
       "         6.1134e-02,  8.1488e-02,  1.2576e-01,  7.5407e-02,  6.1255e-02,\n",
       "         8.9983e-02,  3.8441e-02,  2.9049e-02, -5.1292e-03,  2.0359e-02,\n",
       "        -1.1454e-02, -2.9106e-02, -7.9200e-03,  2.5389e-02, -1.5767e-02,\n",
       "        -3.3170e-02, -1.8811e-02, -7.5758e-03, -1.9156e-02,  2.7563e-02,\n",
       "        -1.5685e-02,  2.5324e-03,  4.7700e-02,  1.1461e-02,  4.0845e-02,\n",
       "         9.0134e-02,  1.1514e-01,  1.1503e-01,  1.9884e-01,  1.7506e-01,\n",
       "         1.7148e-01,  1.9920e-01,  1.4172e-01,  1.4465e-01,  9.9551e-02,\n",
       "         4.9254e-02,  1.3044e-02, -1.3709e-03, -8.3454e-03, -2.7711e-02,\n",
       "         1.3518e-02, -3.2279e-02, -3.5391e-02, -2.7611e-02,  2.5671e-02,\n",
       "         3.2533e-02,  3.5916e-02,  5.4471e-03, -2.5238e-02,  9.0869e-03,\n",
       "         3.5679e-02,  5.7129e-02,  9.7250e-02,  1.5300e-01,  1.5252e-01,\n",
       "         2.3016e-01,  2.4447e-01,  2.6285e-01,  2.5742e-01,  2.3147e-01,\n",
       "         2.4708e-01,  2.0812e-01,  1.3212e-01,  1.2016e-01,  7.0421e-02,\n",
       "         1.7532e-02,  4.2067e-02, -1.6189e-02,  3.8099e-02,  2.6228e-02,\n",
       "         3.5046e-02,  2.0804e-02, -9.9642e-04, -1.9228e-02,  9.6409e-04,\n",
       "         3.4221e-03,  2.4406e-02,  2.6205e-02,  1.7060e-02,  9.5644e-02,\n",
       "         1.0791e-01,  1.4041e-01,  2.2440e-01,  2.7381e-01,  2.9350e-01,\n",
       "         2.5835e-01,  2.6570e-01,  3.1112e-01,  3.0067e-01,  2.6668e-01,\n",
       "         2.2288e-01,  1.3208e-01,  1.1600e-01,  9.3271e-02,  8.8072e-03,\n",
       "         2.8243e-02,  2.8530e-02,  9.6180e-03,  2.8271e-02, -2.1591e-02,\n",
       "         2.1996e-02, -8.3542e-03, -2.3423e-02, -3.4193e-03, -1.0945e-02,\n",
       "         5.9785e-02,  8.0318e-02,  7.6573e-02,  1.1998e-01,  1.6750e-01,\n",
       "         2.5046e-01,  2.6601e-01,  2.3183e-01,  2.2208e-01,  2.3197e-01,\n",
       "         2.3781e-01,  2.5483e-01,  2.1242e-01,  2.4040e-01,  1.3872e-01,\n",
       "         1.2508e-01,  8.4037e-02,  5.7034e-02,  1.6032e-02, -1.1672e-02,\n",
       "         5.2133e-03, -2.7686e-02,  9.0459e-03, -6.1375e-03,  2.3039e-02,\n",
       "         1.0288e-02,  4.4662e-03,  3.8531e-02,  1.8345e-02,  6.6451e-02,\n",
       "         9.7383e-02,  1.3316e-01,  2.0189e-01,  2.4862e-01,  2.2844e-01,\n",
       "         2.2975e-01,  1.5369e-01,  1.7140e-01,  1.9145e-01,  2.2817e-01,\n",
       "         2.4802e-01,  2.2606e-01,  1.6345e-01,  1.0480e-01,  1.0173e-01,\n",
       "         4.9055e-03, -8.9843e-03,  3.0932e-02,  2.0938e-02,  8.8176e-03,\n",
       "         1.1681e-02, -4.7956e-03,  2.4792e-02,  1.9055e-02, -1.4017e-02,\n",
       "         4.6792e-02,  4.8835e-02,  6.3560e-02,  1.4749e-01,  1.7201e-01,\n",
       "         1.8438e-01,  2.0584e-01,  1.8604e-01,  1.7557e-01,  1.0688e-01,\n",
       "         1.6242e-01,  1.7883e-01,  2.0172e-01,  2.2872e-01,  1.7429e-01,\n",
       "         1.3458e-01,  1.3336e-01,  4.8138e-02,  4.4585e-02, -6.5535e-03,\n",
       "        -1.3606e-03,  9.2352e-03,  2.9716e-02, -2.0878e-02, -1.0357e-02,\n",
       "        -3.2496e-03, -1.4686e-02,  3.5649e-02,  5.0879e-03,  2.2987e-02,\n",
       "         1.0155e-01,  1.4782e-01,  1.7915e-01,  2.4666e-01,  2.2298e-01,\n",
       "         1.3927e-01,  1.4164e-01,  1.4339e-01,  1.7116e-01,  2.1423e-01,\n",
       "         2.2922e-01,  2.5246e-01,  1.7834e-01,  1.2574e-01,  1.0084e-01,\n",
       "         3.6485e-02,  3.2690e-02, -8.2795e-03,  3.4181e-02, -1.1062e-02,\n",
       "         1.6114e-02, -2.7419e-02, -1.2033e-02, -1.8510e-02, -2.6403e-02,\n",
       "         2.9225e-02,  3.3391e-02,  5.2312e-02,  9.8856e-02,  1.5458e-01,\n",
       "         2.3044e-01,  1.9960e-01,  2.2705e-01,  2.0142e-01,  1.3366e-01,\n",
       "         2.0040e-01,  2.2422e-01,  2.1054e-01,  2.2674e-01,  2.5544e-01,\n",
       "         1.5308e-01,  1.1568e-01,  8.9197e-02,  3.8089e-02,  7.0228e-03,\n",
       "        -1.8885e-03,  2.1083e-02,  1.4894e-02,  1.4905e-02, -2.9409e-02,\n",
       "         1.1352e-02, -3.2029e-02,  3.2620e-02, -1.2830e-02,  2.3567e-02,\n",
       "         2.8290e-02,  1.2819e-01,  1.7121e-01,  2.4165e-01,  2.0307e-01,\n",
       "         2.0858e-01,  2.3412e-01,  1.9025e-01,  2.4330e-01,  2.6503e-01,\n",
       "         2.9404e-01,  2.8451e-01,  2.6709e-01,  1.8190e-01,  9.1033e-02,\n",
       "         5.6288e-02,  7.4092e-02,  4.4070e-03,  1.2407e-03, -1.0794e-02,\n",
       "         9.4066e-03, -3.3682e-02, -3.2979e-03, -1.4312e-02, -1.9413e-02,\n",
       "         3.5255e-02,  2.4938e-02,  1.7324e-02,  6.6222e-02,  1.1894e-01,\n",
       "         1.9130e-01,  2.2154e-01,  2.1492e-01,  2.5693e-01,  2.3142e-01,\n",
       "         2.2287e-01,  2.9948e-01,  3.2014e-01,  3.0344e-01,  2.7551e-01,\n",
       "         2.7201e-01,  1.4470e-01,  1.3153e-01,  1.2342e-01,  4.8963e-02,\n",
       "         4.0702e-02, -3.2417e-03,  1.1478e-02, -2.6078e-02, -1.7275e-02,\n",
       "        -1.5090e-02,  1.6576e-02,  3.0728e-02,  3.1202e-02, -2.2845e-02,\n",
       "         6.0232e-02,  6.7800e-02,  1.4948e-01,  1.8380e-01,  1.7983e-01,\n",
       "         2.0039e-01,  2.5245e-01,  2.2770e-01,  2.7806e-01,  3.0634e-01,\n",
       "         3.1751e-01,  2.7548e-01,  2.8196e-01,  2.5708e-01,  1.5516e-01,\n",
       "         1.6769e-01,  1.1269e-01,  9.6054e-02, -6.2645e-04, -4.1922e-03,\n",
       "         2.2091e-02, -2.4148e-03, -2.4010e-02,  2.4830e-02, -1.3906e-02,\n",
       "        -2.8750e-02,  8.0576e-03,  3.3594e-02,  2.2575e-02,  7.6231e-02,\n",
       "         1.4375e-01,  1.7522e-01,  1.5707e-01,  2.1285e-01,  1.9806e-01,\n",
       "         1.9096e-01,  2.6806e-01,  2.6160e-01,  2.5505e-01,  2.2815e-01,\n",
       "         2.3284e-01,  2.0095e-01,  1.9054e-01,  1.1142e-01,  1.0536e-01,\n",
       "         4.8164e-02,  6.6405e-02,  5.8272e-03, -8.8552e-03,  1.8640e-03,\n",
       "        -2.6529e-02,  2.0261e-02, -2.5340e-02,  6.2511e-03,  3.6480e-02,\n",
       "         2.7083e-04,  3.8031e-02,  6.2002e-02,  9.8453e-02,  1.4240e-01,\n",
       "         1.5583e-01,  1.3888e-01,  1.8268e-01,  1.7472e-01,  2.2030e-01,\n",
       "         1.9410e-01,  2.2214e-01,  2.1464e-01,  2.3722e-01,  2.3499e-01,\n",
       "         1.4907e-01,  1.3170e-01,  9.8939e-02,  2.8024e-02,  1.0060e-02,\n",
       "         4.4053e-02,  1.3242e-02,  2.0152e-02,  4.1930e-03,  8.2026e-03,\n",
       "        -2.5501e-02,  1.0072e-02, -5.7733e-03, -5.8757e-03,  4.5537e-02,\n",
       "         1.0578e-01,  8.3357e-02,  1.5174e-01,  1.3844e-01,  1.8932e-01,\n",
       "         1.4799e-01,  1.9454e-01,  1.5700e-01,  2.1375e-01,  1.9135e-01,\n",
       "         2.3593e-01,  2.3245e-01,  2.2383e-01,  1.9449e-01,  1.5704e-01,\n",
       "         1.1850e-01,  1.9951e-02,  9.2735e-03,  3.4823e-03, -2.4734e-02,\n",
       "         2.8402e-02,  2.4162e-02,  1.6020e-03, -1.3351e-02, -9.6725e-03,\n",
       "         6.6288e-03,  5.4220e-02,  5.3146e-02,  1.1513e-01,  8.8517e-02,\n",
       "         1.2314e-01,  1.5451e-01,  1.9890e-01,  1.8098e-01,  1.7497e-01,\n",
       "         2.1094e-01,  2.4020e-01,  2.1581e-01,  2.7953e-01,  2.7451e-01,\n",
       "         2.4300e-01,  1.3252e-01,  1.4557e-01,  6.0540e-02,  1.7945e-02,\n",
       "         1.1317e-03,  1.6313e-03, -3.0987e-02,  2.4578e-02, -2.5521e-02,\n",
       "        -3.4756e-02,  2.2179e-02, -1.8346e-02, -2.3189e-02,  3.4556e-02,\n",
       "         3.5975e-02,  5.8663e-02,  1.1252e-01,  1.2335e-01,  1.9024e-01,\n",
       "         1.9419e-01,  2.1911e-01,  2.0110e-01,  2.5562e-01,  2.2402e-01,\n",
       "         2.8690e-01,  2.3407e-01,  2.0995e-01,  2.1244e-01,  1.1434e-01,\n",
       "         8.6321e-02,  6.3378e-02,  3.0940e-02, -8.7453e-03, -4.4276e-03,\n",
       "        -7.5191e-03, -2.2187e-04,  2.0041e-02,  2.1519e-02, -2.9158e-02,\n",
       "         2.0385e-02, -1.0647e-02,  6.0172e-03,  2.4720e-02,  9.4810e-02,\n",
       "         1.0444e-01,  1.4479e-01,  1.8223e-01,  2.4502e-01,  2.0255e-01,\n",
       "         2.7763e-01,  2.6654e-01,  2.6189e-01,  2.9559e-01,  2.6715e-01,\n",
       "         2.1967e-01,  1.3636e-01,  7.2077e-02,  3.1220e-02,  8.4719e-03,\n",
       "         4.1087e-02,  1.6623e-02, -1.1221e-02, -1.7789e-02,  9.4751e-03,\n",
       "         1.8360e-03, -6.6257e-03,  1.9612e-03, -2.2350e-02,  2.4718e-02,\n",
       "         2.5182e-02,  5.9382e-02,  7.8856e-02,  9.0579e-02,  1.0520e-01,\n",
       "         1.3648e-01,  1.8201e-01,  2.5428e-01,  2.2304e-01,  2.8743e-01,\n",
       "         2.7544e-01,  2.5703e-01,  2.1410e-01,  1.3874e-01,  1.2044e-01,\n",
       "         9.3378e-02,  1.9828e-02, -6.1082e-03,  3.0858e-02,  3.9098e-02,\n",
       "        -1.3526e-02, -7.0127e-04,  1.6709e-02, -2.4866e-02, -3.3199e-02,\n",
       "        -2.8145e-02,  1.6082e-02,  1.6893e-02,  2.6979e-02, -2.3784e-02,\n",
       "         5.5999e-02,  6.5747e-02,  9.3843e-02,  1.5396e-01,  1.7477e-01,\n",
       "         2.1577e-01,  2.2685e-01,  2.1812e-01,  1.8490e-01,  1.3291e-01,\n",
       "         1.3268e-01,  1.0876e-01,  8.9720e-02,  2.1060e-02,  5.2318e-03,\n",
       "         4.3518e-02,  5.4727e-03, -1.2776e-02, -3.3833e-02, -2.1199e-02,\n",
       "         2.8649e-03,  1.5378e-03, -2.4573e-02,  2.8207e-02,  1.1156e-02,\n",
       "        -2.9890e-02, -1.9463e-02, -1.2154e-02, -2.0204e-02,  7.9792e-03,\n",
       "         2.5087e-02,  8.5300e-02,  6.2183e-02,  1.0905e-01,  1.2586e-01,\n",
       "         8.5511e-02,  1.1186e-01,  6.3445e-02,  3.1782e-02,  6.6133e-02,\n",
       "         2.0600e-02, -3.3661e-03,  9.8258e-03, -1.5136e-02, -9.2055e-03,\n",
       "         9.2126e-03,  2.4190e-02, -1.7414e-02,  1.5555e-02, -1.1396e-02,\n",
       "        -3.1613e-02, -1.1567e-02, -3.4639e-02, -5.4074e-03, -2.1130e-03,\n",
       "         4.5193e-03,  6.0121e-03, -2.3894e-02,  1.3965e-03,  3.1658e-02,\n",
       "         3.3735e-02,  7.6846e-03,  5.9888e-02,  2.8450e-02,  5.8875e-02,\n",
       "         1.9772e-02,  4.8016e-02,  2.7834e-02,  3.1843e-03,  2.1675e-02,\n",
       "        -2.9227e-02,  3.4888e-02,  3.4767e-02,  3.2577e-02, -2.4136e-03,\n",
       "         1.9130e-02,  3.2476e-02,  1.0022e-02, -6.6460e-03,  3.5028e-02,\n",
       "         3.0023e-02, -7.6657e-03, -3.2230e-02, -1.6855e-02, -1.1828e-02,\n",
       "        -1.3365e-02,  2.7812e-02,  3.5644e-02,  1.9249e-02,  5.1958e-03,\n",
       "        -1.9827e-02,  1.6696e-02,  3.2883e-05, -2.2110e-02,  1.4900e-02,\n",
       "        -2.3437e-02,  3.1971e-02, -3.0510e-02, -1.4400e-02,  2.9201e-02,\n",
       "         1.0671e-02, -3.5944e-03, -8.6520e-04, -2.0098e-02,  6.4415e-03,\n",
       "        -1.0938e-02, -2.6381e-02,  3.4175e-02, -7.1739e-03, -6.3375e-03,\n",
       "        -1.9939e-02,  6.8746e-03, -1.4076e-03, -1.9152e-02,  2.7200e-02,\n",
       "        -3.2523e-02,  1.9043e-02, -3.5003e-02, -1.2307e-02,  1.8325e-02,\n",
       "        -1.8471e-02,  2.4720e-02, -8.9853e-03, -9.8895e-03,  1.3704e-02,\n",
       "         3.1210e-02, -1.4320e-02,  1.1539e-02,  2.0838e-02, -1.9883e-02,\n",
       "         1.7724e-02, -6.6096e-03, -3.5705e-02, -1.3835e-02], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc1.weight[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "latter-packaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 487.3576,  563.0782,    0.0000,  788.0320, 1285.5168,  260.9083,\n",
       "           0.0000,    0.0000,  899.2999,  262.7325,  766.0019,  916.5306,\n",
       "         551.2318,  562.9424,    0.0000,  522.7163,    0.0000,  656.5974,\n",
       "         278.9159,    0.0000,  150.0539,  949.6662,    0.0000,   40.0514,\n",
       "         235.3155,  322.2774,  338.2378,    0.0000,    0.0000,    0.0000,\n",
       "           0.0000,  622.0282,  898.8696,  274.3130,  704.2454,  542.8204,\n",
       "           0.0000,  286.3525,  535.9585,    0.0000,  472.4174,    0.0000,\n",
       "           0.0000,    0.0000,  935.0920,  518.3685,  858.5781,  425.1231,\n",
       "         436.8321,  267.9036,    0.0000,    0.0000,  151.2569,    0.0000,\n",
       "         280.9054,  469.7323,  854.7551,  328.7216,  236.1176,  259.5135,\n",
       "         531.8068,  663.5306,  566.5142,  292.0876,  760.9315,    0.0000,\n",
       "         430.0152,    0.0000,  224.1848,  204.6381,  123.6201,  201.6194,\n",
       "         706.4742,  550.0683,  927.8320,  613.0057,  740.3270,    0.0000,\n",
       "         404.3229,  769.3582,  624.0850,    0.0000,  364.5620,    0.0000,\n",
       "         174.9790,  261.5737,  470.0835,    0.0000,  846.4037,  254.2957,\n",
       "           0.0000,    0.0000,  169.9790,  133.4698,  866.0516,  452.4504,\n",
       "         957.0208,  284.0109,  922.8854,  617.7474], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "hearing-kentucky",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 10])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "religious-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23443.8906,  11664.0293, -30977.0000,  18269.4180, -30130.8613,\n",
       "        -35893.1562,  34281.3125,  15815.0801,  24913.7266,  15030.6855],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "known-jungle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "tamil-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0003, -0.0016,  0.0095, -0.0396, -0.0370,  0.0073, -0.0448,  0.0378,\n",
       "         0.0476,  0.0221], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc2.weight[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cellular-socket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100., 100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc2.weight.grad[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "considered-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature.fc2.weight.grad[0][0:10] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bottom-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_labels = torch.randint(0, 10, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sized-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "tropical-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_loss = 2*model.loss_fn(outputs, labels) - model.loss_fn(outputs, rand_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "encouraging-terrace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3295, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "southwest-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "binary-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_R.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "micro-renewal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0843, -0.9679, -0.9038, -1.0378, -1.0565, -0.9441, -1.0367, -0.9117,\n",
       "        -1.0202, -1.0202], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rand_classifier.weight[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "overall-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rand_classifier.weight.grad[0][0:10] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-humanity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-swing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tamil-heather",
   "metadata": {},
   "source": [
    "## analyze metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unusual-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "timely-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = h5py.File('exp/mnist_mnist_c/translate/_lenet_trainfirst_iterT9999_iterR0_iterRsb1_gam0.0_alf1.0_lrT0.1_lrR0.1_ep15_inittypeprev_fromfresh/perf_metrics.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "complex-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=torch.load('exp/mnist_mnist_c/fog/_lenet_randfirst_iterT0_iterR9999_iterRsb5_gam1.0_alf0.0_lrT0.1_lrR0.1_dgm0.98per300_ep15_csist_posr_inittypereinit_fromfresh/final_model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "incoming-roman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sunrise-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train_loss', 'train_acc', 'val_loss', 'val_acc', 'test_loss',\n",
       "       'test_acc'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file.attrs['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "joint-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_loss = np.array(input_file['train_loss'])\n",
    "orig_val_loss = np.array(input_file['val_loss'])\n",
    "orig_test_loss = np.array(input_file['test_loss'])\n",
    "\n",
    "orig_train_acc = np.array(input_file['train_acc'])\n",
    "orig_val_acc = np.array(input_file['val_acc'])\n",
    "orig_test_acc = np.array(input_file['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "resident-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "korean-alabama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc45df06550>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(orig_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "facial-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2932558 , 0.5998181 , 0.55874026, 0.46569228, 0.45049158,\n",
       "       0.4255676 , 0.3847248 , 0.40330774, 0.39204633, 0.37022448,\n",
       "       0.3361154 , 0.3330903 , 0.32348287, 0.33008718, 0.30198124,\n",
       "       0.31034064, 0.32422924, 0.28173184, 0.29986274, 0.31335452,\n",
       "       0.2884902 , 0.3192304 , 0.26953194, 0.27360663, 0.28837383,\n",
       "       0.28256062, 0.27020952, 0.27938017, 0.27500474, 0.28482935,\n",
       "       0.2830521 , 0.25364268, 0.26431122, 0.26609266, 0.2623072 ,\n",
       "       0.24247855, 0.31599626, 0.266008  , 0.25241718, 0.25159404,\n",
       "       0.25465158, 0.2381976 , 0.26936   , 0.2949132 , 0.22524305,\n",
       "       0.27760792, 0.27456635, 0.26015154, 0.2783044 , 0.28589192,\n",
       "       0.25260788, 0.29126576, 0.2745214 , 0.31174105, 0.25607827,\n",
       "       0.32937205, 0.25082582, 0.24957164, 0.27887645, 0.2519828 ,\n",
       "       0.31203738, 0.3072598 , 0.28016475, 0.26036236, 0.25688252,\n",
       "       0.2830533 , 0.2524959 , 0.2644796 , 0.28432667, 0.28638327,\n",
       "       0.26213753, 0.26806572, 0.3232631 , 0.33061832, 0.29632422,\n",
       "       0.2820536 , 0.26265258, 0.30279034, 0.26173815, 0.3282029 ,\n",
       "       0.2864587 , 0.42904487, 0.2815973 , 0.29689458, 0.29467326,\n",
       "       0.30724263, 0.28094035, 0.3637059 , 0.3184552 , 0.33295807,\n",
       "       0.33358172, 0.31642494, 0.33409432, 0.34634957, 0.33794174,\n",
       "       0.32045445, 0.29280528, 0.3427218 , 0.31327337, 0.3360047 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "federal-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0221, -0.0360,  0.0109,  ..., -0.0212,  0.0420,  0.0539],\n",
      "        [ 0.0665, -0.0150,  0.0654,  ..., -0.0893, -0.0182, -0.0902],\n",
      "        [-0.0695, -0.0110, -0.0557,  ...,  0.0798, -0.0765,  0.0625],\n",
      "        ...,\n",
      "        [ 0.0803, -0.0046,  0.0478,  ...,  0.0588, -0.0892,  0.0326],\n",
      "        [ 0.0360, -0.0322,  0.0301,  ..., -0.0137,  0.0948, -0.0082],\n",
      "        [-0.0652,  0.0347, -0.0570,  ...,  0.0282,  0.0767,  0.0577]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0222,  0.0851,  0.0923, -0.0091,  0.0839, -0.0649, -0.0209,  0.0281,\n",
      "        -0.0481,  0.0778,  0.0672, -0.0319, -0.0773, -0.0891, -0.0533,  0.0841,\n",
      "        -0.0069, -0.0616,  0.0576,  0.0581,  0.0954,  0.0769,  0.0092,  0.0759,\n",
      "        -0.0918,  0.0579,  0.0800,  0.0437,  0.0476, -0.0583, -0.0705, -0.0714,\n",
      "         0.0169, -0.0576, -0.0534,  0.0145, -0.0887,  0.0359,  0.0824,  0.0519,\n",
      "        -0.0280, -0.0057,  0.0309,  0.0348, -0.0592,  0.0321, -0.0852, -0.0133,\n",
      "         0.0979, -0.0963, -0.0224, -0.0611,  0.0155,  0.0056,  0.0318,  0.0551,\n",
      "         0.0506,  0.0152, -0.0219,  0.0947,  0.0988, -0.0040,  0.0595, -0.0066],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.6575e-02, -1.0808e-01,  4.6167e-02,  1.0485e-01, -3.0044e-02,\n",
      "          6.7750e-02,  9.5907e-02, -6.8777e-02,  6.6288e-03,  4.9151e-02,\n",
      "          1.0725e-01,  4.2803e-02,  7.0403e-02, -6.5237e-02, -6.1293e-02,\n",
      "         -8.7756e-02,  1.5457e-02,  3.4192e-02, -2.5773e-02,  4.4097e-02,\n",
      "         -1.1112e-01, -3.7409e-02, -1.0252e-01, -7.5074e-02, -8.3541e-02,\n",
      "          4.7447e-02, -7.0920e-02, -3.6282e-02, -1.1584e-02, -9.3290e-03,\n",
      "         -4.4752e-02,  8.3600e-02, -2.5280e-03, -1.1346e-01,  1.1906e-01,\n",
      "         -9.4776e-02,  6.9228e-02,  1.1617e-02,  6.6392e-02, -9.7941e-02,\n",
      "         -2.0744e-03,  8.9454e-02, -1.1041e-01,  4.6918e-02,  3.5047e-02,\n",
      "          6.8520e-02,  8.8885e-02, -4.5333e-03, -2.4838e-02,  4.3833e-02,\n",
      "         -4.7403e-02,  1.8122e-02, -6.8079e-02, -5.6318e-02,  6.9004e-02,\n",
      "          6.0737e-02,  5.5348e-02,  6.7002e-02, -9.2477e-02, -1.1033e-01,\n",
      "          3.1811e-02, -7.2289e-02, -4.2775e-02,  5.0864e-02],\n",
      "        [-1.1948e-01,  6.4471e-03, -6.3492e-02,  1.1945e-01,  1.8460e-02,\n",
      "          8.8857e-02, -6.1444e-02, -8.6840e-02, -4.8071e-02, -6.5189e-03,\n",
      "          7.3576e-02, -8.9497e-02,  1.1912e-02,  1.0299e-01,  3.4460e-02,\n",
      "          5.0041e-02,  9.8248e-02, -6.2688e-02,  3.4354e-03,  1.1167e-01,\n",
      "          1.0391e-01,  3.3293e-02,  9.6463e-02,  4.4208e-02,  3.9720e-02,\n",
      "          1.1367e-01,  6.1524e-02,  5.5389e-02,  1.4505e-02,  9.9189e-02,\n",
      "          7.1353e-03, -8.8291e-03,  1.2630e-02, -5.1931e-05,  4.1381e-03,\n",
      "         -1.1198e-01,  8.0621e-02,  1.2100e-01,  1.1428e-01,  8.0099e-02,\n",
      "         -1.8877e-02, -5.0360e-03,  5.0554e-02,  1.1514e-01,  1.1816e-01,\n",
      "         -5.0568e-02,  2.5400e-02, -1.5021e-02,  9.5656e-02,  1.1912e-01,\n",
      "         -5.0349e-02,  4.2794e-02, -7.4514e-02,  1.7114e-02, -4.2670e-02,\n",
      "         -9.8849e-02,  9.9601e-03, -5.2349e-02,  8.7421e-02,  1.1493e-02,\n",
      "         -2.7312e-02, -6.1639e-02,  9.1440e-02,  5.7263e-02],\n",
      "        [-8.4165e-02, -1.1078e-01, -2.4127e-02,  7.3987e-02,  8.5848e-02,\n",
      "          1.1301e-01, -4.4960e-02, -1.1557e-02,  1.7723e-02, -9.2291e-03,\n",
      "          1.0635e-02, -6.4254e-02, -5.3644e-02,  4.6602e-02, -1.0580e-01,\n",
      "          1.0097e-01,  4.2717e-03, -1.2272e-01, -2.4195e-02, -4.9948e-02,\n",
      "         -4.3465e-02,  1.0412e-01,  1.0372e-01,  7.4582e-02,  2.7625e-02,\n",
      "          1.0715e-01,  9.0963e-02,  6.7064e-02,  5.9771e-02,  1.2335e-01,\n",
      "          6.2224e-02,  5.0889e-02,  9.3285e-02, -5.7073e-02, -7.0932e-02,\n",
      "          1.1497e-01,  8.9755e-02,  1.2435e-01, -5.9816e-02, -8.8776e-02,\n",
      "          8.0313e-02, -2.4592e-02,  2.7828e-02, -4.4836e-02,  1.2291e-01,\n",
      "          1.0714e-01, -7.2275e-02,  1.2939e-02, -3.1522e-02,  6.9489e-02,\n",
      "         -1.4506e-02, -3.0159e-02, -1.0149e-02, -7.1991e-02,  6.3593e-02,\n",
      "         -9.6701e-02,  1.1115e-01, -8.5808e-02, -8.0561e-02, -1.1861e-01,\n",
      "         -1.0384e-01, -1.1562e-02, -3.7661e-02,  1.0981e-01],\n",
      "        [-1.5383e-02, -6.0077e-02,  6.2827e-02, -1.1918e-01,  9.5808e-02,\n",
      "         -1.0628e-01, -1.2620e-02, -7.4846e-02, -1.1238e-02, -9.6749e-02,\n",
      "          3.2847e-02, -2.3793e-02, -1.0687e-01,  1.4182e-02,  1.1673e-01,\n",
      "         -8.1835e-02, -5.1771e-02,  5.3018e-02,  1.1383e-01,  5.7844e-03,\n",
      "          2.8116e-02,  6.3615e-02,  8.6333e-02,  1.0175e-01, -5.2125e-02,\n",
      "          8.2585e-02,  8.9023e-04,  4.4201e-02, -4.8408e-03, -9.4088e-02,\n",
      "         -9.6292e-02,  4.9564e-02, -1.7625e-02, -3.2290e-02,  2.3012e-02,\n",
      "          9.6548e-02,  1.2050e-01,  1.0140e-01,  3.4147e-02, -5.0188e-02,\n",
      "          1.1150e-01,  2.6993e-02,  2.3323e-02, -1.0738e-01,  8.9899e-02,\n",
      "         -3.5148e-02, -1.1609e-01,  6.0460e-02,  1.1613e-01, -9.5348e-02,\n",
      "          3.0417e-02,  9.4739e-03,  4.2226e-02,  2.6851e-02,  7.3609e-02,\n",
      "          7.2526e-02, -1.1379e-01, -1.1908e-01,  6.5237e-02, -6.9752e-02,\n",
      "         -1.0313e-01, -1.2428e-01,  1.2654e-02, -4.3012e-02],\n",
      "        [-1.1060e-01, -5.0686e-02,  5.8844e-02,  5.2066e-02, -3.8401e-02,\n",
      "         -1.1786e-01,  9.4389e-02,  2.1601e-02, -1.0866e-01, -4.3821e-02,\n",
      "         -2.0057e-02, -9.2091e-02,  8.7735e-02, -5.2898e-02, -1.1147e-01,\n",
      "         -3.7936e-02,  1.1843e-01,  1.0011e-01,  6.8120e-02, -1.0521e-01,\n",
      "         -8.8204e-02, -4.0741e-02,  3.1808e-02, -2.4897e-02, -1.2047e-01,\n",
      "          1.4150e-02,  8.5818e-02, -8.0516e-02,  8.8159e-02,  8.3913e-02,\n",
      "         -9.4422e-02, -9.0994e-02, -2.9480e-02, -1.2484e-01,  7.8499e-02,\n",
      "          7.0725e-02,  1.0394e-01,  7.7222e-02, -7.6432e-02,  1.0958e-01,\n",
      "         -6.5804e-04,  2.7395e-02, -9.0804e-02, -1.0209e-01, -3.3178e-02,\n",
      "          7.1952e-02, -6.6066e-03,  1.2254e-01, -3.2641e-02, -9.2913e-02,\n",
      "         -3.0994e-02,  7.5836e-02, -2.0285e-02, -2.9888e-02,  3.3378e-02,\n",
      "          8.8497e-02, -6.9007e-03, -1.1423e-01, -1.0269e-01,  4.8789e-02,\n",
      "         -1.1113e-01, -4.2086e-03,  9.3572e-02,  6.1892e-02],\n",
      "        [-1.1672e-01, -7.5022e-02, -7.1532e-02,  6.3117e-02,  6.2968e-02,\n",
      "         -1.0232e-01,  1.1933e-01,  1.0419e-01,  3.4613e-02,  7.3627e-02,\n",
      "          5.6548e-02, -8.0318e-02, -8.8169e-02, -9.7636e-02, -4.3137e-02,\n",
      "         -3.4727e-02, -8.7409e-02, -1.1591e-01,  7.5787e-02,  6.9726e-02,\n",
      "         -7.2385e-02, -6.1877e-02,  4.0073e-02,  4.4572e-03,  9.9541e-03,\n",
      "         -9.1344e-02,  4.0480e-02, -3.5241e-02, -7.2469e-02, -2.6605e-02,\n",
      "          9.6135e-02, -6.5656e-02,  2.2220e-02,  4.3922e-02, -1.1571e-01,\n",
      "          1.1524e-01,  7.7184e-02,  1.5373e-02, -8.5523e-03, -1.0290e-01,\n",
      "          8.8076e-02,  1.1532e-02,  1.0051e-01, -1.0044e-01,  7.1008e-02,\n",
      "         -1.1092e-01,  1.1049e-01,  8.9371e-04,  6.9446e-02, -5.5711e-02,\n",
      "         -1.2431e-01,  1.0424e-01,  3.5708e-02, -7.5431e-02,  5.1651e-02,\n",
      "         -1.1870e-01, -8.1866e-02,  3.8427e-02, -8.5371e-02,  7.4192e-03,\n",
      "         -1.1672e-01,  6.5952e-02,  9.6255e-03,  7.4691e-02],\n",
      "        [ 9.0306e-02,  1.0764e-01,  5.3583e-02, -1.8727e-02, -1.9266e-02,\n",
      "          3.0659e-02,  3.1990e-02, -5.9507e-02, -1.0910e-01, -1.0027e-01,\n",
      "          1.2044e-01,  2.0924e-02, -1.0912e-01, -7.0726e-02, -9.9637e-02,\n",
      "         -3.1363e-02, -1.2451e-01, -1.6211e-02, -1.2218e-01, -1.3561e-02,\n",
      "          1.2056e-01, -6.9788e-02, -6.8101e-04, -5.7320e-02,  6.3618e-02,\n",
      "         -9.2986e-02,  2.7948e-02, -6.9975e-02, -9.4208e-02, -3.3570e-03,\n",
      "          8.1904e-02, -9.9990e-04, -3.1081e-02,  3.4172e-02,  1.2462e-01,\n",
      "          4.9244e-02,  6.9898e-02,  6.7886e-02,  9.3501e-02, -6.5014e-02,\n",
      "         -1.0155e-01,  5.8316e-03, -1.2283e-01, -7.7047e-02, -6.2988e-02,\n",
      "          8.8582e-02, -1.0955e-03,  1.2059e-01,  4.6639e-02,  3.3758e-02,\n",
      "          4.0184e-02, -1.1149e-01, -9.9887e-02, -3.0445e-02,  7.9572e-04,\n",
      "          1.3239e-02,  1.0717e-01, -7.1033e-02, -5.9157e-02,  9.9143e-02,\n",
      "          5.6433e-02,  3.7645e-02, -5.7630e-02, -4.5639e-02],\n",
      "        [ 1.7861e-02,  9.7753e-02, -6.9853e-02, -1.1093e-01, -1.2359e-02,\n",
      "         -1.0742e-01, -8.6719e-02, -1.1245e-01, -3.5756e-02, -3.9553e-02,\n",
      "          5.2202e-02,  3.2802e-02,  7.4693e-02, -5.0304e-02,  3.1501e-02,\n",
      "          1.5815e-02,  8.5956e-02, -8.2060e-02,  9.1052e-02, -1.2061e-01,\n",
      "          2.5521e-02,  9.3024e-02,  8.7305e-02,  6.8984e-03, -7.6672e-02,\n",
      "          3.1732e-02,  7.6643e-02, -8.9088e-02, -7.1529e-02,  6.1536e-02,\n",
      "         -1.1797e-01, -8.3982e-02,  6.3919e-02, -1.1150e-01,  9.5439e-02,\n",
      "          2.8763e-02, -1.1156e-01,  2.7775e-02,  1.2011e-01, -5.4394e-02,\n",
      "         -6.5370e-03,  1.0404e-01,  8.7194e-02,  3.0801e-02, -1.5908e-03,\n",
      "         -1.2289e-01,  1.1755e-01,  2.9020e-02, -6.2291e-02, -8.1925e-02,\n",
      "          8.0145e-02, -1.1661e-01,  4.5727e-02,  3.6067e-02, -6.0620e-02,\n",
      "         -5.4888e-02, -1.0206e-01,  3.7062e-02, -1.2101e-01,  8.9998e-02,\n",
      "         -4.0297e-02,  9.3349e-02,  9.3537e-02,  4.7313e-02],\n",
      "        [-1.0989e-01, -5.3193e-04, -3.5845e-02,  3.6406e-02, -3.8764e-02,\n",
      "         -4.6926e-02,  1.6343e-02, -7.7395e-02, -1.2332e-01,  2.2223e-02,\n",
      "          8.4255e-02, -2.3238e-02, -7.0092e-02, -5.9351e-02,  8.1196e-02,\n",
      "         -5.3003e-02,  1.1626e-02,  1.9048e-02,  3.0114e-02, -7.9192e-02,\n",
      "          3.7802e-02, -7.4874e-02,  5.6861e-02, -6.9898e-03,  5.4256e-02,\n",
      "          6.2185e-02,  1.9039e-02,  1.9571e-02, -2.8476e-02,  4.2168e-02,\n",
      "         -4.6004e-02,  7.6543e-02,  1.2367e-01, -4.5582e-02,  1.0827e-01,\n",
      "          9.1548e-02, -3.9329e-02, -1.0600e-01, -7.4555e-02, -9.3825e-02,\n",
      "          2.2446e-02,  1.1572e-02,  8.8195e-02, -3.3341e-02,  8.6269e-02,\n",
      "          1.6510e-02, -1.3533e-02,  1.0030e-01,  4.0669e-02,  1.1274e-01,\n",
      "         -8.0441e-02,  1.2383e-01, -6.8987e-02,  6.2171e-02,  5.6871e-02,\n",
      "          1.1893e-01, -1.2474e-01, -6.3474e-02, -7.8439e-02, -8.8227e-02,\n",
      "          7.3084e-02,  1.1437e-01, -4.0027e-02,  1.6350e-02],\n",
      "        [ 1.0063e-01,  8.1345e-02, -1.0599e-01, -7.3764e-02, -7.7358e-02,\n",
      "          6.3507e-02, -1.1153e-01, -1.0871e-01, -1.6447e-02,  6.6172e-02,\n",
      "         -8.2711e-02,  2.6162e-02,  9.3967e-02, -1.1600e-01, -6.2932e-02,\n",
      "         -2.4336e-02, -1.2135e-01, -1.1809e-01, -6.0606e-02,  2.1781e-02,\n",
      "          5.3734e-02, -5.1631e-02, -7.2325e-02,  1.9122e-02, -8.2675e-02,\n",
      "         -2.9403e-02, -1.0040e-01, -1.2249e-01,  2.5951e-02,  8.1800e-03,\n",
      "         -2.0597e-02,  2.7047e-02, -8.8632e-02, -1.2403e-01, -7.1038e-02,\n",
      "          2.1594e-03, -5.1528e-02,  4.1748e-03, -1.8347e-02, -4.5315e-03,\n",
      "          7.4442e-03, -1.0567e-01,  1.0100e-02,  9.4099e-03,  1.5426e-02,\n",
      "          1.2187e-01,  5.6186e-03,  4.1094e-02,  5.3347e-02, -1.3586e-02,\n",
      "         -6.2723e-02,  3.9311e-02, -1.1032e-01, -1.0072e-01, -9.9938e-02,\n",
      "          9.3261e-02, -5.4935e-02, -4.0135e-02,  6.7297e-02,  2.4751e-02,\n",
      "          1.1774e-01, -6.1260e-02, -6.0969e-02,  1.0739e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0482, -0.0455, -0.0785, -0.0174,  0.1220,  0.0614,  0.0053,  0.0143,\n",
      "        -0.0911, -0.1153], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.rand_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-triumph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-collection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-mistake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-water",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-voice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-leader",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-buffalo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-transport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-bishop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
