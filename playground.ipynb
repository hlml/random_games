{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "colored-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "data_path = '../datasets/'\n",
    "\n",
    "\n",
    "# from backpack import backpack, extend\n",
    "# from backpack.extensions import (\n",
    "#     GGNMP,\n",
    "#     HMP,\n",
    "#     KFAC,\n",
    "#     KFLR,\n",
    "#     KFRA,\n",
    "#     PCHMP,\n",
    "#     BatchGrad,\n",
    "#     BatchL2Grad,\n",
    "#     DiagGGNExact,\n",
    "#     DiagGGNMC,\n",
    "#     DiagHessian,\n",
    "#     SumGradSquared,\n",
    "#     Variance,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regional-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCDataset(Dataset):\n",
    "    \n",
    "    \"\"\"MNIST-C dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root, train, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            self.data = torch.tensor(np.load(root + '/train_images.npy'))\n",
    "            self.targets = torch.tensor(np.load(root + '/train_labels.npy'))\n",
    "        else:\n",
    "            self.data = torch.tensor(np.load(root + '/test_images.npy'))\n",
    "            self.targets = torch.tensor(np.load(root + '/test_labels.npy'))\n",
    "            \n",
    "        self.train=train\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index][:,:,0], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "authentic-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = ([transforms.ToTensor()])\n",
    "trans = transforms.Compose(trans)\n",
    "\n",
    "fulltrainset = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=trans)\n",
    "train_set, valset = torch.utils.data.random_split(fulltrainset, [55000, 5000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "validloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=False,\n",
    "                                          num_workers=4, pin_memory=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=trans)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "nb_classes = 10\n",
    "\n",
    "rand_labels_raw = torch.randint(0, 10, fulltrainset.targets.shape)\n",
    "rand_labels_train, rand_labels_val = torch.utils.data.random_split(rand_labels_raw, [55000, 5000])\n",
    "\n",
    "rand_labels_train = torch.utils.data.DataLoader(rand_labels_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "rand_labels_val = torch.utils.data.DataLoader(rand_labels_val, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "mnist_c_bright_trainset = MNISTCDataset(root='../datasets/mnist_c/translate', train=False, transform=trans)\n",
    "mnist_c_bright_trainloader = torch.utils.data.DataLoader(mnist_c_bright_trainset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accomplished-suite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coral-currency",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() received an invalid combination of arguments - got (int, int, tuple, replace=bool), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1fe0eff3a161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: randint() received an invalid combination of arguments - got (int, int, tuple, replace=bool), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.randint(0, 10, (10,),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dominant-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping = dict(zip(range(10), np.random.choice(10, 10, replace=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "molecular-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list=[new_mapping[x] for x in trainloader.dataset.dataset.targets[0:10].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "single-desert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 7, 3, 8, 7, 6, 7, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "apparent-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, save=False, epoch=0, bn_eval=True):\n",
    "    \n",
    "    if bn_eval: # forward prop data twice to update BN running averages\n",
    "        model.train()\n",
    "        for _ in range(2):\n",
    "            for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                _ = (model(inputs))\n",
    "\n",
    "    model.eval()\n",
    "    correct, total, total_loss = 0,0,0\n",
    "    tot_iters = len(loader)\n",
    "#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\n",
    "#         inputs, targets = next(iter(loader)) \n",
    "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = (model(inputs, True))\n",
    "\n",
    "            _, predicted = torch.max(nn.Softmax(dim=1)(outputs).data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += torch.sum(predicted.eq(targets.data)).cpu()\n",
    "            total_loss += model.loss_fn(outputs, targets)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*float(correct)/float(total)\n",
    "    loss = total_loss/tot_iters\n",
    "#     if save and acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         print('Saving best model..')\n",
    "#         state = {\n",
    "#             'model': model0,\n",
    "#             'epoch': epoch\n",
    "#         }\n",
    "#         with open(args.save_dir + '/best_model.pt', 'wb') as f:\n",
    "#             torch.save(state, f)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pointed-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.final_feat_dim=100\n",
    "#         self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "military-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGame(nn.Module):\n",
    "    def __init__(self, model_func, num_class, multi_rand_layer=[], loss_type = 'softmax'):\n",
    "        super(RandomGame, self).__init__()\n",
    "        self.feature    = model_func()\n",
    "        \n",
    "        self.true_classifier = nn.Linear(self.feature.final_feat_dim, num_class, bias=True)\n",
    "\n",
    "        self.multi_rand_layer = multi_rand_layer\n",
    "        rand_trunk = []\n",
    "        if len(multi_rand_layer) == 0:\n",
    "            rand_trunk.append(nn.Linear(self.feature.final_feat_dim, num_class, bias=True))\n",
    "        else:\n",
    "            prev_layer_dim=self.feature.final_feat_dim\n",
    "            for i in self.multi_rand_layer:\n",
    "                rand_trunk.append(nn.Linear(prev_layer_dim, i, bias = True))\n",
    "                rand_trunk.append(nn.ReLU(inplace=True))\n",
    "                prev_layer_dim = i\n",
    "            rand_trunk.append(nn.Linear(prev_layer_dim, num_class, bias = True))\n",
    "            \n",
    "        self.rand_classifier = nn.Sequential(*rand_trunk)\n",
    "        \n",
    "        self.num_class = num_class\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, true_flag):\n",
    "        embedding  = self.feature.forward(x)\n",
    "        \n",
    "        if true_flag:\n",
    "            out = self.true_classifier.forward(embedding)\n",
    "        else:\n",
    "            out = self.rand_classifier.forward(embedding)\n",
    "        return out\n",
    "    \n",
    "    def train_rand(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    def train_true(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "#     def forward_true(self, x):\n",
    "#         x    = Variable(x.cuda())\n",
    "#         embedding  = self.feature.forward(x)\n",
    "#         out = self.true_classifier.forward(embedding)\n",
    "#         return embedding\n",
    "    \n",
    "#     def forward_rand(self, x):\n",
    "#         x    = Variable(x.cuda())\n",
    "#         embedding  = self.feature.forward(x)\n",
    "#         out = self.rand_classifier.forward(embedding)\n",
    "#         return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lasting-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk=[]\n",
    "prev_layer_dim=100\n",
    "for i in [64, 32]:\n",
    "    trunk.append(nn.Linear(prev_layer_dim, i, bias = True))\n",
    "    trunk.append(nn.ReLU(inplace=True))\n",
    "    prev_layer_dim = i\n",
    "trunk.append(nn.Linear(prev_layer_dim, 10, bias = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "introductory-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=100, out_features=64, bias=True),\n",
       " ReLU(inplace=True),\n",
       " Linear(in_features=64, out_features=32, bias=True),\n",
       " ReLU(inplace=True),\n",
       " Linear(in_features=32, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bound-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = nn.Sequential(*trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "described-patrol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=64, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "portable-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomGame(model_func=LeNet, num_class = 10, multi_rand_layer=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "breeding-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.6505e-04, -2.0728e-02, -3.2922e-02, -3.8461e-02, -6.1595e-02,\n",
       "          -5.6390e-03,  5.0472e-02, -4.4825e-02, -9.6168e-02, -9.1528e-02,\n",
       "           8.1942e-02,  3.8049e-02, -4.0895e-02,  6.4020e-02, -6.7884e-02,\n",
       "           6.3478e-03,  1.6959e-02,  2.3627e-02, -7.6719e-02,  3.2788e-02,\n",
       "           4.3599e-02,  6.5157e-02,  3.9421e-02, -6.3814e-02, -5.5647e-04,\n",
       "          -5.1167e-02,  4.8136e-02, -7.5035e-02, -5.6002e-02,  8.8924e-02,\n",
       "           6.7727e-02,  2.3894e-02,  3.8892e-02,  2.6329e-02, -9.0016e-02,\n",
       "           2.7490e-02,  1.1930e-02, -2.6328e-02,  6.8502e-02, -6.6443e-02,\n",
       "           7.2061e-02, -2.6601e-02, -6.9212e-02,  7.9581e-02, -5.9808e-02,\n",
       "          -6.8134e-02, -9.2334e-02,  4.2729e-02,  9.3029e-02,  2.3482e-02,\n",
       "          -1.6549e-02,  5.7481e-02,  1.6357e-02,  3.0266e-02,  2.0038e-03,\n",
       "          -1.3573e-02,  5.8325e-02,  1.5835e-02, -2.5260e-02, -8.2736e-02,\n",
       "          -1.6604e-02, -9.3048e-02, -6.9821e-02,  2.1179e-02,  8.2467e-02,\n",
       "          -8.3811e-02,  5.6099e-03,  2.7305e-03, -2.2753e-03,  4.4010e-02,\n",
       "          -3.8716e-02,  1.4105e-02,  3.5150e-02,  1.9041e-02, -6.8392e-02,\n",
       "          -9.5304e-02,  2.7087e-02, -8.1020e-02, -6.3901e-02,  5.8718e-02,\n",
       "          -7.6295e-02, -7.8115e-02,  2.3791e-02,  9.4569e-02, -6.7011e-02,\n",
       "          -8.7748e-02,  7.6237e-03,  1.4461e-02, -6.8066e-02,  3.9903e-02,\n",
       "          -3.3167e-02,  6.3585e-02,  4.9157e-02, -4.3071e-02, -3.8644e-02,\n",
       "          -4.7917e-02, -6.7289e-02, -7.2086e-02, -7.8673e-02,  2.1213e-02],\n",
       "         [ 9.5650e-02,  2.9731e-02,  7.8303e-02, -1.1069e-02,  6.0430e-03,\n",
       "           9.6092e-02, -4.1665e-02,  6.7072e-02,  2.2494e-02,  7.4946e-02,\n",
       "           7.5428e-02,  3.2413e-02, -4.8848e-02,  6.8258e-02,  7.0077e-02,\n",
       "          -5.7987e-02,  3.5619e-03,  8.0426e-02, -6.2552e-02, -7.6796e-02,\n",
       "          -4.3533e-02,  7.4522e-02,  7.6010e-02,  3.7710e-02,  9.7326e-02,\n",
       "          -2.1103e-02, -1.0367e-02,  5.0608e-02,  1.8022e-02, -4.3852e-02,\n",
       "           1.3364e-02,  3.0729e-02, -5.3091e-02,  3.1885e-02,  4.9868e-02,\n",
       "           7.6094e-02,  5.4679e-04,  3.8416e-02, -8.1124e-02,  1.7875e-02,\n",
       "          -4.5136e-02,  9.3023e-02, -9.1152e-03, -3.7460e-02, -6.3824e-02,\n",
       "          -6.4831e-02, -1.5006e-03,  5.3754e-02,  2.0767e-02, -2.1563e-02,\n",
       "          -6.9618e-02, -9.6435e-02, -5.5358e-02, -1.3566e-02, -3.4035e-02,\n",
       "           3.4681e-02,  1.5191e-02,  2.5726e-02,  1.3586e-02,  3.4554e-03,\n",
       "           8.1306e-02,  3.6051e-02,  1.0382e-02,  4.6312e-02, -2.1373e-03,\n",
       "          -2.5766e-02, -6.8204e-02,  8.2169e-02,  8.9942e-02, -5.9411e-02,\n",
       "           9.5912e-02, -4.3595e-02,  1.3760e-02,  1.1087e-02,  8.2496e-02,\n",
       "          -1.8614e-03,  3.8638e-02,  2.9307e-03, -9.4315e-02, -3.1731e-02,\n",
       "          -6.2323e-02,  5.7685e-02,  5.9752e-02,  4.8006e-02,  9.4460e-02,\n",
       "          -3.4736e-03,  2.8299e-02, -9.1229e-02, -1.0325e-03, -1.3428e-02,\n",
       "           4.9771e-02, -3.0887e-02,  1.8489e-02,  1.2636e-02,  2.3024e-02,\n",
       "           7.4439e-02,  8.1580e-02, -5.3612e-02,  4.7680e-02, -3.6934e-02],\n",
       "         [-1.4937e-02,  7.2076e-02, -5.0587e-02,  7.3604e-02,  5.4303e-02,\n",
       "          -4.2504e-02, -9.9431e-02, -8.0265e-02,  9.3036e-02,  4.4539e-02,\n",
       "          -4.3344e-02,  7.1490e-02,  2.6028e-02, -6.2505e-02, -8.5340e-02,\n",
       "           7.8472e-02, -3.8033e-02,  1.9694e-02, -5.1763e-02, -6.7184e-02,\n",
       "          -8.0061e-02, -1.6540e-02,  3.4985e-02,  6.6098e-02, -8.8737e-02,\n",
       "          -2.5932e-02, -4.1662e-02,  6.1710e-02, -5.5752e-02,  1.7199e-03,\n",
       "          -7.8579e-02,  7.2932e-02,  8.6061e-02,  2.4509e-02,  7.6846e-02,\n",
       "           3.8289e-02, -8.1965e-02, -2.7108e-02,  4.5476e-02,  1.9358e-02,\n",
       "           5.9759e-02,  1.6518e-03,  3.3581e-02, -8.4691e-02, -1.9032e-02,\n",
       "          -6.5252e-02,  7.4262e-02,  6.7095e-03,  7.6825e-02,  3.9626e-02,\n",
       "          -2.4249e-02,  6.1533e-02, -8.3913e-02,  3.4677e-02,  4.5744e-02,\n",
       "          -7.4536e-02,  1.9441e-02,  3.9845e-02,  9.6522e-02,  6.8252e-02,\n",
       "          -7.6912e-03, -6.7833e-02,  7.4851e-03,  1.0606e-02,  6.5972e-02,\n",
       "          -9.3773e-03,  1.5350e-02,  9.9558e-02, -9.2614e-03,  2.8000e-02,\n",
       "           5.9580e-02, -5.1109e-02, -7.8661e-02, -8.5836e-02, -4.2287e-02,\n",
       "           9.3859e-02, -8.0748e-02,  6.6893e-02, -9.8400e-03, -1.6235e-02,\n",
       "          -5.0795e-02, -9.7932e-02,  6.8981e-02,  1.6972e-02,  5.2765e-02,\n",
       "          -4.7713e-02,  5.4422e-02,  4.2635e-02, -1.2090e-02, -3.9455e-03,\n",
       "           6.1991e-02,  4.9597e-02,  2.1967e-02,  6.6976e-02,  7.0079e-02,\n",
       "           5.6717e-02, -9.1276e-02, -8.8620e-02, -4.3086e-02,  3.6603e-02],\n",
       "         [-9.8838e-02,  2.6489e-02,  8.9977e-02, -1.2555e-03,  2.2303e-03,\n",
       "           6.1560e-02, -3.5024e-02, -5.1771e-02,  4.3984e-02, -9.4286e-02,\n",
       "           6.3501e-02, -2.1972e-02, -7.7843e-02, -5.3110e-02, -7.7884e-02,\n",
       "           9.1531e-02,  8.7967e-03,  3.4592e-02,  6.3793e-02, -4.1441e-02,\n",
       "          -3.5324e-02,  2.4416e-02,  3.5275e-02, -1.1644e-02,  9.5708e-02,\n",
       "          -5.6957e-02,  2.8388e-02, -3.2827e-02,  6.2351e-02,  8.5547e-02,\n",
       "          -2.3000e-02, -3.0142e-02,  6.2799e-02, -9.1986e-03, -4.1676e-05,\n",
       "           6.9216e-02, -1.0364e-02,  2.5907e-02,  8.7383e-02, -7.8229e-03,\n",
       "           4.1000e-02, -2.6522e-02, -9.3903e-02,  2.9396e-03, -8.1445e-02,\n",
       "           7.3849e-02, -1.1475e-02, -3.8501e-02,  6.4983e-02,  6.6825e-02,\n",
       "           3.9243e-02,  9.9157e-03, -3.2220e-02, -8.1078e-03,  5.6025e-02,\n",
       "           4.1316e-02,  2.3781e-02,  1.7165e-02,  4.7358e-02,  5.5519e-02,\n",
       "          -1.8451e-02,  7.5556e-02, -6.1826e-02, -9.9075e-02,  3.9074e-02,\n",
       "           4.3653e-02, -4.5105e-02, -2.8397e-02, -5.4237e-02, -1.1754e-02,\n",
       "          -1.7396e-02,  5.4305e-02,  9.2670e-02, -9.1018e-02, -2.5886e-02,\n",
       "          -2.4553e-02,  2.8561e-02,  7.2391e-02,  8.3657e-02, -3.0599e-02,\n",
       "          -7.9222e-02,  4.0045e-02, -8.7198e-02, -6.9819e-02,  6.5045e-02,\n",
       "           2.7506e-02,  8.6295e-02,  7.4807e-03,  3.5460e-02, -1.7460e-02,\n",
       "           4.2578e-02, -8.9268e-02,  8.9245e-02,  6.7005e-02, -1.4499e-02,\n",
       "          -1.5559e-02,  8.3719e-02, -8.4289e-02, -4.9438e-02,  2.8425e-02],\n",
       "         [ 1.3277e-02,  4.8830e-03, -8.4060e-02,  3.4566e-02, -2.2585e-02,\n",
       "          -3.5712e-04, -8.0626e-02,  4.9344e-02,  3.7253e-02, -5.7216e-02,\n",
       "          -3.9833e-03,  9.8441e-02, -1.7071e-02, -6.4406e-02, -2.0775e-02,\n",
       "           1.2060e-02, -2.4980e-02,  7.8496e-03,  2.8175e-02,  1.2886e-02,\n",
       "          -8.2404e-02,  4.3453e-03,  4.2300e-02,  3.5916e-02, -6.0773e-02,\n",
       "           5.1734e-04,  8.7273e-02,  4.7024e-02,  9.2498e-02,  4.5999e-03,\n",
       "          -1.4417e-02, -8.2818e-02, -3.6491e-02, -3.6620e-02,  9.2971e-02,\n",
       "           8.5383e-02, -3.0052e-02, -5.9763e-03,  9.9557e-02,  6.7772e-02,\n",
       "           4.3489e-02,  8.1138e-02,  6.5569e-02, -8.6672e-02,  1.2435e-02,\n",
       "           7.9588e-02, -8.6762e-02,  7.1444e-03,  6.4105e-02,  3.4942e-02,\n",
       "           2.1877e-02, -8.4561e-04,  7.0689e-02, -2.0788e-02, -8.4177e-02,\n",
       "           3.6614e-02,  8.4826e-02,  1.6000e-02,  4.2486e-02,  9.9046e-02,\n",
       "           3.3750e-02, -5.9101e-02,  7.8547e-02, -8.2008e-02,  3.9292e-02,\n",
       "           3.9938e-02,  5.3259e-02, -9.4838e-02, -1.5601e-02,  7.5608e-02,\n",
       "          -6.5517e-02, -3.0056e-02,  5.2068e-02, -5.0237e-02,  1.5205e-02,\n",
       "           6.0815e-02,  2.2713e-02, -3.3388e-02, -6.5360e-02,  4.0635e-02,\n",
       "           4.0138e-02, -4.6320e-02, -9.9716e-02,  8.3442e-02, -6.5031e-02,\n",
       "          -3.8255e-02, -3.0107e-02, -2.2266e-02, -3.4813e-02, -6.0558e-02,\n",
       "          -9.8561e-02, -1.2683e-02, -3.0351e-02,  4.7097e-02,  2.3150e-02,\n",
       "          -2.5894e-02, -5.7510e-03, -4.3762e-02,  3.5699e-02, -5.7884e-02],\n",
       "         [ 4.3250e-02,  5.2178e-02, -8.8023e-02,  7.3915e-02,  6.2320e-02,\n",
       "          -9.0844e-02, -8.5152e-02, -5.9511e-02,  8.6055e-02,  3.1539e-04,\n",
       "          -6.4010e-02,  7.0980e-03, -4.6370e-03,  2.0873e-02, -2.7529e-02,\n",
       "          -7.1507e-03, -7.8262e-02,  1.8908e-02,  5.0172e-02, -9.8682e-02,\n",
       "           2.3404e-02,  5.6457e-02, -8.3596e-02,  7.0687e-03, -4.2943e-02,\n",
       "          -4.2998e-02,  4.6162e-02, -8.7116e-02,  1.2857e-02, -3.5866e-02,\n",
       "           5.7513e-02,  7.3697e-02, -8.0771e-02, -8.5388e-02,  7.9533e-02,\n",
       "          -9.5228e-02,  7.1832e-02, -9.7243e-02, -3.9653e-02,  9.6857e-02,\n",
       "          -2.6911e-03, -6.7251e-02, -8.0008e-02,  7.3912e-02,  3.4339e-02,\n",
       "           6.6764e-02, -9.5095e-02,  3.4112e-02, -6.7149e-02,  3.2959e-02,\n",
       "          -1.5816e-02, -3.6314e-02, -5.3712e-02, -6.6239e-03,  1.3622e-02,\n",
       "           9.3928e-02,  3.7307e-02,  5.4734e-02, -9.0798e-02,  8.2351e-02,\n",
       "          -5.2991e-02,  6.7745e-02, -8.3979e-02,  1.5422e-02,  7.8652e-02,\n",
       "          -1.8803e-02,  9.5760e-02, -3.7461e-02, -4.0336e-02,  4.7946e-02,\n",
       "          -9.0392e-02,  6.8880e-02,  8.8564e-02,  2.1929e-02,  3.4484e-02,\n",
       "           5.8581e-02, -9.6014e-02, -2.9903e-02, -6.9968e-02, -8.2717e-02,\n",
       "          -4.7959e-04,  6.6130e-02, -1.4713e-03, -6.7540e-02, -4.7126e-02,\n",
       "          -5.9839e-02,  2.8408e-02,  9.4859e-02,  7.4153e-02, -1.8939e-02,\n",
       "          -4.1237e-02,  6.3882e-03,  1.5925e-02,  6.2900e-02, -8.7181e-02,\n",
       "          -2.4407e-02,  9.7392e-02,  2.2682e-02, -3.6696e-03, -2.2383e-02],\n",
       "         [ 9.1007e-02,  4.0953e-02, -5.1071e-02,  8.1366e-02, -1.0360e-02,\n",
       "          -3.9326e-02, -1.6346e-02,  8.4816e-02,  5.1498e-02, -6.6346e-02,\n",
       "           5.6993e-03,  4.6701e-02, -6.3020e-02, -7.7395e-03, -3.0896e-02,\n",
       "          -4.9907e-02,  6.1675e-02,  3.6094e-02, -2.7732e-02,  3.6368e-02,\n",
       "           1.0513e-02, -7.2604e-02,  2.7405e-02,  3.2348e-02,  6.6047e-02,\n",
       "          -8.9388e-02, -9.5705e-03, -8.7862e-02, -1.8665e-02, -7.7607e-02,\n",
       "           5.6434e-02,  1.8542e-02,  4.3302e-02, -7.3788e-02,  9.4835e-02,\n",
       "           3.2261e-02,  8.4641e-03, -6.3718e-02,  2.6133e-02, -5.2875e-02,\n",
       "          -5.1237e-02, -9.2115e-02,  8.5649e-04, -8.3103e-02,  1.5132e-02,\n",
       "          -7.9949e-02,  4.1879e-02,  8.4562e-02, -2.2532e-02, -2.0678e-02,\n",
       "          -5.1660e-02,  4.7627e-02,  4.6511e-02, -7.9319e-03,  9.2112e-02,\n",
       "           2.2790e-02, -2.2607e-03, -3.2094e-02, -4.7863e-03, -6.5579e-02,\n",
       "           9.7489e-02, -9.6272e-02,  1.9435e-02,  5.8480e-02,  8.3919e-02,\n",
       "          -9.6732e-02, -5.4389e-02,  2.4498e-02, -8.0708e-03,  8.2535e-02,\n",
       "          -4.7550e-02, -9.9820e-03, -6.1000e-02, -7.9110e-02, -1.9689e-02,\n",
       "           8.5085e-03,  4.7214e-02, -6.1535e-02,  3.4413e-02,  5.6754e-02,\n",
       "           2.8137e-02,  1.0949e-03, -2.9157e-03, -5.3513e-02,  5.8443e-02,\n",
       "          -7.9493e-02,  4.0191e-02,  1.5077e-02, -7.9565e-02, -8.7363e-02,\n",
       "           9.9308e-02, -9.5872e-02, -7.2233e-02, -7.6029e-02,  3.0986e-02,\n",
       "          -9.6221e-02, -8.2700e-02, -6.5470e-02, -3.3426e-02,  2.1701e-02],\n",
       "         [ 8.1175e-02,  6.1551e-02, -4.7432e-02, -3.0105e-02,  9.2815e-04,\n",
       "          -1.5903e-02,  2.3412e-02, -1.3095e-02, -2.7923e-02,  5.5392e-02,\n",
       "           8.3386e-04, -5.8269e-02, -2.8251e-02,  3.2362e-02, -5.6777e-02,\n",
       "           3.5942e-03, -5.7993e-02, -1.4377e-02,  4.7796e-02, -8.6057e-03,\n",
       "           5.0407e-03, -4.6538e-02, -8.9750e-02, -9.1923e-02, -7.1630e-02,\n",
       "           4.4465e-02,  6.4089e-02, -8.7785e-02, -9.8351e-02,  8.9895e-02,\n",
       "           8.7010e-02, -6.3592e-02, -4.8032e-02,  9.2335e-02, -7.5520e-02,\n",
       "          -4.4057e-02,  8.4456e-03,  9.1549e-02, -6.6448e-02,  5.1927e-02,\n",
       "          -5.6377e-02, -9.2856e-02,  1.5988e-02,  1.2571e-02,  3.6327e-02,\n",
       "          -1.4555e-02,  5.8901e-02,  6.8793e-02,  9.3615e-03, -8.0020e-02,\n",
       "           8.9650e-04, -8.5622e-02, -4.9367e-02,  6.9567e-02, -3.9429e-02,\n",
       "           1.2474e-03, -6.4485e-02, -1.4056e-02,  4.9275e-02, -4.7021e-02,\n",
       "          -8.1105e-02, -1.8080e-02,  8.9686e-02,  8.5571e-02, -3.6244e-02,\n",
       "           3.9263e-02, -9.6793e-02, -5.3941e-02, -4.6485e-02,  8.0566e-02,\n",
       "           3.8985e-02,  4.9557e-02, -1.3801e-02, -5.3644e-02, -7.3497e-02,\n",
       "           5.1768e-02,  3.9251e-02, -8.1344e-02,  1.8492e-02,  3.0977e-02,\n",
       "          -7.8212e-02,  3.3647e-02, -2.1381e-02, -2.9990e-02, -6.7565e-02,\n",
       "           7.4055e-02,  8.3973e-02, -5.3851e-02, -5.1325e-02,  4.1458e-02,\n",
       "          -6.7742e-02,  1.2918e-02, -3.8855e-02, -7.8544e-02,  7.8942e-02,\n",
       "           3.8437e-02, -3.3739e-02,  2.1816e-02, -4.1892e-02,  9.3223e-02],\n",
       "         [-4.2289e-02,  9.9870e-02,  3.7292e-02, -1.6264e-02, -4.6218e-02,\n",
       "          -6.8812e-02,  3.0861e-02, -8.0141e-02, -8.3251e-02,  6.1606e-02,\n",
       "           9.1478e-03,  3.0515e-02, -7.4788e-02,  6.0113e-02,  1.0151e-02,\n",
       "           5.6943e-02, -8.8710e-03, -8.8408e-02,  6.7161e-02, -1.0899e-02,\n",
       "           2.9636e-02,  2.1718e-03, -1.1979e-02, -7.9583e-02, -1.9891e-02,\n",
       "           9.5940e-02,  5.3815e-02, -4.8887e-02, -6.6223e-02, -6.4370e-02,\n",
       "           9.4526e-03,  8.0737e-02, -2.0229e-02,  8.3322e-02, -2.3342e-02,\n",
       "          -2.2079e-02, -9.2367e-02,  5.1952e-03, -7.6018e-02, -7.0138e-02,\n",
       "           5.3652e-02,  9.4279e-02,  9.3839e-02,  6.3395e-02,  3.4372e-02,\n",
       "          -3.7311e-02,  7.5802e-02, -3.6887e-02, -9.7208e-02, -1.6611e-02,\n",
       "           1.5596e-02, -8.4085e-02,  1.6734e-02, -8.0943e-02,  8.9077e-02,\n",
       "           7.1029e-02,  9.1464e-02, -8.6452e-02,  2.9807e-02, -9.2648e-02,\n",
       "           1.8701e-02, -3.7566e-02, -8.2245e-02, -6.8143e-02, -2.8333e-02,\n",
       "           5.8384e-02, -5.3772e-02,  4.1201e-02,  4.6139e-02, -7.4488e-02,\n",
       "          -9.9698e-02,  5.7023e-02,  6.4482e-02, -5.8361e-02, -5.7179e-02,\n",
       "          -8.1320e-03,  9.8501e-02,  6.1981e-02,  4.8969e-02,  1.8052e-02,\n",
       "           2.1192e-03,  2.2778e-02,  7.2232e-02, -7.1824e-02, -5.8970e-03,\n",
       "           5.8594e-02,  9.7427e-02, -5.7116e-02,  2.5408e-02, -2.3541e-02,\n",
       "          -5.5271e-02,  6.4271e-02, -1.0305e-02, -1.8217e-02,  4.2381e-02,\n",
       "           7.5265e-02,  5.5598e-02,  1.7015e-02, -4.5881e-02,  7.0542e-02],\n",
       "         [ 3.2500e-03,  1.2194e-02,  8.5722e-02,  7.5832e-02,  4.4124e-02,\n",
       "          -1.7731e-02, -2.4316e-02,  8.0110e-02,  9.8513e-02, -8.5949e-02,\n",
       "           8.3673e-02, -5.7956e-02,  1.8685e-02,  2.7850e-02,  7.8043e-02,\n",
       "          -8.9352e-02,  4.1976e-03,  3.3281e-02,  8.0812e-03,  1.4211e-03,\n",
       "          -1.5869e-02,  2.3113e-02, -6.3592e-02,  7.2417e-03,  3.9829e-02,\n",
       "           7.5314e-02, -3.6521e-02,  7.1764e-02, -4.7596e-02, -2.0591e-02,\n",
       "          -2.1024e-02,  3.1523e-03,  4.8202e-03,  7.9834e-02,  5.8597e-02,\n",
       "          -8.6470e-02, -2.7538e-02, -7.6286e-02,  8.5731e-02, -6.3942e-02,\n",
       "           2.6953e-02, -9.0465e-02,  2.1708e-02,  3.6697e-02, -9.8031e-02,\n",
       "          -4.3903e-02, -5.3588e-03,  4.3433e-02,  7.5761e-02,  4.5971e-02,\n",
       "           3.3184e-02, -4.0795e-02,  9.2574e-02, -7.4930e-02, -2.8985e-02,\n",
       "           4.9121e-02,  6.9049e-03,  2.3811e-02,  3.5830e-02,  5.0839e-02,\n",
       "           8.8090e-02, -2.3812e-02, -9.3047e-02, -5.9489e-02,  1.6709e-02,\n",
       "          -6.8139e-02, -7.7808e-02,  8.1374e-02,  7.7264e-02, -1.2394e-02,\n",
       "          -4.7619e-02, -8.8915e-02,  5.3767e-02,  3.1637e-02, -3.7035e-02,\n",
       "           9.6850e-02, -7.7601e-02,  9.9766e-02,  4.9084e-02,  1.1780e-02,\n",
       "          -3.5028e-02, -7.0611e-02, -6.7707e-02, -7.8515e-02,  4.0672e-02,\n",
       "          -1.7835e-02, -3.1134e-02, -1.5831e-02,  4.9186e-03,  3.4256e-02,\n",
       "          -7.3483e-03,  8.1709e-02, -9.9309e-02, -1.3093e-02,  1.8239e-02,\n",
       "           3.6779e-02,  9.9976e-02, -1.6132e-02, -2.5499e-02, -7.4301e-02]]),\n",
       " tensor([-0.0828,  0.0291, -0.0981,  0.0603,  0.0170,  0.0675, -0.0447,  0.0399,\n",
       "          0.0088,  0.0798])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "urban-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_weights=[]\n",
    "for name, param in model.named_parameters():\n",
    "    if 'true' in name:\n",
    "        cur_weights.append(param.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rand_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "copyrighted-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.zeros_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "appointed-monthly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0675, -0.0447,  0.0399,\n",
       "         0.0088,  0.0798])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "criminal-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'rand' in name and 'weight' in name:\n",
    "        break\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "senior-logistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.1025e-01,  2.6464e-02,  1.2087e-01,  8.6465e-02, -2.1665e-01,\n",
       "         -4.9262e-02, -1.9231e-01, -7.8100e-02, -4.0198e-02, -1.9107e-01,\n",
       "          2.3115e-01, -2.0622e-01,  5.9121e-02,  1.5132e-02,  1.4697e-01,\n",
       "          2.2291e-01,  1.8676e-01, -1.8293e-02, -7.2818e-02, -1.1411e-01,\n",
       "          1.6557e-01,  1.4624e-01,  2.1000e-01,  1.2686e-01,  3.8684e-02,\n",
       "          2.1641e-01, -8.6895e-03, -3.7500e-02, -1.3888e-01, -4.5061e-02,\n",
       "         -5.8494e-02,  2.5730e-02,  6.6903e-02, -8.9521e-02, -2.2487e-01,\n",
       "         -2.9454e-02,  6.6556e-02, -2.3424e-02,  1.4270e-01,  1.7334e-01,\n",
       "          5.6950e-02,  1.7343e-01,  5.9820e-02, -1.2944e-01, -9.0060e-02,\n",
       "         -1.8634e-01, -1.2320e-01, -4.6478e-02, -5.2073e-02, -8.8389e-02,\n",
       "          1.3605e-01, -1.8000e-01, -6.8154e-03, -1.0941e-01,  1.6078e-01,\n",
       "         -8.1040e-02, -1.0045e-01, -4.7802e-02,  9.8013e-02, -5.6373e-02,\n",
       "         -9.9789e-02,  1.0171e-01,  2.3595e-02,  7.2821e-02,  9.3369e-02,\n",
       "         -5.0843e-02,  1.5744e-01, -2.1121e-01, -7.1621e-02,  1.7820e-01,\n",
       "         -1.6483e-01,  1.6604e-01,  2.0317e-01, -2.2033e-01, -5.0702e-02,\n",
       "         -1.0469e-01,  1.1975e-01,  2.2621e-01, -1.0849e-01,  1.0357e-01,\n",
       "          1.2637e-01, -9.1258e-04,  2.2053e-02,  2.0949e-01, -1.6191e-02,\n",
       "         -5.5530e-02,  1.4820e-01,  2.1312e-01,  1.8958e-01,  4.6864e-02,\n",
       "         -6.4934e-02, -1.7788e-01, -6.4954e-02, -1.8786e-01, -5.7925e-02,\n",
       "         -1.5229e-01,  1.9768e-03,  1.2894e-01, -6.8588e-02,  1.0372e-01],\n",
       "        [-1.6240e-01, -1.2704e-02, -1.9690e-01,  2.4885e-02, -4.9943e-03,\n",
       "          1.8494e-01,  4.1604e-02,  3.5401e-03,  4.1409e-02,  2.1677e-01,\n",
       "         -9.3091e-02,  2.2953e-01,  1.0106e-01, -1.1571e-01,  1.6205e-01,\n",
       "          1.0012e-01,  1.4777e-01,  6.5221e-02,  1.7154e-01,  6.0867e-02,\n",
       "         -2.1149e-02,  1.6048e-01,  9.1581e-02,  3.8817e-02, -1.0804e-01,\n",
       "         -1.9155e-02,  1.6739e-01, -1.4031e-01,  1.0820e-01, -1.7416e-01,\n",
       "          9.8226e-02,  1.2586e-01,  1.9481e-01,  1.8426e-01, -1.2953e-01,\n",
       "         -8.4194e-02,  3.4749e-03,  9.4694e-02, -1.0497e-01, -1.3103e-01,\n",
       "         -1.5242e-01,  1.8880e-02,  4.3339e-02, -9.9720e-02,  1.5541e-01,\n",
       "         -1.5600e-01,  8.9775e-02,  1.2075e-01, -2.1576e-01, -2.1953e-01,\n",
       "         -7.0718e-02, -2.0171e-01, -2.5910e-02,  2.2181e-02,  2.0821e-01,\n",
       "         -1.0554e-01,  5.4867e-02,  1.5884e-01, -1.2174e-01,  1.0058e-01,\n",
       "         -9.7813e-02, -7.8463e-03, -9.0097e-04, -2.2547e-01, -1.7431e-03,\n",
       "         -5.3006e-02,  5.5832e-03,  8.8010e-02, -7.4790e-02, -5.7984e-02,\n",
       "          1.9829e-01,  1.3664e-01, -1.5426e-01,  2.2844e-01, -8.6840e-02,\n",
       "         -2.9419e-02,  1.4417e-01, -1.3187e-01,  1.1495e-01, -2.1514e-01,\n",
       "         -2.6110e-02, -2.2940e-01, -1.6379e-01, -1.3119e-01, -3.9130e-02,\n",
       "         -1.5839e-01, -8.3389e-02,  1.1389e-01, -1.3516e-01, -9.3186e-03,\n",
       "          4.7331e-02, -1.7719e-01, -2.0260e-01, -1.4092e-01, -8.5121e-02,\n",
       "         -1.4717e-01, -1.2830e-01, -2.2765e-01,  1.3665e-01, -1.1654e-01],\n",
       "        [-1.7273e-01, -1.7428e-01,  2.8383e-02, -2.2777e-01,  4.2242e-02,\n",
       "         -2.5441e-02,  5.3036e-02, -2.0950e-01,  7.6873e-02, -1.3238e-01,\n",
       "         -1.0187e-01, -6.7494e-02, -1.2113e-01, -2.0795e-01,  2.2234e-01,\n",
       "         -4.6414e-03,  1.9487e-01, -1.1411e-01,  6.8863e-02, -1.8693e-01,\n",
       "          1.9398e-01, -1.6257e-01,  2.8397e-02, -1.5889e-01,  2.0483e-01,\n",
       "         -7.8023e-02,  1.8343e-01, -2.1304e-01, -1.8856e-01, -1.8468e-01,\n",
       "         -3.1509e-02,  3.7639e-02, -1.8526e-01,  1.7230e-01,  8.1157e-02,\n",
       "         -9.1121e-02, -1.3423e-01, -2.7155e-02,  6.1641e-02,  1.9423e-02,\n",
       "         -2.2009e-01, -1.8082e-01,  1.2268e-01, -3.3484e-02,  1.8789e-01,\n",
       "          6.9998e-03, -5.5854e-03,  5.9006e-02, -1.2837e-01, -2.0051e-01,\n",
       "         -8.1310e-02,  8.4730e-03, -1.9923e-01,  1.7109e-01, -4.1634e-02,\n",
       "          1.8209e-01, -1.2939e-01,  7.7963e-03, -1.1055e-01,  1.7960e-01,\n",
       "         -3.6341e-02, -7.4203e-02, -8.0833e-02,  8.1653e-02,  5.7033e-02,\n",
       "         -5.8150e-02, -1.5785e-01, -2.5003e-02,  2.3068e-01,  1.7789e-01,\n",
       "         -1.3429e-01, -2.3291e-01,  1.4785e-01,  1.7845e-01,  9.0422e-02,\n",
       "         -9.3132e-02,  2.2831e-01,  5.9287e-02,  7.2853e-02, -2.1587e-01,\n",
       "         -7.6398e-02,  6.5969e-02,  7.2486e-02, -1.8085e-01,  1.4487e-01,\n",
       "          2.1723e-01,  7.4372e-04,  2.0918e-01,  5.5882e-02,  1.7095e-01,\n",
       "          1.8248e-01,  1.6191e-01, -1.1947e-01, -1.1431e-01, -1.1510e-01,\n",
       "          2.1779e-01,  8.1741e-03,  1.2589e-01,  8.5182e-02,  3.7855e-02],\n",
       "        [ 1.9988e-01, -2.0272e-01, -2.1104e-02, -6.9581e-02,  6.0808e-02,\n",
       "         -1.6442e-01, -2.2991e-01, -1.8115e-01, -8.0939e-02, -1.1438e-02,\n",
       "          1.0449e-01,  4.7803e-02,  1.1735e-01,  8.8393e-02, -1.2621e-01,\n",
       "         -5.8320e-02,  2.2110e-01,  1.1553e-01, -5.5939e-04, -1.2369e-01,\n",
       "          3.4861e-02,  1.1129e-01,  1.0905e-01,  1.0379e-03,  1.0644e-01,\n",
       "         -1.9904e-01, -6.1932e-02, -2.0594e-01,  9.1592e-02, -1.4360e-01,\n",
       "         -5.1729e-02,  1.9309e-01,  1.6017e-01, -1.9892e-01, -1.2482e-01,\n",
       "         -1.8477e-01, -6.6923e-02,  6.1749e-02,  1.5968e-01,  8.3166e-02,\n",
       "          1.3228e-01, -9.2020e-02, -1.0131e-01,  1.8589e-01,  1.9269e-01,\n",
       "          1.1837e-01,  1.8267e-01,  1.8080e-02,  1.4932e-01, -6.1996e-02,\n",
       "         -1.5053e-01, -1.7565e-01,  6.0842e-02,  2.1700e-01,  1.8516e-01,\n",
       "         -1.4804e-01,  6.6735e-03,  3.8394e-02, -9.2301e-02, -3.0692e-02,\n",
       "         -8.3000e-02, -1.8664e-01, -7.0794e-02,  7.1209e-02, -6.1166e-02,\n",
       "         -1.1298e-01, -4.4234e-02, -4.1519e-02, -2.0733e-01, -1.9765e-01,\n",
       "          1.1182e-01, -2.8297e-02,  3.9009e-03,  1.8901e-01, -5.6547e-02,\n",
       "         -1.3335e-01,  8.9197e-02,  1.9187e-02,  1.0280e-01,  4.1975e-02,\n",
       "          2.1865e-01,  4.2987e-02,  1.4093e-01,  1.6045e-01,  1.1990e-01,\n",
       "         -1.5967e-01,  1.2819e-01,  1.6330e-02, -2.2444e-01,  1.3811e-01,\n",
       "         -7.1238e-02, -2.3054e-01,  1.8994e-01, -2.1175e-01, -2.2757e-01,\n",
       "         -1.7895e-01, -1.3511e-01, -2.0717e-02, -1.1267e-01, -2.0418e-01],\n",
       "        [ 5.1444e-02, -1.5363e-01,  4.7350e-02, -9.2673e-02, -7.1472e-02,\n",
       "          2.0505e-01, -1.5948e-02, -1.7284e-01, -1.2052e-01,  8.4873e-02,\n",
       "          2.1514e-01, -1.8197e-01,  7.2495e-02,  9.6692e-02,  1.1013e-01,\n",
       "         -4.9510e-02, -5.9950e-02,  1.3267e-01, -2.4154e-03,  7.1915e-02,\n",
       "          2.0997e-02, -1.6422e-01, -1.9423e-01,  1.4584e-01, -1.4212e-01,\n",
       "         -2.2387e-01,  1.3713e-01,  7.0224e-02,  2.2511e-01,  3.0865e-02,\n",
       "          7.9551e-03,  5.1596e-02, -6.9997e-02,  1.5982e-01,  6.4972e-02,\n",
       "         -1.7573e-01, -1.2033e-02,  1.9436e-01,  1.5585e-02,  1.1465e-02,\n",
       "          6.8677e-02, -1.4789e-01, -1.2849e-01,  1.2757e-01, -2.2391e-02,\n",
       "          1.9998e-01, -7.1389e-02,  9.9820e-02, -1.4551e-01,  5.9965e-02,\n",
       "          1.3748e-01,  2.0165e-01,  8.9509e-02,  1.6980e-01,  1.7534e-01,\n",
       "          7.6379e-02,  5.8149e-02, -1.7969e-01,  1.7289e-01, -8.3559e-02,\n",
       "         -1.1849e-02, -9.9816e-03, -1.6973e-01, -2.1808e-01,  6.8247e-02,\n",
       "         -1.3719e-01,  2.1726e-01, -2.2874e-01, -1.9676e-01, -9.8713e-02,\n",
       "          7.4146e-02, -6.5163e-02, -6.3385e-02, -1.6223e-01, -1.2737e-01,\n",
       "         -2.3288e-01, -1.1220e-01,  1.1212e-01, -1.8301e-01, -5.4738e-02,\n",
       "          7.1726e-02,  1.2834e-01,  1.7722e-01,  1.5572e-01, -2.3274e-01,\n",
       "          8.5537e-02,  3.7546e-02,  1.4148e-01,  1.6042e-01,  1.4906e-01,\n",
       "          1.1372e-01,  2.4793e-02, -7.5287e-02, -1.7801e-01, -2.0237e-01,\n",
       "          2.2545e-01, -2.2407e-01, -3.4063e-02, -3.8529e-02, -1.9603e-01],\n",
       "        [ 7.9570e-02,  1.2761e-01, -1.9923e-01, -1.8117e-01,  2.1957e-01,\n",
       "         -1.5026e-01, -1.0509e-01, -6.4018e-02, -9.7969e-03,  1.6635e-02,\n",
       "          2.9259e-02,  1.4498e-02,  6.7572e-02,  2.2955e-01,  5.0073e-02,\n",
       "          1.0940e-01,  1.7508e-01,  1.7989e-01,  2.0363e-01, -2.1282e-01,\n",
       "         -2.2215e-01, -1.6909e-01,  1.2885e-01, -1.1963e-01,  7.1186e-02,\n",
       "          2.3147e-01,  7.9029e-02,  2.3329e-01, -1.3165e-01,  1.1567e-01,\n",
       "         -1.3533e-01, -1.0596e-02,  1.5392e-01, -5.2353e-03,  1.8489e-01,\n",
       "         -1.4065e-02, -2.1802e-01,  5.5401e-02, -1.4730e-01,  2.1176e-01,\n",
       "          1.2951e-01, -8.6882e-02,  1.2270e-01, -1.4187e-02, -6.3060e-02,\n",
       "         -9.1962e-03, -1.1413e-01, -1.5930e-01, -1.1284e-01,  5.1725e-03,\n",
       "         -2.7014e-02,  1.1411e-01, -8.1093e-02, -2.2272e-01, -1.5454e-01,\n",
       "         -1.7716e-01,  2.1130e-01, -1.3863e-01, -1.9325e-01, -4.8320e-02,\n",
       "          1.6726e-01, -3.1990e-02,  1.4768e-02,  1.4426e-01, -4.3000e-02,\n",
       "          4.2536e-02,  9.3278e-02, -8.1446e-02, -2.1070e-01,  1.2241e-01,\n",
       "         -7.7774e-03,  3.7334e-02, -2.2670e-01, -1.8918e-01, -1.5955e-01,\n",
       "         -2.2856e-01,  1.7081e-01,  2.4252e-02,  7.5906e-02, -2.2808e-01,\n",
       "          1.6912e-01, -6.0045e-02, -9.8474e-03, -2.2346e-01,  6.2095e-02,\n",
       "          8.0856e-02, -1.0074e-01, -1.0835e-01,  2.2505e-01, -2.7896e-02,\n",
       "         -1.8454e-01, -8.3082e-02,  1.8407e-02, -1.7032e-01,  1.4115e-01,\n",
       "          2.2005e-01,  1.4862e-01,  1.7732e-01,  1.9174e-01,  1.6355e-01],\n",
       "        [-1.5153e-03,  1.1818e-01, -2.3045e-01,  9.5765e-03,  1.3834e-01,\n",
       "         -9.3210e-04, -2.0379e-01, -1.8716e-01, -8.5581e-02, -5.4266e-02,\n",
       "          9.2845e-02,  9.9385e-02,  7.4615e-03,  5.7941e-02, -1.9902e-01,\n",
       "         -9.5427e-02, -7.1229e-02,  1.8841e-01,  7.9395e-02, -3.5484e-02,\n",
       "         -5.6314e-02,  6.4272e-02,  1.3025e-01, -1.6356e-01,  1.0932e-01,\n",
       "         -7.1692e-02, -4.0958e-02, -2.4058e-02,  1.8326e-01, -1.2765e-02,\n",
       "          7.0242e-02,  3.7622e-02,  2.1169e-02,  9.2708e-02, -2.2381e-01,\n",
       "          1.6897e-01, -6.6585e-02,  8.3208e-02, -1.4081e-01,  1.7127e-01,\n",
       "          5.9881e-02, -1.1809e-01, -3.7216e-02,  1.4206e-01, -1.0132e-01,\n",
       "         -1.5863e-01, -1.1489e-01, -2.0453e-01,  3.3418e-02,  2.0006e-01,\n",
       "         -2.2666e-01,  5.9956e-02,  6.0087e-02,  4.3559e-02, -1.1469e-01,\n",
       "          1.3098e-01,  1.5380e-01, -2.3097e-01,  1.9033e-01, -2.3970e-02,\n",
       "          1.1949e-01, -2.1197e-01, -1.7838e-01, -1.6197e-01, -9.8040e-03,\n",
       "          9.5167e-02, -1.3654e-01,  2.9625e-02,  1.6341e-01, -2.1724e-02,\n",
       "         -2.2424e-01,  1.9085e-01, -1.3277e-01,  1.3483e-01,  9.6512e-02,\n",
       "         -2.1521e-01, -1.8775e-01,  1.0827e-01, -5.1021e-02, -5.8967e-02,\n",
       "         -2.1377e-01,  8.3029e-02, -1.0185e-01,  1.5740e-01,  1.9847e-01,\n",
       "         -3.3496e-02, -1.0007e-01, -1.2049e-01,  6.3445e-02, -2.2349e-01,\n",
       "          1.7477e-01, -2.2343e-01, -6.7889e-02,  9.9514e-02,  4.2158e-02,\n",
       "         -1.1302e-02, -4.8765e-03,  1.9008e-01,  1.0200e-01, -1.1704e-01],\n",
       "        [-3.9580e-02, -9.2581e-02,  9.2675e-03,  1.8961e-01, -1.7665e-01,\n",
       "          1.1243e-01, -2.0747e-02,  2.0893e-01, -5.6016e-02,  1.4373e-01,\n",
       "          1.6876e-01,  1.4024e-01,  1.8338e-01, -1.6911e-01, -5.1359e-02,\n",
       "          2.2179e-01,  1.9555e-01,  5.3225e-02, -6.5256e-02, -8.8033e-02,\n",
       "          2.0587e-01,  5.1748e-02, -8.2587e-02, -9.0433e-02,  1.2511e-01,\n",
       "         -1.4400e-01,  4.0442e-02, -4.7919e-02,  1.7188e-01,  8.6325e-02,\n",
       "         -1.3565e-01,  1.7094e-01, -1.8474e-01,  1.4203e-02, -1.3119e-01,\n",
       "          1.8580e-01,  3.4076e-02, -2.2776e-01, -2.2477e-01, -1.4312e-01,\n",
       "         -1.3585e-01, -1.1013e-02, -1.1968e-01, -2.2904e-01,  2.1537e-01,\n",
       "          1.0220e-01,  1.1914e-02,  1.4400e-01,  1.7256e-01, -9.4519e-02,\n",
       "         -8.4329e-02,  7.2745e-02,  8.7107e-02,  3.6771e-02, -8.1686e-02,\n",
       "          1.3065e-01, -1.3011e-01,  7.8199e-02, -2.1157e-02,  1.0244e-01,\n",
       "          5.3785e-03,  2.1250e-01,  2.8758e-02,  1.5956e-01,  2.1108e-01,\n",
       "          2.0710e-01,  7.0176e-03,  1.2718e-01,  1.8602e-01,  1.0171e-01,\n",
       "         -1.3220e-02, -2.3253e-02,  6.4764e-02,  4.0657e-02,  6.9643e-02,\n",
       "         -2.8731e-02,  1.2290e-01,  4.6315e-02, -2.2761e-01,  1.1304e-01,\n",
       "         -2.1942e-01,  1.4456e-01, -1.1161e-01,  6.6789e-02, -1.9609e-01,\n",
       "         -1.5266e-01, -2.2040e-01, -7.3658e-02,  2.0871e-01, -9.7877e-02,\n",
       "          1.3175e-01,  2.2085e-01,  2.2051e-01, -1.1357e-01,  1.4393e-01,\n",
       "         -1.9137e-01,  2.7476e-03,  1.7462e-01, -7.6069e-02, -1.1368e-01],\n",
       "        [-3.1859e-02, -1.3628e-02,  1.9699e-01, -2.2791e-01,  1.7268e-01,\n",
       "          1.6549e-01, -1.4710e-01, -1.8543e-01,  2.0607e-01,  5.3702e-02,\n",
       "         -7.6300e-02,  3.7089e-02, -3.3629e-02, -1.3634e-01,  4.8403e-02,\n",
       "          7.0135e-02,  2.1332e-01, -1.7470e-01,  5.3895e-02,  2.0902e-01,\n",
       "         -6.0329e-02, -1.3766e-01,  1.0880e-01, -1.8354e-01,  7.7723e-02,\n",
       "          4.9917e-02,  2.0906e-01,  2.1042e-02, -7.9640e-02, -5.2503e-02,\n",
       "         -1.4819e-01,  1.7323e-01,  6.3052e-02, -2.1386e-01,  2.1365e-01,\n",
       "          3.7589e-02, -1.0107e-02, -1.6478e-01,  2.0442e-01,  1.2811e-01,\n",
       "         -9.2904e-02, -1.3293e-01, -6.5950e-02, -4.1051e-02,  8.4889e-02,\n",
       "         -2.2434e-01, -2.3184e-01, -2.1554e-01, -1.8831e-01,  1.2201e-01,\n",
       "         -1.9346e-01,  3.5717e-02,  8.3443e-03,  2.1606e-01,  1.0062e-01,\n",
       "         -1.3849e-01,  1.6237e-01,  1.5879e-01,  5.8503e-02,  8.4313e-02,\n",
       "          8.2204e-02,  1.7456e-01, -2.2833e-01,  1.5033e-01, -1.3333e-01,\n",
       "         -7.2337e-02, -2.0940e-01,  7.1428e-02,  1.4104e-01,  1.2313e-02,\n",
       "         -2.7217e-02, -6.2745e-02, -4.6145e-02,  6.4503e-02,  3.9209e-03,\n",
       "         -1.6670e-01, -2.3036e-01,  1.3513e-01, -9.9207e-02, -1.2323e-01,\n",
       "         -8.8915e-02, -1.5924e-01, -1.6492e-01, -2.1145e-01, -4.6438e-02,\n",
       "         -1.0684e-01, -6.2811e-02,  2.2861e-01, -1.5096e-03, -1.6463e-01,\n",
       "          1.4678e-02, -2.0347e-01,  1.3422e-02,  7.8170e-02, -2.1822e-01,\n",
       "          1.4385e-01,  1.8549e-01,  7.3896e-02,  1.4924e-01, -6.3999e-02],\n",
       "        [ 1.1035e-01,  1.3832e-01,  1.5124e-02, -1.8225e-01, -5.1304e-02,\n",
       "         -5.8870e-02, -1.6639e-01, -1.8848e-01, -1.4551e-01, -1.7203e-01,\n",
       "          1.1371e-01,  1.3944e-01, -6.4153e-02,  8.9939e-02,  1.7908e-02,\n",
       "          1.5145e-02, -2.0150e-01,  2.0962e-02,  2.3067e-01, -2.0345e-01,\n",
       "          1.5889e-01, -1.4610e-01, -1.9028e-01,  1.1506e-02, -2.1455e-01,\n",
       "          8.9618e-02, -1.9848e-01,  1.5771e-01, -1.0206e-01,  9.9393e-02,\n",
       "          1.7284e-01, -6.8575e-02,  6.9079e-02,  1.0966e-01, -2.7352e-02,\n",
       "         -1.4440e-01, -6.8460e-02, -1.2253e-02, -9.0630e-02,  2.2944e-01,\n",
       "         -2.0435e-01, -1.2108e-01, -1.0029e-01,  1.4076e-01,  8.7381e-02,\n",
       "          7.7794e-02,  2.0532e-01, -1.8832e-01,  1.8239e-01, -1.0650e-01,\n",
       "         -1.3934e-01, -1.5784e-01,  9.3242e-02, -1.2592e-01, -1.2435e-01,\n",
       "         -1.8402e-01,  1.9708e-01, -2.1639e-01, -1.1618e-01,  1.7133e-02,\n",
       "          1.0236e-01,  7.4835e-02,  1.0078e-01,  1.4659e-01,  4.5131e-02,\n",
       "          2.8343e-03, -2.0768e-01,  1.2065e-01,  3.4807e-02,  4.4335e-03,\n",
       "          1.2116e-01,  1.9267e-01, -8.5582e-02,  7.1277e-02,  6.0918e-02,\n",
       "          1.7604e-01, -1.0255e-01,  9.0070e-02, -1.0526e-01,  1.4630e-01,\n",
       "          8.3851e-02, -6.9398e-02,  9.8493e-02, -1.6467e-01,  1.0476e-01,\n",
       "          5.3045e-02,  3.2407e-05, -2.1588e-01,  7.9762e-02, -1.5777e-01,\n",
       "          3.4187e-02, -1.9628e-01,  1.7446e-01,  4.2207e-02, -1.3966e-01,\n",
       "         -1.2642e-01,  2.9935e-02,  6.2159e-02,  1.7489e-01,  1.5996e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "raising-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model.named_children():\n",
    "    if 'rand' in name:\n",
    "        layer[0].reset_parameters()\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "strange-fairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.children at 0x7f0b32e499d0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "static-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'rand' in name:\n",
    "        param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "shaped-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "param[0:5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ambient-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.6505e-04, -2.0728e-02, -3.2922e-02, -3.8461e-02, -6.1595e-02,\n",
       "          -5.6390e-03,  5.0472e-02, -4.4825e-02, -9.6168e-02, -9.1528e-02,\n",
       "           8.1942e-02,  3.8049e-02, -4.0895e-02,  6.4020e-02, -6.7884e-02,\n",
       "           6.3478e-03,  1.6959e-02,  2.3627e-02, -7.6719e-02,  3.2788e-02,\n",
       "           4.3599e-02,  6.5157e-02,  3.9421e-02, -6.3814e-02, -5.5647e-04,\n",
       "          -5.1167e-02,  4.8136e-02, -7.5035e-02, -5.6002e-02,  8.8924e-02,\n",
       "           6.7727e-02,  2.3894e-02,  3.8892e-02,  2.6329e-02, -9.0016e-02,\n",
       "           2.7490e-02,  1.1930e-02, -2.6328e-02,  6.8502e-02, -6.6443e-02,\n",
       "           7.2061e-02, -2.6601e-02, -6.9212e-02,  7.9581e-02, -5.9808e-02,\n",
       "          -6.8134e-02, -9.2334e-02,  4.2729e-02,  9.3029e-02,  2.3482e-02,\n",
       "          -1.6549e-02,  5.7481e-02,  1.6357e-02,  3.0266e-02,  2.0038e-03,\n",
       "          -1.3573e-02,  5.8325e-02,  1.5835e-02, -2.5260e-02, -8.2736e-02,\n",
       "          -1.6604e-02, -9.3048e-02, -6.9821e-02,  2.1179e-02,  8.2467e-02,\n",
       "          -8.3811e-02,  5.6099e-03,  2.7305e-03, -2.2753e-03,  4.4010e-02,\n",
       "          -3.8716e-02,  1.4105e-02,  3.5150e-02,  1.9041e-02, -6.8392e-02,\n",
       "          -9.5304e-02,  2.7087e-02, -8.1020e-02, -6.3901e-02,  5.8718e-02,\n",
       "          -7.6295e-02, -7.8115e-02,  2.3791e-02,  9.4569e-02, -6.7011e-02,\n",
       "          -8.7748e-02,  7.6237e-03,  1.4461e-02, -6.8066e-02,  3.9903e-02,\n",
       "          -3.3167e-02,  6.3585e-02,  4.9157e-02, -4.3071e-02, -3.8644e-02,\n",
       "          -4.7917e-02, -6.7289e-02, -7.2086e-02, -7.8673e-02,  2.1213e-02],\n",
       "         [ 9.5650e-02,  2.9731e-02,  7.8303e-02, -1.1069e-02,  6.0430e-03,\n",
       "           9.6092e-02, -4.1665e-02,  6.7072e-02,  2.2494e-02,  7.4946e-02,\n",
       "           7.5428e-02,  3.2413e-02, -4.8848e-02,  6.8258e-02,  7.0077e-02,\n",
       "          -5.7987e-02,  3.5619e-03,  8.0426e-02, -6.2552e-02, -7.6796e-02,\n",
       "          -4.3533e-02,  7.4522e-02,  7.6010e-02,  3.7710e-02,  9.7326e-02,\n",
       "          -2.1103e-02, -1.0367e-02,  5.0608e-02,  1.8022e-02, -4.3852e-02,\n",
       "           1.3364e-02,  3.0729e-02, -5.3091e-02,  3.1885e-02,  4.9868e-02,\n",
       "           7.6094e-02,  5.4679e-04,  3.8416e-02, -8.1124e-02,  1.7875e-02,\n",
       "          -4.5136e-02,  9.3023e-02, -9.1152e-03, -3.7460e-02, -6.3824e-02,\n",
       "          -6.4831e-02, -1.5006e-03,  5.3754e-02,  2.0767e-02, -2.1563e-02,\n",
       "          -6.9618e-02, -9.6435e-02, -5.5358e-02, -1.3566e-02, -3.4035e-02,\n",
       "           3.4681e-02,  1.5191e-02,  2.5726e-02,  1.3586e-02,  3.4554e-03,\n",
       "           8.1306e-02,  3.6051e-02,  1.0382e-02,  4.6312e-02, -2.1373e-03,\n",
       "          -2.5766e-02, -6.8204e-02,  8.2169e-02,  8.9942e-02, -5.9411e-02,\n",
       "           9.5912e-02, -4.3595e-02,  1.3760e-02,  1.1087e-02,  8.2496e-02,\n",
       "          -1.8614e-03,  3.8638e-02,  2.9307e-03, -9.4315e-02, -3.1731e-02,\n",
       "          -6.2323e-02,  5.7685e-02,  5.9752e-02,  4.8006e-02,  9.4460e-02,\n",
       "          -3.4736e-03,  2.8299e-02, -9.1229e-02, -1.0325e-03, -1.3428e-02,\n",
       "           4.9771e-02, -3.0887e-02,  1.8489e-02,  1.2636e-02,  2.3024e-02,\n",
       "           7.4439e-02,  8.1580e-02, -5.3612e-02,  4.7680e-02, -3.6934e-02],\n",
       "         [-1.4937e-02,  7.2076e-02, -5.0587e-02,  7.3604e-02,  5.4303e-02,\n",
       "          -4.2504e-02, -9.9431e-02, -8.0265e-02,  9.3036e-02,  4.4539e-02,\n",
       "          -4.3344e-02,  7.1490e-02,  2.6028e-02, -6.2505e-02, -8.5340e-02,\n",
       "           7.8472e-02, -3.8033e-02,  1.9694e-02, -5.1763e-02, -6.7184e-02,\n",
       "          -8.0061e-02, -1.6540e-02,  3.4985e-02,  6.6098e-02, -8.8737e-02,\n",
       "          -2.5932e-02, -4.1662e-02,  6.1710e-02, -5.5752e-02,  1.7199e-03,\n",
       "          -7.8579e-02,  7.2932e-02,  8.6061e-02,  2.4509e-02,  7.6846e-02,\n",
       "           3.8289e-02, -8.1965e-02, -2.7108e-02,  4.5476e-02,  1.9358e-02,\n",
       "           5.9759e-02,  1.6518e-03,  3.3581e-02, -8.4691e-02, -1.9032e-02,\n",
       "          -6.5252e-02,  7.4262e-02,  6.7095e-03,  7.6825e-02,  3.9626e-02,\n",
       "          -2.4249e-02,  6.1533e-02, -8.3913e-02,  3.4677e-02,  4.5744e-02,\n",
       "          -7.4536e-02,  1.9441e-02,  3.9845e-02,  9.6522e-02,  6.8252e-02,\n",
       "          -7.6912e-03, -6.7833e-02,  7.4851e-03,  1.0606e-02,  6.5972e-02,\n",
       "          -9.3773e-03,  1.5350e-02,  9.9558e-02, -9.2614e-03,  2.8000e-02,\n",
       "           5.9580e-02, -5.1109e-02, -7.8661e-02, -8.5836e-02, -4.2287e-02,\n",
       "           9.3859e-02, -8.0748e-02,  6.6893e-02, -9.8400e-03, -1.6235e-02,\n",
       "          -5.0795e-02, -9.7932e-02,  6.8981e-02,  1.6972e-02,  5.2765e-02,\n",
       "          -4.7713e-02,  5.4422e-02,  4.2635e-02, -1.2090e-02, -3.9455e-03,\n",
       "           6.1991e-02,  4.9597e-02,  2.1967e-02,  6.6976e-02,  7.0079e-02,\n",
       "           5.6717e-02, -9.1276e-02, -8.8620e-02, -4.3086e-02,  3.6603e-02],\n",
       "         [-9.8838e-02,  2.6489e-02,  8.9977e-02, -1.2555e-03,  2.2303e-03,\n",
       "           6.1560e-02, -3.5024e-02, -5.1771e-02,  4.3984e-02, -9.4286e-02,\n",
       "           6.3501e-02, -2.1972e-02, -7.7843e-02, -5.3110e-02, -7.7884e-02,\n",
       "           9.1531e-02,  8.7967e-03,  3.4592e-02,  6.3793e-02, -4.1441e-02,\n",
       "          -3.5324e-02,  2.4416e-02,  3.5275e-02, -1.1644e-02,  9.5708e-02,\n",
       "          -5.6957e-02,  2.8388e-02, -3.2827e-02,  6.2351e-02,  8.5547e-02,\n",
       "          -2.3000e-02, -3.0142e-02,  6.2799e-02, -9.1986e-03, -4.1676e-05,\n",
       "           6.9216e-02, -1.0364e-02,  2.5907e-02,  8.7383e-02, -7.8229e-03,\n",
       "           4.1000e-02, -2.6522e-02, -9.3903e-02,  2.9396e-03, -8.1445e-02,\n",
       "           7.3849e-02, -1.1475e-02, -3.8501e-02,  6.4983e-02,  6.6825e-02,\n",
       "           3.9243e-02,  9.9157e-03, -3.2220e-02, -8.1078e-03,  5.6025e-02,\n",
       "           4.1316e-02,  2.3781e-02,  1.7165e-02,  4.7358e-02,  5.5519e-02,\n",
       "          -1.8451e-02,  7.5556e-02, -6.1826e-02, -9.9075e-02,  3.9074e-02,\n",
       "           4.3653e-02, -4.5105e-02, -2.8397e-02, -5.4237e-02, -1.1754e-02,\n",
       "          -1.7396e-02,  5.4305e-02,  9.2670e-02, -9.1018e-02, -2.5886e-02,\n",
       "          -2.4553e-02,  2.8561e-02,  7.2391e-02,  8.3657e-02, -3.0599e-02,\n",
       "          -7.9222e-02,  4.0045e-02, -8.7198e-02, -6.9819e-02,  6.5045e-02,\n",
       "           2.7506e-02,  8.6295e-02,  7.4807e-03,  3.5460e-02, -1.7460e-02,\n",
       "           4.2578e-02, -8.9268e-02,  8.9245e-02,  6.7005e-02, -1.4499e-02,\n",
       "          -1.5559e-02,  8.3719e-02, -8.4289e-02, -4.9438e-02,  2.8425e-02],\n",
       "         [ 1.3277e-02,  4.8830e-03, -8.4060e-02,  3.4566e-02, -2.2585e-02,\n",
       "          -3.5712e-04, -8.0626e-02,  4.9344e-02,  3.7253e-02, -5.7216e-02,\n",
       "          -3.9833e-03,  9.8441e-02, -1.7071e-02, -6.4406e-02, -2.0775e-02,\n",
       "           1.2060e-02, -2.4980e-02,  7.8496e-03,  2.8175e-02,  1.2886e-02,\n",
       "          -8.2404e-02,  4.3453e-03,  4.2300e-02,  3.5916e-02, -6.0773e-02,\n",
       "           5.1734e-04,  8.7273e-02,  4.7024e-02,  9.2498e-02,  4.5999e-03,\n",
       "          -1.4417e-02, -8.2818e-02, -3.6491e-02, -3.6620e-02,  9.2971e-02,\n",
       "           8.5383e-02, -3.0052e-02, -5.9763e-03,  9.9557e-02,  6.7772e-02,\n",
       "           4.3489e-02,  8.1138e-02,  6.5569e-02, -8.6672e-02,  1.2435e-02,\n",
       "           7.9588e-02, -8.6762e-02,  7.1444e-03,  6.4105e-02,  3.4942e-02,\n",
       "           2.1877e-02, -8.4561e-04,  7.0689e-02, -2.0788e-02, -8.4177e-02,\n",
       "           3.6614e-02,  8.4826e-02,  1.6000e-02,  4.2486e-02,  9.9046e-02,\n",
       "           3.3750e-02, -5.9101e-02,  7.8547e-02, -8.2008e-02,  3.9292e-02,\n",
       "           3.9938e-02,  5.3259e-02, -9.4838e-02, -1.5601e-02,  7.5608e-02,\n",
       "          -6.5517e-02, -3.0056e-02,  5.2068e-02, -5.0237e-02,  1.5205e-02,\n",
       "           6.0815e-02,  2.2713e-02, -3.3388e-02, -6.5360e-02,  4.0635e-02,\n",
       "           4.0138e-02, -4.6320e-02, -9.9716e-02,  8.3442e-02, -6.5031e-02,\n",
       "          -3.8255e-02, -3.0107e-02, -2.2266e-02, -3.4813e-02, -6.0558e-02,\n",
       "          -9.8561e-02, -1.2683e-02, -3.0351e-02,  4.7097e-02,  2.3150e-02,\n",
       "          -2.5894e-02, -5.7510e-03, -4.3762e-02,  3.5699e-02, -5.7884e-02],\n",
       "         [ 4.3250e-02,  5.2178e-02, -8.8023e-02,  7.3915e-02,  6.2320e-02,\n",
       "          -9.0844e-02, -8.5152e-02, -5.9511e-02,  8.6055e-02,  3.1539e-04,\n",
       "          -6.4010e-02,  7.0980e-03, -4.6370e-03,  2.0873e-02, -2.7529e-02,\n",
       "          -7.1507e-03, -7.8262e-02,  1.8908e-02,  5.0172e-02, -9.8682e-02,\n",
       "           2.3404e-02,  5.6457e-02, -8.3596e-02,  7.0687e-03, -4.2943e-02,\n",
       "          -4.2998e-02,  4.6162e-02, -8.7116e-02,  1.2857e-02, -3.5866e-02,\n",
       "           5.7513e-02,  7.3697e-02, -8.0771e-02, -8.5388e-02,  7.9533e-02,\n",
       "          -9.5228e-02,  7.1832e-02, -9.7243e-02, -3.9653e-02,  9.6857e-02,\n",
       "          -2.6911e-03, -6.7251e-02, -8.0008e-02,  7.3912e-02,  3.4339e-02,\n",
       "           6.6764e-02, -9.5095e-02,  3.4112e-02, -6.7149e-02,  3.2959e-02,\n",
       "          -1.5816e-02, -3.6314e-02, -5.3712e-02, -6.6239e-03,  1.3622e-02,\n",
       "           9.3928e-02,  3.7307e-02,  5.4734e-02, -9.0798e-02,  8.2351e-02,\n",
       "          -5.2991e-02,  6.7745e-02, -8.3979e-02,  1.5422e-02,  7.8652e-02,\n",
       "          -1.8803e-02,  9.5760e-02, -3.7461e-02, -4.0336e-02,  4.7946e-02,\n",
       "          -9.0392e-02,  6.8880e-02,  8.8564e-02,  2.1929e-02,  3.4484e-02,\n",
       "           5.8581e-02, -9.6014e-02, -2.9903e-02, -6.9968e-02, -8.2717e-02,\n",
       "          -4.7959e-04,  6.6130e-02, -1.4713e-03, -6.7540e-02, -4.7126e-02,\n",
       "          -5.9839e-02,  2.8408e-02,  9.4859e-02,  7.4153e-02, -1.8939e-02,\n",
       "          -4.1237e-02,  6.3882e-03,  1.5925e-02,  6.2900e-02, -8.7181e-02,\n",
       "          -2.4407e-02,  9.7392e-02,  2.2682e-02, -3.6696e-03, -2.2383e-02],\n",
       "         [ 9.1007e-02,  4.0953e-02, -5.1071e-02,  8.1366e-02, -1.0360e-02,\n",
       "          -3.9326e-02, -1.6346e-02,  8.4816e-02,  5.1498e-02, -6.6346e-02,\n",
       "           5.6993e-03,  4.6701e-02, -6.3020e-02, -7.7395e-03, -3.0896e-02,\n",
       "          -4.9907e-02,  6.1675e-02,  3.6094e-02, -2.7732e-02,  3.6368e-02,\n",
       "           1.0513e-02, -7.2604e-02,  2.7405e-02,  3.2348e-02,  6.6047e-02,\n",
       "          -8.9388e-02, -9.5705e-03, -8.7862e-02, -1.8665e-02, -7.7607e-02,\n",
       "           5.6434e-02,  1.8542e-02,  4.3302e-02, -7.3788e-02,  9.4835e-02,\n",
       "           3.2261e-02,  8.4641e-03, -6.3718e-02,  2.6133e-02, -5.2875e-02,\n",
       "          -5.1237e-02, -9.2115e-02,  8.5649e-04, -8.3103e-02,  1.5132e-02,\n",
       "          -7.9949e-02,  4.1879e-02,  8.4562e-02, -2.2532e-02, -2.0678e-02,\n",
       "          -5.1660e-02,  4.7627e-02,  4.6511e-02, -7.9319e-03,  9.2112e-02,\n",
       "           2.2790e-02, -2.2607e-03, -3.2094e-02, -4.7863e-03, -6.5579e-02,\n",
       "           9.7489e-02, -9.6272e-02,  1.9435e-02,  5.8480e-02,  8.3919e-02,\n",
       "          -9.6732e-02, -5.4389e-02,  2.4498e-02, -8.0708e-03,  8.2535e-02,\n",
       "          -4.7550e-02, -9.9820e-03, -6.1000e-02, -7.9110e-02, -1.9689e-02,\n",
       "           8.5085e-03,  4.7214e-02, -6.1535e-02,  3.4413e-02,  5.6754e-02,\n",
       "           2.8137e-02,  1.0949e-03, -2.9157e-03, -5.3513e-02,  5.8443e-02,\n",
       "          -7.9493e-02,  4.0191e-02,  1.5077e-02, -7.9565e-02, -8.7363e-02,\n",
       "           9.9308e-02, -9.5872e-02, -7.2233e-02, -7.6029e-02,  3.0986e-02,\n",
       "          -9.6221e-02, -8.2700e-02, -6.5470e-02, -3.3426e-02,  2.1701e-02],\n",
       "         [ 8.1175e-02,  6.1551e-02, -4.7432e-02, -3.0105e-02,  9.2815e-04,\n",
       "          -1.5903e-02,  2.3412e-02, -1.3095e-02, -2.7923e-02,  5.5392e-02,\n",
       "           8.3386e-04, -5.8269e-02, -2.8251e-02,  3.2362e-02, -5.6777e-02,\n",
       "           3.5942e-03, -5.7993e-02, -1.4377e-02,  4.7796e-02, -8.6057e-03,\n",
       "           5.0407e-03, -4.6538e-02, -8.9750e-02, -9.1923e-02, -7.1630e-02,\n",
       "           4.4465e-02,  6.4089e-02, -8.7785e-02, -9.8351e-02,  8.9895e-02,\n",
       "           8.7010e-02, -6.3592e-02, -4.8032e-02,  9.2335e-02, -7.5520e-02,\n",
       "          -4.4057e-02,  8.4456e-03,  9.1549e-02, -6.6448e-02,  5.1927e-02,\n",
       "          -5.6377e-02, -9.2856e-02,  1.5988e-02,  1.2571e-02,  3.6327e-02,\n",
       "          -1.4555e-02,  5.8901e-02,  6.8793e-02,  9.3615e-03, -8.0020e-02,\n",
       "           8.9650e-04, -8.5622e-02, -4.9367e-02,  6.9567e-02, -3.9429e-02,\n",
       "           1.2474e-03, -6.4485e-02, -1.4056e-02,  4.9275e-02, -4.7021e-02,\n",
       "          -8.1105e-02, -1.8080e-02,  8.9686e-02,  8.5571e-02, -3.6244e-02,\n",
       "           3.9263e-02, -9.6793e-02, -5.3941e-02, -4.6485e-02,  8.0566e-02,\n",
       "           3.8985e-02,  4.9557e-02, -1.3801e-02, -5.3644e-02, -7.3497e-02,\n",
       "           5.1768e-02,  3.9251e-02, -8.1344e-02,  1.8492e-02,  3.0977e-02,\n",
       "          -7.8212e-02,  3.3647e-02, -2.1381e-02, -2.9990e-02, -6.7565e-02,\n",
       "           7.4055e-02,  8.3973e-02, -5.3851e-02, -5.1325e-02,  4.1458e-02,\n",
       "          -6.7742e-02,  1.2918e-02, -3.8855e-02, -7.8544e-02,  7.8942e-02,\n",
       "           3.8437e-02, -3.3739e-02,  2.1816e-02, -4.1892e-02,  9.3223e-02],\n",
       "         [-4.2289e-02,  9.9870e-02,  3.7292e-02, -1.6264e-02, -4.6218e-02,\n",
       "          -6.8812e-02,  3.0861e-02, -8.0141e-02, -8.3251e-02,  6.1606e-02,\n",
       "           9.1478e-03,  3.0515e-02, -7.4788e-02,  6.0113e-02,  1.0151e-02,\n",
       "           5.6943e-02, -8.8710e-03, -8.8408e-02,  6.7161e-02, -1.0899e-02,\n",
       "           2.9636e-02,  2.1718e-03, -1.1979e-02, -7.9583e-02, -1.9891e-02,\n",
       "           9.5940e-02,  5.3815e-02, -4.8887e-02, -6.6223e-02, -6.4370e-02,\n",
       "           9.4526e-03,  8.0737e-02, -2.0229e-02,  8.3322e-02, -2.3342e-02,\n",
       "          -2.2079e-02, -9.2367e-02,  5.1952e-03, -7.6018e-02, -7.0138e-02,\n",
       "           5.3652e-02,  9.4279e-02,  9.3839e-02,  6.3395e-02,  3.4372e-02,\n",
       "          -3.7311e-02,  7.5802e-02, -3.6887e-02, -9.7208e-02, -1.6611e-02,\n",
       "           1.5596e-02, -8.4085e-02,  1.6734e-02, -8.0943e-02,  8.9077e-02,\n",
       "           7.1029e-02,  9.1464e-02, -8.6452e-02,  2.9807e-02, -9.2648e-02,\n",
       "           1.8701e-02, -3.7566e-02, -8.2245e-02, -6.8143e-02, -2.8333e-02,\n",
       "           5.8384e-02, -5.3772e-02,  4.1201e-02,  4.6139e-02, -7.4488e-02,\n",
       "          -9.9698e-02,  5.7023e-02,  6.4482e-02, -5.8361e-02, -5.7179e-02,\n",
       "          -8.1320e-03,  9.8501e-02,  6.1981e-02,  4.8969e-02,  1.8052e-02,\n",
       "           2.1192e-03,  2.2778e-02,  7.2232e-02, -7.1824e-02, -5.8970e-03,\n",
       "           5.8594e-02,  9.7427e-02, -5.7116e-02,  2.5408e-02, -2.3541e-02,\n",
       "          -5.5271e-02,  6.4271e-02, -1.0305e-02, -1.8217e-02,  4.2381e-02,\n",
       "           7.5265e-02,  5.5598e-02,  1.7015e-02, -4.5881e-02,  7.0542e-02],\n",
       "         [ 3.2500e-03,  1.2194e-02,  8.5722e-02,  7.5832e-02,  4.4124e-02,\n",
       "          -1.7731e-02, -2.4316e-02,  8.0110e-02,  9.8513e-02, -8.5949e-02,\n",
       "           8.3673e-02, -5.7956e-02,  1.8685e-02,  2.7850e-02,  7.8043e-02,\n",
       "          -8.9352e-02,  4.1976e-03,  3.3281e-02,  8.0812e-03,  1.4211e-03,\n",
       "          -1.5869e-02,  2.3113e-02, -6.3592e-02,  7.2417e-03,  3.9829e-02,\n",
       "           7.5314e-02, -3.6521e-02,  7.1764e-02, -4.7596e-02, -2.0591e-02,\n",
       "          -2.1024e-02,  3.1523e-03,  4.8202e-03,  7.9834e-02,  5.8597e-02,\n",
       "          -8.6470e-02, -2.7538e-02, -7.6286e-02,  8.5731e-02, -6.3942e-02,\n",
       "           2.6953e-02, -9.0465e-02,  2.1708e-02,  3.6697e-02, -9.8031e-02,\n",
       "          -4.3903e-02, -5.3588e-03,  4.3433e-02,  7.5761e-02,  4.5971e-02,\n",
       "           3.3184e-02, -4.0795e-02,  9.2574e-02, -7.4930e-02, -2.8985e-02,\n",
       "           4.9121e-02,  6.9049e-03,  2.3811e-02,  3.5830e-02,  5.0839e-02,\n",
       "           8.8090e-02, -2.3812e-02, -9.3047e-02, -5.9489e-02,  1.6709e-02,\n",
       "          -6.8139e-02, -7.7808e-02,  8.1374e-02,  7.7264e-02, -1.2394e-02,\n",
       "          -4.7619e-02, -8.8915e-02,  5.3767e-02,  3.1637e-02, -3.7035e-02,\n",
       "           9.6850e-02, -7.7601e-02,  9.9766e-02,  4.9084e-02,  1.1780e-02,\n",
       "          -3.5028e-02, -7.0611e-02, -6.7707e-02, -7.8515e-02,  4.0672e-02,\n",
       "          -1.7835e-02, -3.1134e-02, -1.5831e-02,  4.9186e-03,  3.4256e-02,\n",
       "          -7.3483e-03,  8.1709e-02, -9.9309e-02, -1.3093e-02,  1.8239e-02,\n",
       "           3.6779e-02,  9.9976e-02, -1.6132e-02, -2.5499e-02, -7.4301e-02]]),\n",
       " tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0675, -0.0447,  0.0399,\n",
       "          0.0088,  0.0798])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "duplicate-enlargement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-4.6505e-04, -2.0728e-02, -3.2922e-02, -3.8461e-02, -6.1595e-02,\n",
      "         -5.6390e-03,  5.0472e-02, -4.4825e-02, -9.6168e-02, -9.1528e-02,\n",
      "          8.1942e-02,  3.8049e-02, -4.0895e-02,  6.4020e-02, -6.7884e-02,\n",
      "          6.3478e-03,  1.6959e-02,  2.3627e-02, -7.6719e-02,  3.2788e-02,\n",
      "          4.3599e-02,  6.5157e-02,  3.9421e-02, -6.3814e-02, -5.5647e-04,\n",
      "         -5.1167e-02,  4.8136e-02, -7.5035e-02, -5.6002e-02,  8.8924e-02,\n",
      "          6.7727e-02,  2.3894e-02,  3.8892e-02,  2.6329e-02, -9.0016e-02,\n",
      "          2.7490e-02,  1.1930e-02, -2.6328e-02,  6.8502e-02, -6.6443e-02,\n",
      "          7.2061e-02, -2.6601e-02, -6.9212e-02,  7.9581e-02, -5.9808e-02,\n",
      "         -6.8134e-02, -9.2334e-02,  4.2729e-02,  9.3029e-02,  2.3482e-02,\n",
      "         -1.6549e-02,  5.7481e-02,  1.6357e-02,  3.0266e-02,  2.0038e-03,\n",
      "         -1.3573e-02,  5.8325e-02,  1.5835e-02, -2.5260e-02, -8.2736e-02,\n",
      "         -1.6604e-02, -9.3048e-02, -6.9821e-02,  2.1179e-02,  8.2467e-02,\n",
      "         -8.3811e-02,  5.6099e-03,  2.7305e-03, -2.2753e-03,  4.4010e-02,\n",
      "         -3.8716e-02,  1.4105e-02,  3.5150e-02,  1.9041e-02, -6.8392e-02,\n",
      "         -9.5304e-02,  2.7087e-02, -8.1020e-02, -6.3901e-02,  5.8718e-02,\n",
      "         -7.6295e-02, -7.8115e-02,  2.3791e-02,  9.4569e-02, -6.7011e-02,\n",
      "         -8.7748e-02,  7.6237e-03,  1.4461e-02, -6.8066e-02,  3.9903e-02,\n",
      "         -3.3167e-02,  6.3585e-02,  4.9157e-02, -4.3071e-02, -3.8644e-02,\n",
      "         -4.7917e-02, -6.7289e-02, -7.2086e-02, -7.8673e-02,  2.1213e-02],\n",
      "        [ 9.5650e-02,  2.9731e-02,  7.8303e-02, -1.1069e-02,  6.0430e-03,\n",
      "          9.6092e-02, -4.1665e-02,  6.7072e-02,  2.2494e-02,  7.4946e-02,\n",
      "          7.5428e-02,  3.2413e-02, -4.8848e-02,  6.8258e-02,  7.0077e-02,\n",
      "         -5.7987e-02,  3.5619e-03,  8.0426e-02, -6.2552e-02, -7.6796e-02,\n",
      "         -4.3533e-02,  7.4522e-02,  7.6010e-02,  3.7710e-02,  9.7326e-02,\n",
      "         -2.1103e-02, -1.0367e-02,  5.0608e-02,  1.8022e-02, -4.3852e-02,\n",
      "          1.3364e-02,  3.0729e-02, -5.3091e-02,  3.1885e-02,  4.9868e-02,\n",
      "          7.6094e-02,  5.4679e-04,  3.8416e-02, -8.1124e-02,  1.7875e-02,\n",
      "         -4.5136e-02,  9.3023e-02, -9.1152e-03, -3.7460e-02, -6.3824e-02,\n",
      "         -6.4831e-02, -1.5006e-03,  5.3754e-02,  2.0767e-02, -2.1563e-02,\n",
      "         -6.9618e-02, -9.6435e-02, -5.5358e-02, -1.3566e-02, -3.4035e-02,\n",
      "          3.4681e-02,  1.5191e-02,  2.5726e-02,  1.3586e-02,  3.4554e-03,\n",
      "          8.1306e-02,  3.6051e-02,  1.0382e-02,  4.6312e-02, -2.1373e-03,\n",
      "         -2.5766e-02, -6.8204e-02,  8.2169e-02,  8.9942e-02, -5.9411e-02,\n",
      "          9.5912e-02, -4.3595e-02,  1.3760e-02,  1.1087e-02,  8.2496e-02,\n",
      "         -1.8614e-03,  3.8638e-02,  2.9307e-03, -9.4315e-02, -3.1731e-02,\n",
      "         -6.2323e-02,  5.7685e-02,  5.9752e-02,  4.8006e-02,  9.4460e-02,\n",
      "         -3.4736e-03,  2.8299e-02, -9.1229e-02, -1.0325e-03, -1.3428e-02,\n",
      "          4.9771e-02, -3.0887e-02,  1.8489e-02,  1.2636e-02,  2.3024e-02,\n",
      "          7.4439e-02,  8.1580e-02, -5.3612e-02,  4.7680e-02, -3.6934e-02],\n",
      "        [-1.4937e-02,  7.2076e-02, -5.0587e-02,  7.3604e-02,  5.4303e-02,\n",
      "         -4.2504e-02, -9.9431e-02, -8.0265e-02,  9.3036e-02,  4.4539e-02,\n",
      "         -4.3344e-02,  7.1490e-02,  2.6028e-02, -6.2505e-02, -8.5340e-02,\n",
      "          7.8472e-02, -3.8033e-02,  1.9694e-02, -5.1763e-02, -6.7184e-02,\n",
      "         -8.0061e-02, -1.6540e-02,  3.4985e-02,  6.6098e-02, -8.8737e-02,\n",
      "         -2.5932e-02, -4.1662e-02,  6.1710e-02, -5.5752e-02,  1.7199e-03,\n",
      "         -7.8579e-02,  7.2932e-02,  8.6061e-02,  2.4509e-02,  7.6846e-02,\n",
      "          3.8289e-02, -8.1965e-02, -2.7108e-02,  4.5476e-02,  1.9358e-02,\n",
      "          5.9759e-02,  1.6518e-03,  3.3581e-02, -8.4691e-02, -1.9032e-02,\n",
      "         -6.5252e-02,  7.4262e-02,  6.7095e-03,  7.6825e-02,  3.9626e-02,\n",
      "         -2.4249e-02,  6.1533e-02, -8.3913e-02,  3.4677e-02,  4.5744e-02,\n",
      "         -7.4536e-02,  1.9441e-02,  3.9845e-02,  9.6522e-02,  6.8252e-02,\n",
      "         -7.6912e-03, -6.7833e-02,  7.4851e-03,  1.0606e-02,  6.5972e-02,\n",
      "         -9.3773e-03,  1.5350e-02,  9.9558e-02, -9.2614e-03,  2.8000e-02,\n",
      "          5.9580e-02, -5.1109e-02, -7.8661e-02, -8.5836e-02, -4.2287e-02,\n",
      "          9.3859e-02, -8.0748e-02,  6.6893e-02, -9.8400e-03, -1.6235e-02,\n",
      "         -5.0795e-02, -9.7932e-02,  6.8981e-02,  1.6972e-02,  5.2765e-02,\n",
      "         -4.7713e-02,  5.4422e-02,  4.2635e-02, -1.2090e-02, -3.9455e-03,\n",
      "          6.1991e-02,  4.9597e-02,  2.1967e-02,  6.6976e-02,  7.0079e-02,\n",
      "          5.6717e-02, -9.1276e-02, -8.8620e-02, -4.3086e-02,  3.6603e-02],\n",
      "        [-9.8838e-02,  2.6489e-02,  8.9977e-02, -1.2555e-03,  2.2303e-03,\n",
      "          6.1560e-02, -3.5024e-02, -5.1771e-02,  4.3984e-02, -9.4286e-02,\n",
      "          6.3501e-02, -2.1972e-02, -7.7843e-02, -5.3110e-02, -7.7884e-02,\n",
      "          9.1531e-02,  8.7967e-03,  3.4592e-02,  6.3793e-02, -4.1441e-02,\n",
      "         -3.5324e-02,  2.4416e-02,  3.5275e-02, -1.1644e-02,  9.5708e-02,\n",
      "         -5.6957e-02,  2.8388e-02, -3.2827e-02,  6.2351e-02,  8.5547e-02,\n",
      "         -2.3000e-02, -3.0142e-02,  6.2799e-02, -9.1986e-03, -4.1676e-05,\n",
      "          6.9216e-02, -1.0364e-02,  2.5907e-02,  8.7383e-02, -7.8229e-03,\n",
      "          4.1000e-02, -2.6522e-02, -9.3903e-02,  2.9396e-03, -8.1445e-02,\n",
      "          7.3849e-02, -1.1475e-02, -3.8501e-02,  6.4983e-02,  6.6825e-02,\n",
      "          3.9243e-02,  9.9157e-03, -3.2220e-02, -8.1078e-03,  5.6025e-02,\n",
      "          4.1316e-02,  2.3781e-02,  1.7165e-02,  4.7358e-02,  5.5519e-02,\n",
      "         -1.8451e-02,  7.5556e-02, -6.1826e-02, -9.9075e-02,  3.9074e-02,\n",
      "          4.3653e-02, -4.5105e-02, -2.8397e-02, -5.4237e-02, -1.1754e-02,\n",
      "         -1.7396e-02,  5.4305e-02,  9.2670e-02, -9.1018e-02, -2.5886e-02,\n",
      "         -2.4553e-02,  2.8561e-02,  7.2391e-02,  8.3657e-02, -3.0599e-02,\n",
      "         -7.9222e-02,  4.0045e-02, -8.7198e-02, -6.9819e-02,  6.5045e-02,\n",
      "          2.7506e-02,  8.6295e-02,  7.4807e-03,  3.5460e-02, -1.7460e-02,\n",
      "          4.2578e-02, -8.9268e-02,  8.9245e-02,  6.7005e-02, -1.4499e-02,\n",
      "         -1.5559e-02,  8.3719e-02, -8.4289e-02, -4.9438e-02,  2.8425e-02],\n",
      "        [ 1.3277e-02,  4.8830e-03, -8.4060e-02,  3.4566e-02, -2.2585e-02,\n",
      "         -3.5712e-04, -8.0626e-02,  4.9344e-02,  3.7253e-02, -5.7216e-02,\n",
      "         -3.9833e-03,  9.8441e-02, -1.7071e-02, -6.4406e-02, -2.0775e-02,\n",
      "          1.2060e-02, -2.4980e-02,  7.8496e-03,  2.8175e-02,  1.2886e-02,\n",
      "         -8.2404e-02,  4.3453e-03,  4.2300e-02,  3.5916e-02, -6.0773e-02,\n",
      "          5.1734e-04,  8.7273e-02,  4.7024e-02,  9.2498e-02,  4.5999e-03,\n",
      "         -1.4417e-02, -8.2818e-02, -3.6491e-02, -3.6620e-02,  9.2971e-02,\n",
      "          8.5383e-02, -3.0052e-02, -5.9763e-03,  9.9557e-02,  6.7772e-02,\n",
      "          4.3489e-02,  8.1138e-02,  6.5569e-02, -8.6672e-02,  1.2435e-02,\n",
      "          7.9588e-02, -8.6762e-02,  7.1444e-03,  6.4105e-02,  3.4942e-02,\n",
      "          2.1877e-02, -8.4561e-04,  7.0689e-02, -2.0788e-02, -8.4177e-02,\n",
      "          3.6614e-02,  8.4826e-02,  1.6000e-02,  4.2486e-02,  9.9046e-02,\n",
      "          3.3750e-02, -5.9101e-02,  7.8547e-02, -8.2008e-02,  3.9292e-02,\n",
      "          3.9938e-02,  5.3259e-02, -9.4838e-02, -1.5601e-02,  7.5608e-02,\n",
      "         -6.5517e-02, -3.0056e-02,  5.2068e-02, -5.0237e-02,  1.5205e-02,\n",
      "          6.0815e-02,  2.2713e-02, -3.3388e-02, -6.5360e-02,  4.0635e-02,\n",
      "          4.0138e-02, -4.6320e-02, -9.9716e-02,  8.3442e-02, -6.5031e-02,\n",
      "         -3.8255e-02, -3.0107e-02, -2.2266e-02, -3.4813e-02, -6.0558e-02,\n",
      "         -9.8561e-02, -1.2683e-02, -3.0351e-02,  4.7097e-02,  2.3150e-02,\n",
      "         -2.5894e-02, -5.7510e-03, -4.3762e-02,  3.5699e-02, -5.7884e-02],\n",
      "        [ 4.3250e-02,  5.2178e-02, -8.8023e-02,  7.3915e-02,  6.2320e-02,\n",
      "         -9.0844e-02, -8.5152e-02, -5.9511e-02,  8.6055e-02,  3.1539e-04,\n",
      "         -6.4010e-02,  7.0980e-03, -4.6370e-03,  2.0873e-02, -2.7529e-02,\n",
      "         -7.1507e-03, -7.8262e-02,  1.8908e-02,  5.0172e-02, -9.8682e-02,\n",
      "          2.3404e-02,  5.6457e-02, -8.3596e-02,  7.0687e-03, -4.2943e-02,\n",
      "         -4.2998e-02,  4.6162e-02, -8.7116e-02,  1.2857e-02, -3.5866e-02,\n",
      "          5.7513e-02,  7.3697e-02, -8.0771e-02, -8.5388e-02,  7.9533e-02,\n",
      "         -9.5228e-02,  7.1832e-02, -9.7243e-02, -3.9653e-02,  9.6857e-02,\n",
      "         -2.6911e-03, -6.7251e-02, -8.0008e-02,  7.3912e-02,  3.4339e-02,\n",
      "          6.6764e-02, -9.5095e-02,  3.4112e-02, -6.7149e-02,  3.2959e-02,\n",
      "         -1.5816e-02, -3.6314e-02, -5.3712e-02, -6.6239e-03,  1.3622e-02,\n",
      "          9.3928e-02,  3.7307e-02,  5.4734e-02, -9.0798e-02,  8.2351e-02,\n",
      "         -5.2991e-02,  6.7745e-02, -8.3979e-02,  1.5422e-02,  7.8652e-02,\n",
      "         -1.8803e-02,  9.5760e-02, -3.7461e-02, -4.0336e-02,  4.7946e-02,\n",
      "         -9.0392e-02,  6.8880e-02,  8.8564e-02,  2.1929e-02,  3.4484e-02,\n",
      "          5.8581e-02, -9.6014e-02, -2.9903e-02, -6.9968e-02, -8.2717e-02,\n",
      "         -4.7959e-04,  6.6130e-02, -1.4713e-03, -6.7540e-02, -4.7126e-02,\n",
      "         -5.9839e-02,  2.8408e-02,  9.4859e-02,  7.4153e-02, -1.8939e-02,\n",
      "         -4.1237e-02,  6.3882e-03,  1.5925e-02,  6.2900e-02, -8.7181e-02,\n",
      "         -2.4407e-02,  9.7392e-02,  2.2682e-02, -3.6696e-03, -2.2383e-02],\n",
      "        [ 9.1007e-02,  4.0953e-02, -5.1071e-02,  8.1366e-02, -1.0360e-02,\n",
      "         -3.9326e-02, -1.6346e-02,  8.4816e-02,  5.1498e-02, -6.6346e-02,\n",
      "          5.6993e-03,  4.6701e-02, -6.3020e-02, -7.7395e-03, -3.0896e-02,\n",
      "         -4.9907e-02,  6.1675e-02,  3.6094e-02, -2.7732e-02,  3.6368e-02,\n",
      "          1.0513e-02, -7.2604e-02,  2.7405e-02,  3.2348e-02,  6.6047e-02,\n",
      "         -8.9388e-02, -9.5705e-03, -8.7862e-02, -1.8665e-02, -7.7607e-02,\n",
      "          5.6434e-02,  1.8542e-02,  4.3302e-02, -7.3788e-02,  9.4835e-02,\n",
      "          3.2261e-02,  8.4641e-03, -6.3718e-02,  2.6133e-02, -5.2875e-02,\n",
      "         -5.1237e-02, -9.2115e-02,  8.5649e-04, -8.3103e-02,  1.5132e-02,\n",
      "         -7.9949e-02,  4.1879e-02,  8.4562e-02, -2.2532e-02, -2.0678e-02,\n",
      "         -5.1660e-02,  4.7627e-02,  4.6511e-02, -7.9319e-03,  9.2112e-02,\n",
      "          2.2790e-02, -2.2607e-03, -3.2094e-02, -4.7863e-03, -6.5579e-02,\n",
      "          9.7489e-02, -9.6272e-02,  1.9435e-02,  5.8480e-02,  8.3919e-02,\n",
      "         -9.6732e-02, -5.4389e-02,  2.4498e-02, -8.0708e-03,  8.2535e-02,\n",
      "         -4.7550e-02, -9.9820e-03, -6.1000e-02, -7.9110e-02, -1.9689e-02,\n",
      "          8.5085e-03,  4.7214e-02, -6.1535e-02,  3.4413e-02,  5.6754e-02,\n",
      "          2.8137e-02,  1.0949e-03, -2.9157e-03, -5.3513e-02,  5.8443e-02,\n",
      "         -7.9493e-02,  4.0191e-02,  1.5077e-02, -7.9565e-02, -8.7363e-02,\n",
      "          9.9308e-02, -9.5872e-02, -7.2233e-02, -7.6029e-02,  3.0986e-02,\n",
      "         -9.6221e-02, -8.2700e-02, -6.5470e-02, -3.3426e-02,  2.1701e-02],\n",
      "        [ 8.1175e-02,  6.1551e-02, -4.7432e-02, -3.0105e-02,  9.2815e-04,\n",
      "         -1.5903e-02,  2.3412e-02, -1.3095e-02, -2.7923e-02,  5.5392e-02,\n",
      "          8.3386e-04, -5.8269e-02, -2.8251e-02,  3.2362e-02, -5.6777e-02,\n",
      "          3.5942e-03, -5.7993e-02, -1.4377e-02,  4.7796e-02, -8.6057e-03,\n",
      "          5.0407e-03, -4.6538e-02, -8.9750e-02, -9.1923e-02, -7.1630e-02,\n",
      "          4.4465e-02,  6.4089e-02, -8.7785e-02, -9.8351e-02,  8.9895e-02,\n",
      "          8.7010e-02, -6.3592e-02, -4.8032e-02,  9.2335e-02, -7.5520e-02,\n",
      "         -4.4057e-02,  8.4456e-03,  9.1549e-02, -6.6448e-02,  5.1927e-02,\n",
      "         -5.6377e-02, -9.2856e-02,  1.5988e-02,  1.2571e-02,  3.6327e-02,\n",
      "         -1.4555e-02,  5.8901e-02,  6.8793e-02,  9.3615e-03, -8.0020e-02,\n",
      "          8.9650e-04, -8.5622e-02, -4.9367e-02,  6.9567e-02, -3.9429e-02,\n",
      "          1.2474e-03, -6.4485e-02, -1.4056e-02,  4.9275e-02, -4.7021e-02,\n",
      "         -8.1105e-02, -1.8080e-02,  8.9686e-02,  8.5571e-02, -3.6244e-02,\n",
      "          3.9263e-02, -9.6793e-02, -5.3941e-02, -4.6485e-02,  8.0566e-02,\n",
      "          3.8985e-02,  4.9557e-02, -1.3801e-02, -5.3644e-02, -7.3497e-02,\n",
      "          5.1768e-02,  3.9251e-02, -8.1344e-02,  1.8492e-02,  3.0977e-02,\n",
      "         -7.8212e-02,  3.3647e-02, -2.1381e-02, -2.9990e-02, -6.7565e-02,\n",
      "          7.4055e-02,  8.3973e-02, -5.3851e-02, -5.1325e-02,  4.1458e-02,\n",
      "         -6.7742e-02,  1.2918e-02, -3.8855e-02, -7.8544e-02,  7.8942e-02,\n",
      "          3.8437e-02, -3.3739e-02,  2.1816e-02, -4.1892e-02,  9.3223e-02],\n",
      "        [-4.2289e-02,  9.9870e-02,  3.7292e-02, -1.6264e-02, -4.6218e-02,\n",
      "         -6.8812e-02,  3.0861e-02, -8.0141e-02, -8.3251e-02,  6.1606e-02,\n",
      "          9.1478e-03,  3.0515e-02, -7.4788e-02,  6.0113e-02,  1.0151e-02,\n",
      "          5.6943e-02, -8.8710e-03, -8.8408e-02,  6.7161e-02, -1.0899e-02,\n",
      "          2.9636e-02,  2.1718e-03, -1.1979e-02, -7.9583e-02, -1.9891e-02,\n",
      "          9.5940e-02,  5.3815e-02, -4.8887e-02, -6.6223e-02, -6.4370e-02,\n",
      "          9.4526e-03,  8.0737e-02, -2.0229e-02,  8.3322e-02, -2.3342e-02,\n",
      "         -2.2079e-02, -9.2367e-02,  5.1952e-03, -7.6018e-02, -7.0138e-02,\n",
      "          5.3652e-02,  9.4279e-02,  9.3839e-02,  6.3395e-02,  3.4372e-02,\n",
      "         -3.7311e-02,  7.5802e-02, -3.6887e-02, -9.7208e-02, -1.6611e-02,\n",
      "          1.5596e-02, -8.4085e-02,  1.6734e-02, -8.0943e-02,  8.9077e-02,\n",
      "          7.1029e-02,  9.1464e-02, -8.6452e-02,  2.9807e-02, -9.2648e-02,\n",
      "          1.8701e-02, -3.7566e-02, -8.2245e-02, -6.8143e-02, -2.8333e-02,\n",
      "          5.8384e-02, -5.3772e-02,  4.1201e-02,  4.6139e-02, -7.4488e-02,\n",
      "         -9.9698e-02,  5.7023e-02,  6.4482e-02, -5.8361e-02, -5.7179e-02,\n",
      "         -8.1320e-03,  9.8501e-02,  6.1981e-02,  4.8969e-02,  1.8052e-02,\n",
      "          2.1192e-03,  2.2778e-02,  7.2232e-02, -7.1824e-02, -5.8970e-03,\n",
      "          5.8594e-02,  9.7427e-02, -5.7116e-02,  2.5408e-02, -2.3541e-02,\n",
      "         -5.5271e-02,  6.4271e-02, -1.0305e-02, -1.8217e-02,  4.2381e-02,\n",
      "          7.5265e-02,  5.5598e-02,  1.7015e-02, -4.5881e-02,  7.0542e-02],\n",
      "        [ 3.2500e-03,  1.2194e-02,  8.5722e-02,  7.5832e-02,  4.4124e-02,\n",
      "         -1.7731e-02, -2.4316e-02,  8.0110e-02,  9.8513e-02, -8.5949e-02,\n",
      "          8.3673e-02, -5.7956e-02,  1.8685e-02,  2.7850e-02,  7.8043e-02,\n",
      "         -8.9352e-02,  4.1976e-03,  3.3281e-02,  8.0812e-03,  1.4211e-03,\n",
      "         -1.5869e-02,  2.3113e-02, -6.3592e-02,  7.2417e-03,  3.9829e-02,\n",
      "          7.5314e-02, -3.6521e-02,  7.1764e-02, -4.7596e-02, -2.0591e-02,\n",
      "         -2.1024e-02,  3.1523e-03,  4.8202e-03,  7.9834e-02,  5.8597e-02,\n",
      "         -8.6470e-02, -2.7538e-02, -7.6286e-02,  8.5731e-02, -6.3942e-02,\n",
      "          2.6953e-02, -9.0465e-02,  2.1708e-02,  3.6697e-02, -9.8031e-02,\n",
      "         -4.3903e-02, -5.3588e-03,  4.3433e-02,  7.5761e-02,  4.5971e-02,\n",
      "          3.3184e-02, -4.0795e-02,  9.2574e-02, -7.4930e-02, -2.8985e-02,\n",
      "          4.9121e-02,  6.9049e-03,  2.3811e-02,  3.5830e-02,  5.0839e-02,\n",
      "          8.8090e-02, -2.3812e-02, -9.3047e-02, -5.9489e-02,  1.6709e-02,\n",
      "         -6.8139e-02, -7.7808e-02,  8.1374e-02,  7.7264e-02, -1.2394e-02,\n",
      "         -4.7619e-02, -8.8915e-02,  5.3767e-02,  3.1637e-02, -3.7035e-02,\n",
      "          9.6850e-02, -7.7601e-02,  9.9766e-02,  4.9084e-02,  1.1780e-02,\n",
      "         -3.5028e-02, -7.0611e-02, -6.7707e-02, -7.8515e-02,  4.0672e-02,\n",
      "         -1.7835e-02, -3.1134e-02, -1.5831e-02,  4.9186e-03,  3.4256e-02,\n",
      "         -7.3483e-03,  8.1709e-02, -9.9309e-02, -1.3093e-02,  1.8239e-02,\n",
      "          3.6779e-02,  9.9976e-02, -1.6132e-02, -2.5499e-02, -7.4301e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0828,  0.0291, -0.0981,  0.0603,  0.0170,  0.0675, -0.0447,  0.0399,\n",
      "         0.0088,  0.0798], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'true' in name:\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-fighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "stuffed-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "mse_loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "democratic-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = list(model.feature.parameters()) + list(model.true_classifier.parameters())\n",
    "\n",
    "# Optimizers\n",
    "optimizer_T = torch.optim.SGD(true_parameters, lr = 0.1, weight_decay=0.01)\n",
    "optimizer_R = torch.optim.SGD(model.rand_classifier.parameters(), lr=0.1, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-player",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "impressed-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true = True\n",
    "iter_counter = 0\n",
    "changer_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "hundred-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "validloss_all = []\n",
    "validacc_all = []\n",
    "testloss_all = []\n",
    "testacc_all = []\n",
    "trainloss_all = []\n",
    "trainacc_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "intelligent-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "total=0\n",
    "correct_rand=0\n",
    "validacc=0\n",
    "testacc=0\n",
    "validloss=0\n",
    "testloss=0\n",
    "print_every=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "introductory-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "pointed-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomGame(\n",
       "  (feature): LeNet(\n",
       "    (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  )\n",
       "  (true_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (rand_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "gothic-permission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.sum(model.true_classifier.weight)\n",
    "\n",
    "# model.true_classifier.bias\n",
    "\n",
    "# torch.sum(model.feature.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "intellectual-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "id": "frank-ordinary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomGame(\n",
       "  (feature): LeNet(\n",
       "    (fc1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  )\n",
       "  (true_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (rand_classifier): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-christianity",
   "metadata": {},
   "source": [
    "## ce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monetary-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2%1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "novel-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.306, true loss: 2.223, loss: 2.531, train acc: 43.641, rand_train_acc: 9.781, test acc: 9.660, train_true is False, gamma is 0.25\n",
      "[1,   100] rand loss: 2.319, true loss: 1.703, loss: 2.144, train acc: 65.188, rand_train_acc: 10.656, test acc: 9.850, train_true is False, gamma is 0.25\n",
      "[1,   150] rand loss: 2.322, true loss: 0.943, loss: 1.577, train acc: 76.203, rand_train_acc: 12.000, test acc: 9.210, train_true is False, gamma is 0.25\n",
      "[1,   200] rand loss: 2.333, true loss: 0.717, loss: 1.407, train acc: 80.344, rand_train_acc: 12.375, test acc: 9.910, train_true is False, gamma is 0.25\n",
      "[1,   250] rand loss: 2.336, true loss: 0.536, loss: 1.283, train acc: 84.266, rand_train_acc: 12.281, test acc: 10.310, train_true is False, gamma is 0.25\n",
      "[1,   300] rand loss: 2.330, true loss: 0.530, loss: 1.275, train acc: 84.922, rand_train_acc: 12.641, test acc: 10.970, train_true is False, gamma is 0.25\n",
      "[1,   350] rand loss: 2.323, true loss: 0.481, loss: 1.242, train acc: 87.984, rand_train_acc: 13.547, test acc: 10.620, train_true is False, gamma is 0.25\n",
      "[1,   400] rand loss: 2.339, true loss: 0.443, loss: 1.208, train acc: 89.391, rand_train_acc: 12.969, test acc: 10.710, train_true is False, gamma is 0.25\n",
      "[2,    50] rand loss: 2.339, true loss: 0.411, loss: 1.182, train acc: 89.637, rand_train_acc: 12.735, test acc: 11.890, train_true is False, gamma is 0.25\n",
      "[2,   100] rand loss: 2.348, true loss: 0.385, loss: 1.166, train acc: 89.969, rand_train_acc: 12.625, test acc: 12.150, train_true is False, gamma is 0.25\n",
      "[2,   150] rand loss: 2.340, true loss: 0.356, loss: 1.144, train acc: 91.000, rand_train_acc: 12.188, test acc: 12.480, train_true is False, gamma is 0.25\n",
      "[2,   200] rand loss: 2.324, true loss: 0.369, loss: 1.151, train acc: 91.531, rand_train_acc: 13.094, test acc: 12.530, train_true is False, gamma is 0.25\n",
      "[2,   250] rand loss: 2.336, true loss: 0.377, loss: 1.163, train acc: 90.922, rand_train_acc: 13.062, test acc: 12.260, train_true is False, gamma is 0.25\n",
      "[2,   300] rand loss: 2.338, true loss: 0.357, loss: 1.144, train acc: 90.812, rand_train_acc: 12.484, test acc: 12.140, train_true is False, gamma is 0.25\n",
      "[2,   350] rand loss: 2.343, true loss: 0.333, loss: 1.129, train acc: 91.453, rand_train_acc: 13.078, test acc: 12.330, train_true is False, gamma is 0.25\n",
      "[2,   400] rand loss: 2.341, true loss: 0.328, loss: 1.124, train acc: 91.531, rand_train_acc: 12.406, test acc: 12.780, train_true is False, gamma is 0.25\n",
      "[3,    50] rand loss: 2.329, true loss: 0.343, loss: 1.131, train acc: 91.775, rand_train_acc: 13.088, test acc: 12.590, train_true is False, gamma is 0.25\n",
      "[3,   100] rand loss: 2.333, true loss: 0.349, loss: 1.137, train acc: 92.422, rand_train_acc: 13.109, test acc: 13.110, train_true is False, gamma is 0.25\n",
      "[3,   150] rand loss: 2.326, true loss: 0.312, loss: 1.110, train acc: 92.047, rand_train_acc: 13.375, test acc: 13.330, train_true is False, gamma is 0.25\n",
      "[3,   200] rand loss: 2.341, true loss: 0.304, loss: 1.100, train acc: 93.031, rand_train_acc: 12.766, test acc: 12.230, train_true is False, gamma is 0.25\n",
      "[3,   250] rand loss: 2.331, true loss: 0.322, loss: 1.114, train acc: 92.234, rand_train_acc: 13.031, test acc: 13.520, train_true is False, gamma is 0.25\n",
      "[3,   300] rand loss: 2.344, true loss: 0.307, loss: 1.105, train acc: 92.500, rand_train_acc: 12.750, test acc: 13.120, train_true is False, gamma is 0.25\n",
      "[3,   350] rand loss: 2.340, true loss: 0.290, loss: 1.087, train acc: 92.516, rand_train_acc: 12.656, test acc: 13.600, train_true is False, gamma is 0.25\n",
      "[3,   400] rand loss: 2.340, true loss: 0.253, loss: 1.066, train acc: 93.500, rand_train_acc: 12.859, test acc: 13.270, train_true is False, gamma is 0.25\n",
      "[4,    50] rand loss: 2.332, true loss: 0.311, loss: 1.109, train acc: 92.814, rand_train_acc: 13.333, test acc: 13.810, train_true is False, gamma is 0.25\n",
      "[4,   100] rand loss: 2.332, true loss: 0.263, loss: 1.071, train acc: 93.719, rand_train_acc: 13.125, test acc: 13.670, train_true is False, gamma is 0.25\n",
      "[4,   150] rand loss: 2.329, true loss: 0.268, loss: 1.077, train acc: 93.391, rand_train_acc: 13.703, test acc: 14.100, train_true is False, gamma is 0.25\n",
      "[4,   200] rand loss: 2.329, true loss: 0.255, loss: 1.070, train acc: 93.359, rand_train_acc: 13.891, test acc: 13.330, train_true is False, gamma is 0.25\n",
      "[4,   250] rand loss: 2.339, true loss: 0.268, loss: 1.076, train acc: 93.609, rand_train_acc: 13.172, test acc: 13.260, train_true is False, gamma is 0.25\n",
      "[4,   300] rand loss: 2.331, true loss: 0.292, loss: 1.091, train acc: 93.547, rand_train_acc: 12.984, test acc: 14.210, train_true is False, gamma is 0.25\n",
      "[4,   350] rand loss: 2.327, true loss: 0.268, loss: 1.075, train acc: 93.562, rand_train_acc: 12.562, test acc: 13.990, train_true is False, gamma is 0.25\n",
      "[4,   400] rand loss: 2.332, true loss: 0.285, loss: 1.087, train acc: 93.609, rand_train_acc: 12.703, test acc: 14.160, train_true is False, gamma is 0.25\n",
      "[5,    50] rand loss: 2.330, true loss: 0.277, loss: 1.084, train acc: 94.059, rand_train_acc: 13.265, test acc: 14.330, train_true is False, gamma is 0.25\n",
      "[5,   100] rand loss: 2.328, true loss: 0.251, loss: 1.062, train acc: 94.188, rand_train_acc: 13.125, test acc: 14.110, train_true is False, gamma is 0.25\n",
      "[5,   150] rand loss: 2.336, true loss: 0.259, loss: 1.068, train acc: 94.125, rand_train_acc: 12.953, test acc: 14.480, train_true is False, gamma is 0.25\n",
      "[5,   200] rand loss: 2.331, true loss: 0.233, loss: 1.047, train acc: 94.266, rand_train_acc: 12.844, test acc: 14.450, train_true is False, gamma is 0.25\n",
      "[5,   250] rand loss: 2.340, true loss: 0.265, loss: 1.069, train acc: 93.828, rand_train_acc: 12.234, test acc: 15.080, train_true is False, gamma is 0.25\n",
      "[5,   300] rand loss: 2.339, true loss: 0.232, loss: 1.047, train acc: 94.203, rand_train_acc: 12.219, test acc: 14.360, train_true is False, gamma is 0.25\n",
      "[5,   350] rand loss: 2.333, true loss: 0.219, loss: 1.040, train acc: 94.609, rand_train_acc: 12.828, test acc: 15.760, train_true is False, gamma is 0.25\n",
      "[5,   400] rand loss: 2.330, true loss: 0.241, loss: 1.054, train acc: 94.422, rand_train_acc: 13.031, test acc: 15.010, train_true is False, gamma is 0.25\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_true=True\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if train_true:\n",
    "            # zero the parameter gradients\n",
    "            optimizer_T.zero_grad()        \n",
    "            model.train_true()\n",
    "            outputs_true = model(inputs, True)\n",
    "            outputs_rand = model(inputs, False)\n",
    "\n",
    "            true_loss=ce_loss_fn(outputs_true, labels)\n",
    "            rand_loss=ce_loss_fn(outputs_rand, rand_labels)\n",
    "            loss = 1.5*true_loss - gamma*rand_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_T.step()\n",
    "        \n",
    "        \n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "#             for k in range(20):\n",
    "            optimizer_R.zero_grad()\n",
    "            outputs_rand = model(inputs, False)\n",
    "            loss = ce_loss_fn(outputs_rand, rand_labels)\n",
    "#             print(loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "#                 train_true=True\n",
    "\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "        \n",
    "        if iter_counter >= 1 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "#         if i % 50 == 49:\n",
    "#             gamma *= 1.1\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "alternate-hammer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "going-charger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.298, true loss: 1.917, loss: 2.645, train acc: 51.828, rand_train_acc: 10.859, test acc: 19.920, train_true is True, gamma is 0.10\n",
      "[1,   100] rand loss: 2.264, true loss: 0.746, loss: 0.893, train acc: 78.375, rand_train_acc: 15.391, test acc: 23.990, train_true is True, gamma is 0.10\n",
      "[1,   150] rand loss: 2.247, true loss: 0.494, loss: 0.539, train acc: 85.188, rand_train_acc: 15.672, test acc: 42.090, train_true is True, gamma is 0.09\n",
      "[1,   200] rand loss: 2.249, true loss: 0.413, loss: 0.417, train acc: 88.141, rand_train_acc: 15.984, test acc: 33.610, train_true is True, gamma is 0.09\n",
      "[1,   250] rand loss: 2.246, true loss: 0.364, loss: 0.364, train acc: 89.531, rand_train_acc: 16.344, test acc: 42.690, train_true is True, gamma is 0.08\n",
      "[1,   300] rand loss: 2.248, true loss: 0.338, loss: 0.325, train acc: 89.844, rand_train_acc: 15.859, test acc: 40.320, train_true is True, gamma is 0.08\n",
      "[1,   350] rand loss: 2.248, true loss: 0.316, loss: 0.311, train acc: 90.719, rand_train_acc: 15.234, test acc: 38.470, train_true is True, gamma is 0.07\n",
      "[1,   400] rand loss: 2.247, true loss: 0.291, loss: 0.273, train acc: 91.594, rand_train_acc: 16.281, test acc: 42.230, train_true is True, gamma is 0.07\n",
      "[2,    50] rand loss: 2.246, true loss: 0.255, loss: 0.236, train acc: 92.480, rand_train_acc: 16.176, test acc: 37.000, train_true is True, gamma is 0.07\n",
      "[2,   100] rand loss: 2.249, true loss: 0.239, loss: 0.212, train acc: 93.109, rand_train_acc: 16.234, test acc: 28.530, train_true is True, gamma is 0.07\n",
      "[2,   150] rand loss: 2.255, true loss: 0.250, loss: 0.241, train acc: 92.750, rand_train_acc: 15.859, test acc: 36.140, train_true is True, gamma is 0.06\n",
      "[2,   200] rand loss: 2.252, true loss: 0.233, loss: 0.217, train acc: 93.438, rand_train_acc: 16.531, test acc: 34.740, train_true is True, gamma is 0.06\n",
      "[2,   250] rand loss: 2.256, true loss: 0.222, loss: 0.213, train acc: 93.812, rand_train_acc: 15.516, test acc: 35.490, train_true is True, gamma is 0.05\n",
      "[2,   300] rand loss: 2.254, true loss: 0.205, loss: 0.188, train acc: 93.891, rand_train_acc: 15.672, test acc: 20.250, train_true is True, gamma is 0.05\n",
      "[2,   350] rand loss: 2.251, true loss: 0.189, loss: 0.175, train acc: 94.469, rand_train_acc: 16.266, test acc: 27.750, train_true is True, gamma is 0.05\n",
      "[2,   400] rand loss: 2.255, true loss: 0.199, loss: 0.191, train acc: 94.281, rand_train_acc: 16.000, test acc: 30.930, train_true is True, gamma is 0.05\n",
      "[3,    50] rand loss: 2.251, true loss: 0.171, loss: 0.159, train acc: 94.588, rand_train_acc: 16.196, test acc: 37.780, train_true is True, gamma is 0.04\n",
      "[3,   100] rand loss: 2.254, true loss: 0.163, loss: 0.147, train acc: 95.266, rand_train_acc: 15.922, test acc: 27.730, train_true is True, gamma is 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-586-92b7c2bf168c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#             validloss, validacc = test(validloader, model, bn_eval=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mtestloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_c_bright_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#             validloss_all.append(validloss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e968ef14151e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, save, epoch, bn_eval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         inputs, targets = next(iter(loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyreg_dispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_reducers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_true=False\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "            for k in range(1):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = ce_loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "                train_true=True\n",
    "        \n",
    "        model.train_true()\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=ce_loss_fn(outputs_true, labels)\n",
    "        rand_loss=ce_loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 1.5*true_loss - gamma*rand_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            gamma *= 0.9\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "interested-squad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-marriage",
   "metadata": {},
   "source": [
    "## train without game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "endangered-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.187, train acc: 43.438, test acc: 12.080, train_true is True, gamma is 0.10\n",
      "[1,   100] loss: 1.297, train acc: 72.594, test acc: 10.610, train_true is True, gamma is 0.10\n",
      "[1,   150] loss: 0.639, train acc: 82.391, test acc: 9.250, train_true is True, gamma is 0.10\n",
      "[1,   200] loss: 0.474, train acc: 86.656, test acc: 10.200, train_true is True, gamma is 0.10\n",
      "[1,   250] loss: 0.424, train acc: 87.922, test acc: 10.110, train_true is True, gamma is 0.10\n",
      "[1,   300] loss: 0.391, train acc: 88.578, test acc: 11.690, train_true is True, gamma is 0.10\n",
      "[1,   350] loss: 0.381, train acc: 89.078, test acc: 11.770, train_true is True, gamma is 0.10\n",
      "[1,   400] loss: 0.354, train acc: 89.703, test acc: 12.020, train_true is True, gamma is 0.10\n",
      "[2,    50] loss: 0.319, train acc: 91.147, test acc: 12.540, train_true is True, gamma is 0.10\n",
      "[2,   100] loss: 0.312, train acc: 91.031, test acc: 12.600, train_true is True, gamma is 0.10\n",
      "[2,   150] loss: 0.304, train acc: 91.422, test acc: 13.510, train_true is True, gamma is 0.10\n",
      "[2,   200] loss: 0.286, train acc: 91.797, test acc: 13.560, train_true is True, gamma is 0.10\n",
      "[2,   250] loss: 0.292, train acc: 91.734, test acc: 14.220, train_true is True, gamma is 0.10\n",
      "[2,   300] loss: 0.270, train acc: 91.984, test acc: 14.000, train_true is True, gamma is 0.10\n",
      "[2,   350] loss: 0.255, train acc: 92.422, test acc: 14.220, train_true is True, gamma is 0.10\n",
      "[2,   400] loss: 0.252, train acc: 92.500, test acc: 14.620, train_true is True, gamma is 0.10\n",
      "[3,    50] loss: 0.230, train acc: 93.284, test acc: 14.980, train_true is True, gamma is 0.10\n",
      "[3,   100] loss: 0.235, train acc: 93.250, test acc: 14.700, train_true is True, gamma is 0.10\n",
      "[3,   150] loss: 0.217, train acc: 93.656, test acc: 15.360, train_true is True, gamma is 0.10\n",
      "[3,   200] loss: 0.221, train acc: 93.391, test acc: 15.520, train_true is True, gamma is 0.10\n",
      "[3,   250] loss: 0.198, train acc: 94.062, test acc: 15.910, train_true is True, gamma is 0.10\n",
      "[3,   300] loss: 0.215, train acc: 94.047, test acc: 15.880, train_true is True, gamma is 0.10\n",
      "[3,   350] loss: 0.196, train acc: 94.094, test acc: 15.900, train_true is True, gamma is 0.10\n",
      "[3,   400] loss: 0.203, train acc: 94.094, test acc: 16.400, train_true is True, gamma is 0.10\n",
      "[4,    50] loss: 0.177, train acc: 94.559, test acc: 17.380, train_true is True, gamma is 0.10\n",
      "[4,   100] loss: 0.163, train acc: 95.125, test acc: 16.850, train_true is True, gamma is 0.10\n",
      "[4,   150] loss: 0.176, train acc: 95.281, test acc: 17.330, train_true is True, gamma is 0.10\n",
      "[4,   200] loss: 0.171, train acc: 95.000, test acc: 16.940, train_true is True, gamma is 0.10\n",
      "[4,   250] loss: 0.174, train acc: 95.047, test acc: 16.870, train_true is True, gamma is 0.10\n",
      "[4,   300] loss: 0.154, train acc: 95.484, test acc: 17.850, train_true is True, gamma is 0.10\n",
      "[4,   350] loss: 0.168, train acc: 95.109, test acc: 17.580, train_true is True, gamma is 0.10\n",
      "[4,   400] loss: 0.159, train acc: 95.344, test acc: 18.000, train_true is True, gamma is 0.10\n",
      "[5,    50] loss: 0.133, train acc: 95.961, test acc: 18.270, train_true is True, gamma is 0.10\n",
      "[5,   100] loss: 0.140, train acc: 95.875, test acc: 18.650, train_true is True, gamma is 0.10\n",
      "[5,   150] loss: 0.137, train acc: 96.109, test acc: 18.730, train_true is True, gamma is 0.10\n",
      "[5,   200] loss: 0.136, train acc: 95.922, test acc: 18.890, train_true is True, gamma is 0.10\n",
      "[5,   250] loss: 0.139, train acc: 96.172, test acc: 18.600, train_true is True, gamma is 0.10\n",
      "[5,   300] loss: 0.126, train acc: 96.391, test acc: 18.490, train_true is True, gamma is 0.10\n",
      "[5,   350] loss: 0.136, train acc: 96.156, test acc: 18.280, train_true is True, gamma is 0.10\n",
      "[5,   400] loss: 0.133, train acc: 96.094, test acc: 19.280, train_true is True, gamma is 0.10\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model.train_true()\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "#         rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        outputs_true = model(inputs, True)\n",
    "        \n",
    "        loss=ce_loss_fn(outputs_true, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] loss: %.3f, train acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 1 and train_true:\n",
    "#             train_true = False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "#         if i % 100 == 99:\n",
    "#             gamma *= 1.1\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-webmaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "bronze-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "selected-configuration",
   "metadata": {},
   "source": [
    "## mse version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "smart-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 0.105, true loss: 1.858, loss: 2.682, train acc: 52.141, rand_train_acc: 10.562, test acc: 29.930, train_true is True, gamma is 1.00\n",
      "[1,   100] rand loss: 0.100, true loss: 0.752, loss: 1.029, train acc: 77.422, rand_train_acc: 12.078, test acc: 34.550, train_true is True, gamma is 1.00\n",
      "[1,   150] rand loss: 94.333, true loss: 6.696, loss: -74.855, train acc: 53.484, rand_train_acc: 10.391, test acc: 17.630, train_true is True, gamma is 0.90\n",
      "[1,   200] rand loss: 4932.527, true loss: 92.846, loss: -4300.005, train acc: 19.750, rand_train_acc: 11.344, test acc: 9.800, train_true is True, gamma is 0.90\n",
      "[1,   250] rand loss: 44288.547, true loss: 363.182, loss: -35328.950, train acc: 11.594, rand_train_acc: 10.703, test acc: 9.820, train_true is True, gamma is 0.81\n",
      "[1,   300] rand loss: 185581.833, true loss: 797.359, loss: -149125.247, train acc: 10.688, rand_train_acc: 10.094, test acc: 9.580, train_true is True, gamma is 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-570-8132297249fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#             validloss, validacc = test(validloader, model, bn_eval=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mtestloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_c_bright_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#             validloss_all.append(validloss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e968ef14151e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader, model, save, epoch, bn_eval)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for batch_idx in tqdm.tqdm(range(tot_iters), total=tot_iters):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         inputs, targets = next(iter(loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, (data, rand_labels) in enumerate(zip(trainloader, rand_labels_train), 0):\n",
    "#     for i, data in enumerate(trainloader):\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels_oh = torch.nn.functional.one_hot(rand_labels).float()\n",
    "#         rand_labels = torch.randint(0, 10, labels.shape)\n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels, rand_labels_oh = inputs.cuda(), labels.cuda(), rand_labels.cuda(), rand_labels_oh.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        if not train_true:\n",
    "            model.train_rand()\n",
    "            for k in range(1):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = mse_loss_fn(outputs_rand, rand_labels_oh)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer_R.step()\n",
    "#                 print(loss.cpu().detach().numpy())\n",
    "                train_true=True\n",
    "        \n",
    "        model.train_true()\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=ce_loss_fn(outputs_true, labels)\n",
    "        rand_loss=mse_loss_fn(outputs_rand, rand_labels_oh)\n",
    "        loss = 1.5*true_loss - gamma*rand_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer_T.step()\n",
    "        \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "            testloss, testacc = test(mnist_c_bright_trainloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "            testloss_all.append(testloss)\n",
    "            testacc_all.append(testacc)\n",
    "            trainloss_all.append(running_loss / print_every)\n",
    "            trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, test acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, \n",
    "                   correct.numpy() / total * 100, correct_rand.numpy() / total * 100, testacc, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 1 and train_true:\n",
    "            train_true = False\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            gamma *= 0.9\n",
    "        \n",
    "#     if epoch > 2:\n",
    "#         print('setting gamma to 0 at epoch %d' % epoch)\n",
    "#         gamma = 0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "tamil-fishing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "impaired-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-george",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-stephen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-captain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-layer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-rogers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 2.5*true_loss - gamma*(rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-shopper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-charlotte",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-claim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-fossil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-bread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-tournament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-healing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-investor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-bidder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "biological-palmer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.376, true loss: 1.603, loss: 1.633, train acc: 54.719, rand_train_acc: 12.016, train_true is False, gamma is 1.00\n",
      "[1,   100] rand loss: 2.593, true loss: 0.681, loss: -0.891, train acc: 77.750, rand_train_acc: 14.016, train_true is False, gamma is 1.00\n",
      "[1,   150] rand loss: 2.909, true loss: 0.607, loss: -1.392, train acc: 82.625, rand_train_acc: 14.016, train_true is False, gamma is 1.00\n",
      "[1,   200] rand loss: 3.747, true loss: 1.716, loss: 0.544, train acc: 76.094, rand_train_acc: 13.062, train_true is False, gamma is 1.00\n",
      "[1,   250] rand loss: 3.135, true loss: 0.891, loss: -0.908, train acc: 82.094, rand_train_acc: 13.594, train_true is False, gamma is 1.00\n",
      "[1,   300] rand loss: 2.755, true loss: 0.418, loss: -1.709, train acc: 87.891, rand_train_acc: 14.734, train_true is False, gamma is 1.00\n",
      "[1,   350] rand loss: 4.828, true loss: 2.030, loss: 0.488, train acc: 78.219, rand_train_acc: 14.141, train_true is False, gamma is 0.95\n",
      "[1,   400] rand loss: 2.674, true loss: 0.444, loss: -1.431, train acc: 87.781, rand_train_acc: 13.906, train_true is False, gamma is 0.95\n",
      "[2,    50] rand loss: 3.198, true loss: 0.805, loss: -1.025, train acc: 86.461, rand_train_acc: 14.520, train_true is False, gamma is 0.95\n",
      "[2,   100] rand loss: 3.062, true loss: 0.439, loss: -1.811, train acc: 88.906, rand_train_acc: 14.578, train_true is False, gamma is 0.95\n",
      "[2,   150] rand loss: 4.355, true loss: 1.320, loss: -0.836, train acc: 84.219, rand_train_acc: 14.453, train_true is False, gamma is 0.95\n",
      "[2,   200] rand loss: 6.064, true loss: 2.331, loss: 0.068, train acc: 83.359, rand_train_acc: 13.906, train_true is False, gamma is 0.95\n",
      "[2,   250] rand loss: 2.765, true loss: 0.471, loss: -1.450, train acc: 88.516, rand_train_acc: 14.656, train_true is False, gamma is 0.95\n",
      "[2,   300] rand loss: 2.873, true loss: 0.365, loss: -1.817, train acc: 91.312, rand_train_acc: 15.250, train_true is False, gamma is 0.95\n",
      "[2,   350] rand loss: 5.584, true loss: 3.077, loss: 2.654, train acc: 80.844, rand_train_acc: 14.281, train_true is False, gamma is 0.90\n",
      "[2,   400] rand loss: 2.845, true loss: 0.470, loss: -1.393, train acc: 87.953, rand_train_acc: 13.672, train_true is False, gamma is 0.90\n",
      "[3,    50] rand loss: 2.939, true loss: 0.427, loss: -1.584, train acc: 90.755, rand_train_acc: 15.000, train_true is False, gamma is 0.90\n",
      "[3,   100] rand loss: 4.265, true loss: 0.773, loss: -1.918, train acc: 88.234, rand_train_acc: 13.969, train_true is False, gamma is 0.90\n",
      "[3,   150] rand loss: 4.063, true loss: 0.600, loss: -2.168, train acc: 90.203, rand_train_acc: 14.578, train_true is False, gamma is 0.90\n",
      "[3,   200] rand loss: 7.477, true loss: 1.509, loss: -2.977, train acc: 84.688, rand_train_acc: 13.188, train_true is False, gamma is 0.90\n",
      "[3,   250] rand loss: 9.488, true loss: 2.401, loss: -2.559, train acc: 86.594, rand_train_acc: 15.016, train_true is False, gamma is 0.90\n",
      "[3,   300] rand loss: 12.698, true loss: 3.332, loss: -3.131, train acc: 81.500, rand_train_acc: 14.141, train_true is False, gamma is 0.90\n",
      "[3,   350] rand loss: 6.292, true loss: 1.935, loss: -0.556, train acc: 82.438, rand_train_acc: 14.406, train_true is False, gamma is 0.86\n",
      "[3,   400] rand loss: 3.303, true loss: 0.459, loss: -1.684, train acc: 91.078, rand_train_acc: 16.312, train_true is False, gamma is 0.86\n",
      "[4,    50] rand loss: 19.926, true loss: 7.648, loss: 2.036, train acc: 81.176, rand_train_acc: 14.225, train_true is False, gamma is 0.86\n",
      "[4,   100] rand loss: 22.956, true loss: 5.270, loss: -6.507, train acc: 80.047, rand_train_acc: 14.438, train_true is False, gamma is 0.86\n",
      "[4,   150] rand loss: 4.113, true loss: 0.626, loss: -1.962, train acc: 89.078, rand_train_acc: 15.844, train_true is False, gamma is 0.86\n",
      "[4,   200] rand loss: 25.407, true loss: 13.358, loss: 11.610, train acc: 78.062, rand_train_acc: 14.578, train_true is False, gamma is 0.86\n",
      "[4,   250] rand loss: 66.027, true loss: 43.958, loss: 53.285, train acc: 41.328, rand_train_acc: 12.312, train_true is False, gamma is 0.86\n",
      "[4,   300] rand loss: 3.024, true loss: 0.842, loss: -0.487, train acc: 73.109, rand_train_acc: 13.203, train_true is False, gamma is 0.86\n",
      "[4,   350] rand loss: 3.034, true loss: 0.584, loss: -1.012, train acc: 83.953, rand_train_acc: 14.875, train_true is False, gamma is 0.81\n",
      "[4,   400] rand loss: 3.104, true loss: 0.545, loss: -1.165, train acc: 86.016, rand_train_acc: 15.391, train_true is False, gamma is 0.81\n",
      "[5,    50] rand loss: 2.458, true loss: 1.809, loss: 2.519, train acc: 31.471, rand_train_acc: 11.255, train_true is False, gamma is 0.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-de1532c3544d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer_R\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0moutputs_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#             print(loss.cpu().detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-67da3d9a33d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, true_flag)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fsl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 2.5*true_loss - gamma*(rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "likely-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "flush-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] rand loss: 2.316, true loss: 2.167, loss: 2.167, train acc: 44.484, rand_train_acc: 11.719, train_true is False, gamma is 1.00\n",
      "[1,   100] rand loss: 2.420, true loss: 1.231, loss: 1.231, train acc: 69.891, rand_train_acc: 12.703, train_true is False, gamma is 1.00\n",
      "[1,   150] rand loss: 2.442, true loss: 0.647, loss: 0.647, train acc: 80.812, rand_train_acc: 13.125, train_true is False, gamma is 1.00\n",
      "[1,   200] rand loss: 2.504, true loss: 0.490, loss: 0.490, train acc: 85.234, rand_train_acc: 12.531, train_true is False, gamma is 1.00\n",
      "[1,   250] rand loss: 2.473, true loss: 0.433, loss: 0.433, train acc: 87.203, rand_train_acc: 13.781, train_true is False, gamma is 1.00\n",
      "[1,   300] rand loss: 2.527, true loss: 0.395, loss: 0.395, train acc: 88.547, rand_train_acc: 13.406, train_true is False, gamma is 1.00\n",
      "[1,   350] rand loss: 2.512, true loss: 0.367, loss: 0.367, train acc: 89.750, rand_train_acc: 13.828, train_true is False, gamma is 1.10\n",
      "[1,   400] rand loss: 2.509, true loss: 0.360, loss: 0.360, train acc: 89.750, rand_train_acc: 13.703, train_true is False, gamma is 1.10\n",
      "[2,    50] rand loss: 2.538, true loss: 0.332, loss: 0.332, train acc: 89.990, rand_train_acc: 13.745, train_true is False, gamma is 1.10\n",
      "[2,   100] rand loss: 2.514, true loss: 0.313, loss: 0.313, train acc: 90.578, rand_train_acc: 13.344, train_true is False, gamma is 1.10\n",
      "[2,   150] rand loss: 2.508, true loss: 0.279, loss: 0.279, train acc: 91.734, rand_train_acc: 14.062, train_true is False, gamma is 1.10\n",
      "[2,   200] rand loss: 2.563, true loss: 0.281, loss: 0.281, train acc: 91.844, rand_train_acc: 14.891, train_true is False, gamma is 1.10\n",
      "[2,   250] rand loss: 2.506, true loss: 0.275, loss: 0.275, train acc: 91.922, rand_train_acc: 14.078, train_true is False, gamma is 1.10\n",
      "[2,   300] rand loss: 2.521, true loss: 0.258, loss: 0.258, train acc: 92.297, rand_train_acc: 14.203, train_true is False, gamma is 1.10\n",
      "[2,   350] rand loss: 2.540, true loss: 0.255, loss: 0.255, train acc: 93.031, rand_train_acc: 13.750, train_true is False, gamma is 1.21\n",
      "[2,   400] rand loss: 2.544, true loss: 0.255, loss: 0.255, train acc: 92.469, rand_train_acc: 13.531, train_true is False, gamma is 1.21\n",
      "[3,    50] rand loss: 2.543, true loss: 0.227, loss: 0.227, train acc: 93.343, rand_train_acc: 13.931, train_true is False, gamma is 1.21\n",
      "[3,   100] rand loss: 2.554, true loss: 0.218, loss: 0.218, train acc: 93.547, rand_train_acc: 13.875, train_true is False, gamma is 1.21\n",
      "[3,   150] rand loss: 2.524, true loss: 0.201, loss: 0.201, train acc: 94.266, rand_train_acc: 14.719, train_true is False, gamma is 1.21\n",
      "[3,   200] rand loss: 2.520, true loss: 0.215, loss: 0.215, train acc: 93.953, rand_train_acc: 14.109, train_true is False, gamma is 1.21\n",
      "[3,   250] rand loss: 2.561, true loss: 0.213, loss: 0.213, train acc: 93.844, rand_train_acc: 14.703, train_true is False, gamma is 1.21\n",
      "[3,   300] rand loss: 2.546, true loss: 0.185, loss: 0.185, train acc: 94.656, rand_train_acc: 14.172, train_true is False, gamma is 1.21\n",
      "[3,   350] rand loss: 2.520, true loss: 0.213, loss: 0.213, train acc: 93.281, rand_train_acc: 14.422, train_true is False, gamma is 1.33\n",
      "[3,   400] rand loss: 2.551, true loss: 0.188, loss: 0.188, train acc: 94.719, rand_train_acc: 14.547, train_true is False, gamma is 1.33\n",
      "[4,    50] rand loss: 2.521, true loss: 0.172, loss: 0.172, train acc: 94.961, rand_train_acc: 14.696, train_true is False, gamma is 1.33\n",
      "[4,   100] rand loss: 2.510, true loss: 0.189, loss: 0.189, train acc: 94.516, rand_train_acc: 14.500, train_true is False, gamma is 1.33\n",
      "[4,   150] rand loss: 2.552, true loss: 0.176, loss: 0.176, train acc: 94.812, rand_train_acc: 15.125, train_true is False, gamma is 1.33\n",
      "[4,   200] rand loss: 2.526, true loss: 0.160, loss: 0.160, train acc: 95.203, rand_train_acc: 13.453, train_true is False, gamma is 1.33\n",
      "[4,   250] rand loss: 2.543, true loss: 0.152, loss: 0.152, train acc: 95.125, rand_train_acc: 14.125, train_true is False, gamma is 1.33\n",
      "[4,   300] rand loss: 2.571, true loss: 0.150, loss: 0.150, train acc: 95.953, rand_train_acc: 13.922, train_true is False, gamma is 1.33\n",
      "[4,   350] rand loss: 2.553, true loss: 0.160, loss: 0.160, train acc: 95.016, rand_train_acc: 14.156, train_true is False, gamma is 1.46\n",
      "[4,   400] rand loss: 2.519, true loss: 0.146, loss: 0.146, train acc: 95.953, rand_train_acc: 14.484, train_true is False, gamma is 1.46\n",
      "[5,    50] rand loss: 2.554, true loss: 0.143, loss: 0.143, train acc: 95.735, rand_train_acc: 14.510, train_true is False, gamma is 1.46\n",
      "[5,   100] rand loss: 2.543, true loss: 0.127, loss: 0.127, train acc: 96.422, rand_train_acc: 14.734, train_true is False, gamma is 1.46\n",
      "[5,   150] rand loss: 2.534, true loss: 0.134, loss: 0.134, train acc: 96.375, rand_train_acc: 14.391, train_true is False, gamma is 1.46\n",
      "[5,   200] rand loss: 2.556, true loss: 0.134, loss: 0.134, train acc: 95.969, rand_train_acc: 14.422, train_true is False, gamma is 1.46\n",
      "[5,   250] rand loss: 2.522, true loss: 0.134, loss: 0.134, train acc: 96.297, rand_train_acc: 14.938, train_true is False, gamma is 1.46\n",
      "[5,   300] rand loss: 2.544, true loss: 0.137, loss: 0.137, train acc: 95.969, rand_train_acc: 14.922, train_true is False, gamma is 1.46\n",
      "[5,   350] rand loss: 2.566, true loss: 0.137, loss: 0.137, train acc: 96.062, rand_train_acc: 14.328, train_true is False, gamma is 1.61\n",
      "[5,   400] rand loss: 2.570, true loss: 0.131, loss: 0.131, train acc: 96.219, rand_train_acc: 15.078, train_true is False, gamma is 1.61\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_true = 0.0\n",
    "    running_loss_rand = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward + backward + optimize\n",
    "     \n",
    "        if train_true:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'rand' not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "            for k in range(100):\n",
    "                optimizer_R.zero_grad()\n",
    "                outputs_rand = model(inputs, False)\n",
    "                loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "    #             print(loss.cpu().detach().numpy())\n",
    "                loss.backward()\n",
    "                optimizer_R.step()\n",
    "    #         print(loss.cpu().detach().numpy())\n",
    "            train_true=False\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'rand' not in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        true_loss=model.loss_fn(outputs_true, labels)\n",
    "        rand_loss=model.loss_fn(outputs_rand, rand_labels)\n",
    "        loss = 1*true_loss# - gamma*(2.3-rand_loss)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_T.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_true += true_loss.item()\n",
    "        running_loss_rand += rand_loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] rand loss: %.3f, true loss: %.3f, loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss_rand / print_every, running_loss_true / print_every, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            running_loss_true = 0.0\n",
    "            running_loss_rand = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 5 and not train_true:\n",
    "            train_true = True\n",
    "            iter_counter = 0\n",
    "            \n",
    "#         iter_counter += 1\n",
    "#         if iter_counter >= 2 and train_true:\n",
    "#             train_true=False\n",
    "#             iter_counter = 0\n",
    "            \n",
    "#         if iter_counter >= 150 and not train_true:\n",
    "#             train_true=True\n",
    "#             iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 300 == 299:\n",
    "            gamma *= 1.1\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "chemical-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] train loss: 2.348, train acc: 8.094, rand_train_acc: 10.484, train_true is False, gamma is 0.50\n",
      "[1,   100] train loss: 2.304, train acc: 8.484, rand_train_acc: 9.938, train_true is False, gamma is 0.50\n",
      "[1,   150] train loss: 2.303, train acc: 9.172, rand_train_acc: 9.484, train_true is False, gamma is 0.50\n",
      "[1,   200] train loss: 2.349, train acc: 7.672, rand_train_acc: 10.188, train_true is False, gamma is 0.50\n",
      "[1,   250] train loss: 2.303, train acc: 8.891, rand_train_acc: 9.953, train_true is False, gamma is 0.50\n",
      "[1,   300] train loss: 2.303, train acc: 9.297, rand_train_acc: 9.391, train_true is False, gamma is 0.50\n",
      "[1,   350] train loss: 2.349, train acc: 8.203, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[1,   400] train loss: 2.303, train acc: 9.078, rand_train_acc: 10.312, train_true is False, gamma is 0.50\n",
      "[2,    50] train loss: 2.349, train acc: 8.490, rand_train_acc: 9.833, train_true is False, gamma is 0.50\n",
      "[2,   100] train loss: 2.303, train acc: 8.641, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[2,   150] train loss: 2.303, train acc: 8.844, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[2,   200] train loss: 2.349, train acc: 8.703, rand_train_acc: 10.062, train_true is False, gamma is 0.50\n",
      "[2,   250] train loss: 2.303, train acc: 7.922, rand_train_acc: 10.531, train_true is False, gamma is 0.50\n",
      "[2,   300] train loss: 2.303, train acc: 8.734, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[2,   350] train loss: 2.349, train acc: 8.422, rand_train_acc: 10.422, train_true is False, gamma is 0.50\n",
      "[2,   400] train loss: 2.303, train acc: 8.875, rand_train_acc: 10.484, train_true is False, gamma is 0.50\n",
      "[3,    50] train loss: 2.303, train acc: 8.520, rand_train_acc: 10.157, train_true is False, gamma is 0.50\n",
      "[3,   100] train loss: 2.349, train acc: 8.188, rand_train_acc: 10.578, train_true is False, gamma is 0.50\n",
      "[3,   150] train loss: 2.303, train acc: 8.625, rand_train_acc: 9.719, train_true is False, gamma is 0.50\n",
      "[3,   200] train loss: 2.303, train acc: 8.750, rand_train_acc: 9.516, train_true is False, gamma is 0.50\n",
      "[3,   250] train loss: 2.348, train acc: 8.750, rand_train_acc: 10.375, train_true is False, gamma is 0.50\n",
      "[3,   300] train loss: 2.302, train acc: 8.844, rand_train_acc: 10.562, train_true is False, gamma is 0.50\n",
      "[3,   350] train loss: 2.303, train acc: 8.875, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[3,   400] train loss: 2.349, train acc: 8.422, rand_train_acc: 9.812, train_true is False, gamma is 0.50\n",
      "[4,    50] train loss: 2.303, train acc: 8.373, rand_train_acc: 9.804, train_true is False, gamma is 0.50\n",
      "[4,   100] train loss: 2.349, train acc: 8.750, rand_train_acc: 10.047, train_true is False, gamma is 0.50\n",
      "[4,   150] train loss: 2.303, train acc: 8.922, rand_train_acc: 9.375, train_true is False, gamma is 0.50\n",
      "[4,   200] train loss: 2.303, train acc: 8.641, rand_train_acc: 9.828, train_true is False, gamma is 0.50\n",
      "[4,   250] train loss: 2.349, train acc: 8.281, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[4,   300] train loss: 2.303, train acc: 8.375, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[4,   350] train loss: 2.303, train acc: 8.391, rand_train_acc: 9.516, train_true is False, gamma is 0.50\n",
      "[4,   400] train loss: 2.348, train acc: 9.047, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[5,    50] train loss: 2.303, train acc: 8.980, rand_train_acc: 9.961, train_true is False, gamma is 0.50\n",
      "[5,   100] train loss: 2.303, train acc: 8.203, rand_train_acc: 9.641, train_true is False, gamma is 0.50\n",
      "[5,   150] train loss: 2.349, train acc: 8.906, rand_train_acc: 10.078, train_true is False, gamma is 0.50\n",
      "[5,   200] train loss: 2.303, train acc: 8.391, rand_train_acc: 10.750, train_true is False, gamma is 0.50\n",
      "[5,   250] train loss: 2.303, train acc: 8.406, rand_train_acc: 9.828, train_true is False, gamma is 0.50\n",
      "[5,   300] train loss: 2.349, train acc: 8.172, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[5,   350] train loss: 2.303, train acc: 8.828, rand_train_acc: 9.672, train_true is False, gamma is 0.50\n",
      "[5,   400] train loss: 2.303, train acc: 8.797, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[6,    50] train loss: 2.303, train acc: 8.480, rand_train_acc: 10.412, train_true is False, gamma is 0.50\n",
      "[6,   100] train loss: 2.302, train acc: 9.000, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[6,   150] train loss: 2.348, train acc: 8.688, rand_train_acc: 10.906, train_true is False, gamma is 0.50\n",
      "[6,   200] train loss: 2.303, train acc: 8.031, rand_train_acc: 9.750, train_true is False, gamma is 0.50\n",
      "[6,   250] train loss: 2.303, train acc: 9.188, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[6,   300] train loss: 2.349, train acc: 8.594, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[6,   350] train loss: 2.303, train acc: 8.812, rand_train_acc: 9.906, train_true is False, gamma is 0.50\n",
      "[6,   400] train loss: 2.303, train acc: 8.391, rand_train_acc: 9.969, train_true is False, gamma is 0.50\n",
      "[7,    50] train loss: 2.349, train acc: 8.314, rand_train_acc: 10.353, train_true is False, gamma is 0.50\n",
      "[7,   100] train loss: 2.303, train acc: 8.766, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[7,   150] train loss: 2.303, train acc: 8.656, rand_train_acc: 10.266, train_true is False, gamma is 0.50\n",
      "[7,   200] train loss: 2.348, train acc: 8.656, rand_train_acc: 10.109, train_true is False, gamma is 0.50\n",
      "[7,   250] train loss: 2.303, train acc: 8.422, rand_train_acc: 10.141, train_true is False, gamma is 0.50\n",
      "[7,   300] train loss: 2.303, train acc: 8.609, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[7,   350] train loss: 2.349, train acc: 8.703, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[7,   400] train loss: 2.303, train acc: 8.234, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[8,    50] train loss: 2.349, train acc: 8.745, rand_train_acc: 10.265, train_true is False, gamma is 0.50\n",
      "[8,   100] train loss: 2.303, train acc: 8.328, rand_train_acc: 10.406, train_true is False, gamma is 0.50\n",
      "[8,   150] train loss: 2.303, train acc: 8.594, rand_train_acc: 9.812, train_true is False, gamma is 0.50\n",
      "[8,   200] train loss: 2.349, train acc: 8.453, rand_train_acc: 10.375, train_true is False, gamma is 0.50\n",
      "[8,   250] train loss: 2.304, train acc: 8.672, rand_train_acc: 9.375, train_true is False, gamma is 0.50\n",
      "[8,   300] train loss: 2.303, train acc: 8.547, rand_train_acc: 10.016, train_true is False, gamma is 0.50\n",
      "[8,   350] train loss: 2.349, train acc: 8.531, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[8,   400] train loss: 2.303, train acc: 9.047, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[9,    50] train loss: 2.303, train acc: 8.353, rand_train_acc: 9.745, train_true is False, gamma is 0.50\n",
      "[9,   100] train loss: 2.349, train acc: 8.953, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[9,   150] train loss: 2.303, train acc: 8.828, rand_train_acc: 10.172, train_true is False, gamma is 0.50\n",
      "[9,   200] train loss: 2.303, train acc: 8.078, rand_train_acc: 10.328, train_true is False, gamma is 0.50\n",
      "[9,   250] train loss: 2.349, train acc: 8.719, rand_train_acc: 10.219, train_true is False, gamma is 0.50\n",
      "[9,   300] train loss: 2.303, train acc: 8.719, rand_train_acc: 9.984, train_true is False, gamma is 0.50\n",
      "[9,   350] train loss: 2.303, train acc: 8.375, rand_train_acc: 9.703, train_true is False, gamma is 0.50\n",
      "[9,   400] train loss: 2.349, train acc: 8.891, rand_train_acc: 9.391, train_true is False, gamma is 0.50\n",
      "[10,    50] train loss: 2.303, train acc: 8.343, rand_train_acc: 9.696, train_true is False, gamma is 0.50\n",
      "[10,   100] train loss: 2.349, train acc: 8.344, rand_train_acc: 10.656, train_true is False, gamma is 0.50\n",
      "[10,   150] train loss: 2.303, train acc: 8.469, rand_train_acc: 9.766, train_true is False, gamma is 0.50\n",
      "[10,   200] train loss: 2.303, train acc: 8.625, rand_train_acc: 9.844, train_true is False, gamma is 0.50\n",
      "[10,   250] train loss: 2.349, train acc: 9.062, rand_train_acc: 10.203, train_true is False, gamma is 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10,   300] train loss: 2.303, train acc: 8.469, rand_train_acc: 10.828, train_true is False, gamma is 0.50\n",
      "[10,   350] train loss: 2.303, train acc: 8.922, rand_train_acc: 10.516, train_true is False, gamma is 0.50\n",
      "[10,   400] train loss: 2.349, train acc: 8.797, rand_train_acc: 9.891, train_true is False, gamma is 0.50\n",
      "[11,    50] train loss: 2.302, train acc: 8.255, rand_train_acc: 10.373, train_true is False, gamma is 0.50\n",
      "[11,   100] train loss: 2.303, train acc: 8.594, rand_train_acc: 10.234, train_true is False, gamma is 0.50\n",
      "[11,   150] train loss: 2.349, train acc: 9.000, rand_train_acc: 10.156, train_true is False, gamma is 0.50\n",
      "[11,   200] train loss: 2.303, train acc: 8.484, rand_train_acc: 9.984, train_true is False, gamma is 0.50\n",
      "[11,   250] train loss: 2.303, train acc: 8.266, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[11,   300] train loss: 2.349, train acc: 8.984, rand_train_acc: 10.094, train_true is False, gamma is 0.50\n",
      "[11,   350] train loss: 2.303, train acc: 8.641, rand_train_acc: 9.797, train_true is False, gamma is 0.50\n",
      "[11,   400] train loss: 2.303, train acc: 8.547, rand_train_acc: 9.781, train_true is False, gamma is 0.50\n",
      "[12,    50] train loss: 2.303, train acc: 8.696, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[12,   100] train loss: 2.302, train acc: 8.156, rand_train_acc: 10.516, train_true is False, gamma is 0.50\n",
      "[12,   150] train loss: 2.349, train acc: 8.141, rand_train_acc: 9.953, train_true is False, gamma is 0.50\n",
      "[12,   200] train loss: 2.303, train acc: 8.703, rand_train_acc: 9.438, train_true is False, gamma is 0.50\n",
      "[12,   250] train loss: 2.303, train acc: 8.891, rand_train_acc: 9.422, train_true is False, gamma is 0.50\n",
      "[12,   300] train loss: 2.349, train acc: 8.453, rand_train_acc: 10.328, train_true is False, gamma is 0.50\n",
      "[12,   350] train loss: 2.303, train acc: 8.266, rand_train_acc: 10.078, train_true is False, gamma is 0.50\n",
      "[12,   400] train loss: 2.303, train acc: 8.719, rand_train_acc: 10.062, train_true is False, gamma is 0.50\n",
      "[13,    50] train loss: 2.349, train acc: 8.853, rand_train_acc: 10.657, train_true is False, gamma is 0.50\n",
      "[13,   100] train loss: 2.303, train acc: 8.250, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[13,   150] train loss: 2.303, train acc: 9.031, rand_train_acc: 10.875, train_true is False, gamma is 0.50\n",
      "[13,   200] train loss: 2.348, train acc: 8.562, rand_train_acc: 10.812, train_true is False, gamma is 0.50\n",
      "[13,   250] train loss: 2.303, train acc: 8.266, rand_train_acc: 10.312, train_true is False, gamma is 0.50\n",
      "[13,   300] train loss: 2.302, train acc: 8.375, rand_train_acc: 10.703, train_true is False, gamma is 0.50\n",
      "[13,   350] train loss: 2.349, train acc: 9.281, rand_train_acc: 10.250, train_true is False, gamma is 0.50\n",
      "[13,   400] train loss: 2.303, train acc: 8.859, rand_train_acc: 10.656, train_true is False, gamma is 0.50\n",
      "[14,    50] train loss: 2.349, train acc: 8.402, rand_train_acc: 10.431, train_true is False, gamma is 0.50\n",
      "[14,   100] train loss: 2.303, train acc: 8.484, rand_train_acc: 9.875, train_true is False, gamma is 0.50\n",
      "[14,   150] train loss: 2.303, train acc: 8.438, rand_train_acc: 10.000, train_true is False, gamma is 0.50\n",
      "[14,   200] train loss: 2.350, train acc: 8.578, rand_train_acc: 10.047, train_true is False, gamma is 0.50\n",
      "[14,   250] train loss: 2.303, train acc: 8.844, rand_train_acc: 9.609, train_true is False, gamma is 0.50\n",
      "[14,   300] train loss: 2.303, train acc: 8.547, rand_train_acc: 9.891, train_true is False, gamma is 0.50\n",
      "[14,   350] train loss: 2.349, train acc: 8.266, rand_train_acc: 10.031, train_true is False, gamma is 0.50\n",
      "[14,   400] train loss: 2.303, train acc: 9.031, rand_train_acc: 10.266, train_true is False, gamma is 0.50\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(14):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        rand_labels = torch.randint(0, 10, labels.shape)\n",
    "            \n",
    "        if use_cuda:\n",
    "            inputs, labels, rand_labels = inputs.cuda(), labels.cuda(), rand_labels.cuda()\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer_T.zero_grad()\n",
    "        optimizer_R.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_true = model(inputs, True)\n",
    "        outputs_rand = model(inputs, False)\n",
    "        \n",
    "        if train_true:\n",
    "            loss = 2*model.loss_fn(outputs_true, labels) - gamma*model.loss_fn(outputs_rand, rand_labels)\n",
    "        else:\n",
    "            loss = model.loss_fn(outputs_rand, rand_labels)\n",
    "            \n",
    "        _, predicted = torch.max(nn.Softmax(dim=1)(outputs_true).data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        _, predicted_rand = torch.max(nn.Softmax(dim=1)(outputs_rand).data, 1)\n",
    "        correct_rand += predicted_rand.eq(rand_labels.data).cpu().sum()\n",
    "        \n",
    "#         if i == 0:\n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(loss.item())\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, loss.item(), correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if train_true:\n",
    "            optimizer_T.step()\n",
    "        else:\n",
    "            optimizer_R.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every-1:   \n",
    "#             print(i)\n",
    "#             weight_all, grad_all, variance_all, gsnr_all = save_gsnr_batch(model)\n",
    "#             weight_all_time.append(weight_all)\n",
    "#             grad_all_time.append(grad_all)\n",
    "#             variance_all_time.append(variance_all)\n",
    "#             gsnr_all_time.append(gsnr_all)\n",
    "            \n",
    "#             validloss, validacc = test(validloader, model, bn_eval=False)\n",
    "#             testloss, testacc = test(testloader, model, bn_eval=False)\n",
    "            \n",
    "#             validloss_all.append(validloss)\n",
    "#             validacc_all.append(validacc)\n",
    "#             testloss_all.append(testloss)\n",
    "#             testacc_all.append(testacc)\n",
    "#             trainloss_all.append(running_loss / print_every)\n",
    "#             trainacc_all.append(correct.numpy() / total * 100)\n",
    "            print('[%d, %5d] train loss: %.3f, train acc: %.3f, rand_train_acc: %.3f, train_true is %s, gamma is %.2f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, correct_rand.numpy() / total * 100, train_true, gamma))\n",
    "            \n",
    "#             print('[%d, %5d] train loss: %.3f, train acc: %.3f, val loss: %.3f, val acc: %.3f, test loss: %.3f, test acc: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / print_every, correct.numpy() / total * 100, validloss, validacc, testloss, testacc))\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            correct_rand=0\n",
    "            total = 0\n",
    "        iter_counter += 1\n",
    "        if iter_counter >= 2 and train_true:\n",
    "            train_true=False\n",
    "            iter_counter = 0\n",
    "            \n",
    "        if iter_counter >= 150 and not train_true:\n",
    "            train_true=True\n",
    "            iter_counter = 0\n",
    "#             print('train_true is ' + str(train_true))\n",
    "\n",
    "        if i % 500 == 499:\n",
    "            gamma *= 0.95\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "difficult-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_T.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "third-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_R.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aerial-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "circular-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.cuda()\n",
    "labels=labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "operating-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward + backward + optimize\n",
    "outputs = model(inputs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "undefined-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2884e-02, -1.3882e-02,  1.2817e-02, -9.4555e-04,  3.0092e-02,\n",
       "         1.0486e-02, -3.0941e-02, -4.4655e-03, -3.3369e-02, -1.8193e-02,\n",
       "        -9.4091e-03, -1.3727e-03, -2.6099e-02, -5.4899e-03,  2.5781e-02,\n",
       "         1.6908e-02,  1.7572e-02,  3.4798e-03,  2.0891e-02,  2.8016e-02,\n",
       "        -2.0708e-02,  2.2318e-03, -2.9543e-02,  2.4213e-02, -6.9216e-03,\n",
       "        -1.2713e-02, -3.1164e-02, -3.1157e-02, -2.5164e-02,  1.8522e-02,\n",
       "        -2.3435e-02,  7.9300e-03, -2.5209e-02, -2.6475e-02, -3.1741e-02,\n",
       "        -1.6724e-02, -8.0129e-03,  1.8957e-02, -7.8422e-04, -1.4845e-02,\n",
       "        -1.9399e-02,  3.2636e-02,  3.7357e-03, -5.1018e-03, -1.9835e-02,\n",
       "         6.9270e-03, -1.9509e-02, -3.5025e-02,  2.1834e-02, -1.0204e-02,\n",
       "        -2.1467e-02,  3.0699e-02, -1.5840e-02,  1.4424e-02,  2.7146e-02,\n",
       "         1.8442e-02, -1.6263e-02, -1.9906e-03,  3.1360e-02,  2.9571e-02,\n",
       "         9.1939e-03,  2.5311e-02, -3.4110e-03,  1.5451e-02, -9.0994e-03,\n",
       "        -1.9657e-03,  3.5384e-02,  5.8600e-03,  2.1896e-02,  1.5586e-02,\n",
       "         1.0265e-02,  1.7733e-02, -2.3972e-02,  2.8380e-02, -5.0356e-03,\n",
       "         3.3850e-02, -2.4760e-02,  4.3374e-03, -1.6485e-02, -1.0450e-02,\n",
       "        -2.8341e-02, -2.2571e-02, -2.1294e-02, -2.6533e-02, -2.7138e-02,\n",
       "        -2.0946e-02, -3.4908e-02, -2.0826e-02,  1.5211e-02,  2.6404e-02,\n",
       "        -1.2574e-02,  3.6687e-02,  1.8213e-04, -9.4334e-03, -1.5012e-02,\n",
       "        -1.9071e-02,  5.4779e-03,  4.4809e-02, -3.0874e-03, -1.3790e-03,\n",
       "         1.7485e-02,  1.1420e-02,  2.9561e-02,  1.1621e-02, -3.4348e-03,\n",
       "        -2.0989e-02,  9.8980e-03, -5.3693e-03,  2.6606e-02,  3.3313e-02,\n",
       "         2.7127e-02, -2.1401e-02, -1.9578e-02,  1.9718e-02,  5.0845e-03,\n",
       "         1.7148e-02, -2.8259e-02,  2.9030e-02, -2.8951e-02,  2.6423e-02,\n",
       "        -1.7914e-02,  4.0643e-02,  6.1854e-02,  6.6175e-02,  3.8729e-02,\n",
       "         6.1134e-02,  8.1488e-02,  1.2576e-01,  7.5407e-02,  6.1255e-02,\n",
       "         8.9983e-02,  3.8441e-02,  2.9049e-02, -5.1292e-03,  2.0359e-02,\n",
       "        -1.1454e-02, -2.9106e-02, -7.9200e-03,  2.5389e-02, -1.5767e-02,\n",
       "        -3.3170e-02, -1.8811e-02, -7.5758e-03, -1.9156e-02,  2.7563e-02,\n",
       "        -1.5685e-02,  2.5324e-03,  4.7700e-02,  1.1461e-02,  4.0845e-02,\n",
       "         9.0134e-02,  1.1514e-01,  1.1503e-01,  1.9884e-01,  1.7506e-01,\n",
       "         1.7148e-01,  1.9920e-01,  1.4172e-01,  1.4465e-01,  9.9551e-02,\n",
       "         4.9254e-02,  1.3044e-02, -1.3709e-03, -8.3454e-03, -2.7711e-02,\n",
       "         1.3518e-02, -3.2279e-02, -3.5391e-02, -2.7611e-02,  2.5671e-02,\n",
       "         3.2533e-02,  3.5916e-02,  5.4471e-03, -2.5238e-02,  9.0869e-03,\n",
       "         3.5679e-02,  5.7129e-02,  9.7250e-02,  1.5300e-01,  1.5252e-01,\n",
       "         2.3016e-01,  2.4447e-01,  2.6285e-01,  2.5742e-01,  2.3147e-01,\n",
       "         2.4708e-01,  2.0812e-01,  1.3212e-01,  1.2016e-01,  7.0421e-02,\n",
       "         1.7532e-02,  4.2067e-02, -1.6189e-02,  3.8099e-02,  2.6228e-02,\n",
       "         3.5046e-02,  2.0804e-02, -9.9642e-04, -1.9228e-02,  9.6409e-04,\n",
       "         3.4221e-03,  2.4406e-02,  2.6205e-02,  1.7060e-02,  9.5644e-02,\n",
       "         1.0791e-01,  1.4041e-01,  2.2440e-01,  2.7381e-01,  2.9350e-01,\n",
       "         2.5835e-01,  2.6570e-01,  3.1112e-01,  3.0067e-01,  2.6668e-01,\n",
       "         2.2288e-01,  1.3208e-01,  1.1600e-01,  9.3271e-02,  8.8072e-03,\n",
       "         2.8243e-02,  2.8530e-02,  9.6180e-03,  2.8271e-02, -2.1591e-02,\n",
       "         2.1996e-02, -8.3542e-03, -2.3423e-02, -3.4193e-03, -1.0945e-02,\n",
       "         5.9785e-02,  8.0318e-02,  7.6573e-02,  1.1998e-01,  1.6750e-01,\n",
       "         2.5046e-01,  2.6601e-01,  2.3183e-01,  2.2208e-01,  2.3197e-01,\n",
       "         2.3781e-01,  2.5483e-01,  2.1242e-01,  2.4040e-01,  1.3872e-01,\n",
       "         1.2508e-01,  8.4037e-02,  5.7034e-02,  1.6032e-02, -1.1672e-02,\n",
       "         5.2133e-03, -2.7686e-02,  9.0459e-03, -6.1375e-03,  2.3039e-02,\n",
       "         1.0288e-02,  4.4662e-03,  3.8531e-02,  1.8345e-02,  6.6451e-02,\n",
       "         9.7383e-02,  1.3316e-01,  2.0189e-01,  2.4862e-01,  2.2844e-01,\n",
       "         2.2975e-01,  1.5369e-01,  1.7140e-01,  1.9145e-01,  2.2817e-01,\n",
       "         2.4802e-01,  2.2606e-01,  1.6345e-01,  1.0480e-01,  1.0173e-01,\n",
       "         4.9055e-03, -8.9843e-03,  3.0932e-02,  2.0938e-02,  8.8176e-03,\n",
       "         1.1681e-02, -4.7956e-03,  2.4792e-02,  1.9055e-02, -1.4017e-02,\n",
       "         4.6792e-02,  4.8835e-02,  6.3560e-02,  1.4749e-01,  1.7201e-01,\n",
       "         1.8438e-01,  2.0584e-01,  1.8604e-01,  1.7557e-01,  1.0688e-01,\n",
       "         1.6242e-01,  1.7883e-01,  2.0172e-01,  2.2872e-01,  1.7429e-01,\n",
       "         1.3458e-01,  1.3336e-01,  4.8138e-02,  4.4585e-02, -6.5535e-03,\n",
       "        -1.3606e-03,  9.2352e-03,  2.9716e-02, -2.0878e-02, -1.0357e-02,\n",
       "        -3.2496e-03, -1.4686e-02,  3.5649e-02,  5.0879e-03,  2.2987e-02,\n",
       "         1.0155e-01,  1.4782e-01,  1.7915e-01,  2.4666e-01,  2.2298e-01,\n",
       "         1.3927e-01,  1.4164e-01,  1.4339e-01,  1.7116e-01,  2.1423e-01,\n",
       "         2.2922e-01,  2.5246e-01,  1.7834e-01,  1.2574e-01,  1.0084e-01,\n",
       "         3.6485e-02,  3.2690e-02, -8.2795e-03,  3.4181e-02, -1.1062e-02,\n",
       "         1.6114e-02, -2.7419e-02, -1.2033e-02, -1.8510e-02, -2.6403e-02,\n",
       "         2.9225e-02,  3.3391e-02,  5.2312e-02,  9.8856e-02,  1.5458e-01,\n",
       "         2.3044e-01,  1.9960e-01,  2.2705e-01,  2.0142e-01,  1.3366e-01,\n",
       "         2.0040e-01,  2.2422e-01,  2.1054e-01,  2.2674e-01,  2.5544e-01,\n",
       "         1.5308e-01,  1.1568e-01,  8.9197e-02,  3.8089e-02,  7.0228e-03,\n",
       "        -1.8885e-03,  2.1083e-02,  1.4894e-02,  1.4905e-02, -2.9409e-02,\n",
       "         1.1352e-02, -3.2029e-02,  3.2620e-02, -1.2830e-02,  2.3567e-02,\n",
       "         2.8290e-02,  1.2819e-01,  1.7121e-01,  2.4165e-01,  2.0307e-01,\n",
       "         2.0858e-01,  2.3412e-01,  1.9025e-01,  2.4330e-01,  2.6503e-01,\n",
       "         2.9404e-01,  2.8451e-01,  2.6709e-01,  1.8190e-01,  9.1033e-02,\n",
       "         5.6288e-02,  7.4092e-02,  4.4070e-03,  1.2407e-03, -1.0794e-02,\n",
       "         9.4066e-03, -3.3682e-02, -3.2979e-03, -1.4312e-02, -1.9413e-02,\n",
       "         3.5255e-02,  2.4938e-02,  1.7324e-02,  6.6222e-02,  1.1894e-01,\n",
       "         1.9130e-01,  2.2154e-01,  2.1492e-01,  2.5693e-01,  2.3142e-01,\n",
       "         2.2287e-01,  2.9948e-01,  3.2014e-01,  3.0344e-01,  2.7551e-01,\n",
       "         2.7201e-01,  1.4470e-01,  1.3153e-01,  1.2342e-01,  4.8963e-02,\n",
       "         4.0702e-02, -3.2417e-03,  1.1478e-02, -2.6078e-02, -1.7275e-02,\n",
       "        -1.5090e-02,  1.6576e-02,  3.0728e-02,  3.1202e-02, -2.2845e-02,\n",
       "         6.0232e-02,  6.7800e-02,  1.4948e-01,  1.8380e-01,  1.7983e-01,\n",
       "         2.0039e-01,  2.5245e-01,  2.2770e-01,  2.7806e-01,  3.0634e-01,\n",
       "         3.1751e-01,  2.7548e-01,  2.8196e-01,  2.5708e-01,  1.5516e-01,\n",
       "         1.6769e-01,  1.1269e-01,  9.6054e-02, -6.2645e-04, -4.1922e-03,\n",
       "         2.2091e-02, -2.4148e-03, -2.4010e-02,  2.4830e-02, -1.3906e-02,\n",
       "        -2.8750e-02,  8.0576e-03,  3.3594e-02,  2.2575e-02,  7.6231e-02,\n",
       "         1.4375e-01,  1.7522e-01,  1.5707e-01,  2.1285e-01,  1.9806e-01,\n",
       "         1.9096e-01,  2.6806e-01,  2.6160e-01,  2.5505e-01,  2.2815e-01,\n",
       "         2.3284e-01,  2.0095e-01,  1.9054e-01,  1.1142e-01,  1.0536e-01,\n",
       "         4.8164e-02,  6.6405e-02,  5.8272e-03, -8.8552e-03,  1.8640e-03,\n",
       "        -2.6529e-02,  2.0261e-02, -2.5340e-02,  6.2511e-03,  3.6480e-02,\n",
       "         2.7083e-04,  3.8031e-02,  6.2002e-02,  9.8453e-02,  1.4240e-01,\n",
       "         1.5583e-01,  1.3888e-01,  1.8268e-01,  1.7472e-01,  2.2030e-01,\n",
       "         1.9410e-01,  2.2214e-01,  2.1464e-01,  2.3722e-01,  2.3499e-01,\n",
       "         1.4907e-01,  1.3170e-01,  9.8939e-02,  2.8024e-02,  1.0060e-02,\n",
       "         4.4053e-02,  1.3242e-02,  2.0152e-02,  4.1930e-03,  8.2026e-03,\n",
       "        -2.5501e-02,  1.0072e-02, -5.7733e-03, -5.8757e-03,  4.5537e-02,\n",
       "         1.0578e-01,  8.3357e-02,  1.5174e-01,  1.3844e-01,  1.8932e-01,\n",
       "         1.4799e-01,  1.9454e-01,  1.5700e-01,  2.1375e-01,  1.9135e-01,\n",
       "         2.3593e-01,  2.3245e-01,  2.2383e-01,  1.9449e-01,  1.5704e-01,\n",
       "         1.1850e-01,  1.9951e-02,  9.2735e-03,  3.4823e-03, -2.4734e-02,\n",
       "         2.8402e-02,  2.4162e-02,  1.6020e-03, -1.3351e-02, -9.6725e-03,\n",
       "         6.6288e-03,  5.4220e-02,  5.3146e-02,  1.1513e-01,  8.8517e-02,\n",
       "         1.2314e-01,  1.5451e-01,  1.9890e-01,  1.8098e-01,  1.7497e-01,\n",
       "         2.1094e-01,  2.4020e-01,  2.1581e-01,  2.7953e-01,  2.7451e-01,\n",
       "         2.4300e-01,  1.3252e-01,  1.4557e-01,  6.0540e-02,  1.7945e-02,\n",
       "         1.1317e-03,  1.6313e-03, -3.0987e-02,  2.4578e-02, -2.5521e-02,\n",
       "        -3.4756e-02,  2.2179e-02, -1.8346e-02, -2.3189e-02,  3.4556e-02,\n",
       "         3.5975e-02,  5.8663e-02,  1.1252e-01,  1.2335e-01,  1.9024e-01,\n",
       "         1.9419e-01,  2.1911e-01,  2.0110e-01,  2.5562e-01,  2.2402e-01,\n",
       "         2.8690e-01,  2.3407e-01,  2.0995e-01,  2.1244e-01,  1.1434e-01,\n",
       "         8.6321e-02,  6.3378e-02,  3.0940e-02, -8.7453e-03, -4.4276e-03,\n",
       "        -7.5191e-03, -2.2187e-04,  2.0041e-02,  2.1519e-02, -2.9158e-02,\n",
       "         2.0385e-02, -1.0647e-02,  6.0172e-03,  2.4720e-02,  9.4810e-02,\n",
       "         1.0444e-01,  1.4479e-01,  1.8223e-01,  2.4502e-01,  2.0255e-01,\n",
       "         2.7763e-01,  2.6654e-01,  2.6189e-01,  2.9559e-01,  2.6715e-01,\n",
       "         2.1967e-01,  1.3636e-01,  7.2077e-02,  3.1220e-02,  8.4719e-03,\n",
       "         4.1087e-02,  1.6623e-02, -1.1221e-02, -1.7789e-02,  9.4751e-03,\n",
       "         1.8360e-03, -6.6257e-03,  1.9612e-03, -2.2350e-02,  2.4718e-02,\n",
       "         2.5182e-02,  5.9382e-02,  7.8856e-02,  9.0579e-02,  1.0520e-01,\n",
       "         1.3648e-01,  1.8201e-01,  2.5428e-01,  2.2304e-01,  2.8743e-01,\n",
       "         2.7544e-01,  2.5703e-01,  2.1410e-01,  1.3874e-01,  1.2044e-01,\n",
       "         9.3378e-02,  1.9828e-02, -6.1082e-03,  3.0858e-02,  3.9098e-02,\n",
       "        -1.3526e-02, -7.0127e-04,  1.6709e-02, -2.4866e-02, -3.3199e-02,\n",
       "        -2.8145e-02,  1.6082e-02,  1.6893e-02,  2.6979e-02, -2.3784e-02,\n",
       "         5.5999e-02,  6.5747e-02,  9.3843e-02,  1.5396e-01,  1.7477e-01,\n",
       "         2.1577e-01,  2.2685e-01,  2.1812e-01,  1.8490e-01,  1.3291e-01,\n",
       "         1.3268e-01,  1.0876e-01,  8.9720e-02,  2.1060e-02,  5.2318e-03,\n",
       "         4.3518e-02,  5.4727e-03, -1.2776e-02, -3.3833e-02, -2.1199e-02,\n",
       "         2.8649e-03,  1.5378e-03, -2.4573e-02,  2.8207e-02,  1.1156e-02,\n",
       "        -2.9890e-02, -1.9463e-02, -1.2154e-02, -2.0204e-02,  7.9792e-03,\n",
       "         2.5087e-02,  8.5300e-02,  6.2183e-02,  1.0905e-01,  1.2586e-01,\n",
       "         8.5511e-02,  1.1186e-01,  6.3445e-02,  3.1782e-02,  6.6133e-02,\n",
       "         2.0600e-02, -3.3661e-03,  9.8258e-03, -1.5136e-02, -9.2055e-03,\n",
       "         9.2126e-03,  2.4190e-02, -1.7414e-02,  1.5555e-02, -1.1396e-02,\n",
       "        -3.1613e-02, -1.1567e-02, -3.4639e-02, -5.4074e-03, -2.1130e-03,\n",
       "         4.5193e-03,  6.0121e-03, -2.3894e-02,  1.3965e-03,  3.1658e-02,\n",
       "         3.3735e-02,  7.6846e-03,  5.9888e-02,  2.8450e-02,  5.8875e-02,\n",
       "         1.9772e-02,  4.8016e-02,  2.7834e-02,  3.1843e-03,  2.1675e-02,\n",
       "        -2.9227e-02,  3.4888e-02,  3.4767e-02,  3.2577e-02, -2.4136e-03,\n",
       "         1.9130e-02,  3.2476e-02,  1.0022e-02, -6.6460e-03,  3.5028e-02,\n",
       "         3.0023e-02, -7.6657e-03, -3.2230e-02, -1.6855e-02, -1.1828e-02,\n",
       "        -1.3365e-02,  2.7812e-02,  3.5644e-02,  1.9249e-02,  5.1958e-03,\n",
       "        -1.9827e-02,  1.6696e-02,  3.2883e-05, -2.2110e-02,  1.4900e-02,\n",
       "        -2.3437e-02,  3.1971e-02, -3.0510e-02, -1.4400e-02,  2.9201e-02,\n",
       "         1.0671e-02, -3.5944e-03, -8.6520e-04, -2.0098e-02,  6.4415e-03,\n",
       "        -1.0938e-02, -2.6381e-02,  3.4175e-02, -7.1739e-03, -6.3375e-03,\n",
       "        -1.9939e-02,  6.8746e-03, -1.4076e-03, -1.9152e-02,  2.7200e-02,\n",
       "        -3.2523e-02,  1.9043e-02, -3.5003e-02, -1.2307e-02,  1.8325e-02,\n",
       "        -1.8471e-02,  2.4720e-02, -8.9853e-03, -9.8895e-03,  1.3704e-02,\n",
       "         3.1210e-02, -1.4320e-02,  1.1539e-02,  2.0838e-02, -1.9883e-02,\n",
       "         1.7724e-02, -6.6096e-03, -3.5705e-02, -1.3835e-02], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc1.weight[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "latter-packaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 487.3576,  563.0782,    0.0000,  788.0320, 1285.5168,  260.9083,\n",
       "           0.0000,    0.0000,  899.2999,  262.7325,  766.0019,  916.5306,\n",
       "         551.2318,  562.9424,    0.0000,  522.7163,    0.0000,  656.5974,\n",
       "         278.9159,    0.0000,  150.0539,  949.6662,    0.0000,   40.0514,\n",
       "         235.3155,  322.2774,  338.2378,    0.0000,    0.0000,    0.0000,\n",
       "           0.0000,  622.0282,  898.8696,  274.3130,  704.2454,  542.8204,\n",
       "           0.0000,  286.3525,  535.9585,    0.0000,  472.4174,    0.0000,\n",
       "           0.0000,    0.0000,  935.0920,  518.3685,  858.5781,  425.1231,\n",
       "         436.8321,  267.9036,    0.0000,    0.0000,  151.2569,    0.0000,\n",
       "         280.9054,  469.7323,  854.7551,  328.7216,  236.1176,  259.5135,\n",
       "         531.8068,  663.5306,  566.5142,  292.0876,  760.9315,    0.0000,\n",
       "         430.0152,    0.0000,  224.1848,  204.6381,  123.6201,  201.6194,\n",
       "         706.4742,  550.0683,  927.8320,  613.0057,  740.3270,    0.0000,\n",
       "         404.3229,  769.3582,  624.0850,    0.0000,  364.5620,    0.0000,\n",
       "         174.9790,  261.5737,  470.0835,    0.0000,  846.4037,  254.2957,\n",
       "           0.0000,    0.0000,  169.9790,  133.4698,  866.0516,  452.4504,\n",
       "         957.0208,  284.0109,  922.8854,  617.7474], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "hearing-kentucky",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 10])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "religious-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23443.8906,  11664.0293, -30977.0000,  18269.4180, -30130.8613,\n",
       "        -35893.1562,  34281.3125,  15815.0801,  24913.7266,  15030.6855],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "known-jungle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "tamil-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0003, -0.0016,  0.0095, -0.0396, -0.0370,  0.0073, -0.0448,  0.0378,\n",
       "         0.0476,  0.0221], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc2.weight[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cellular-socket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100., 100., 100., 100., 100., 100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature.fc2.weight.grad[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "considered-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature.fc2.weight.grad[0][0:10] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bottom-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_labels = torch.randint(0, 10, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sized-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "tropical-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_loss = 2*model.loss_fn(outputs, labels) - model.loss_fn(outputs, rand_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "encouraging-terrace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3295, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "southwest-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "binary-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_R.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "micro-renewal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0843, -0.9679, -0.9038, -1.0378, -1.0565, -0.9441, -1.0367, -0.9117,\n",
       "        -1.0202, -1.0202], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rand_classifier.weight[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "overall-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rand_classifier.weight.grad[0][0:10] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-humanity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-swing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tamil-heather",
   "metadata": {},
   "source": [
    "## analyze metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unusual-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "timely-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = h5py.File('exp/mnist_mnist_c/translate/_lenet_trainfirst_iterT9999_iterR0_iterRsb1_gam0.0_alf1.0_lrT0.1_lrR0.1_ep15_inittypeprev_fromfresh/perf_metrics.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sunrise-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train_loss', 'train_acc', 'val_loss', 'val_acc', 'test_loss',\n",
       "       'test_acc'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file.attrs['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "joint-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_loss = np.array(input_file['train_loss'])\n",
    "orig_val_loss = np.array(input_file['val_loss'])\n",
    "orig_test_loss = np.array(input_file['test_loss'])\n",
    "\n",
    "orig_train_acc = np.array(input_file['train_acc'])\n",
    "orig_val_acc = np.array(input_file['val_acc'])\n",
    "orig_test_acc = np.array(input_file['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "resident-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "korean-alabama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc45df06550>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(orig_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "facial-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2932558 , 0.5998181 , 0.55874026, 0.46569228, 0.45049158,\n",
       "       0.4255676 , 0.3847248 , 0.40330774, 0.39204633, 0.37022448,\n",
       "       0.3361154 , 0.3330903 , 0.32348287, 0.33008718, 0.30198124,\n",
       "       0.31034064, 0.32422924, 0.28173184, 0.29986274, 0.31335452,\n",
       "       0.2884902 , 0.3192304 , 0.26953194, 0.27360663, 0.28837383,\n",
       "       0.28256062, 0.27020952, 0.27938017, 0.27500474, 0.28482935,\n",
       "       0.2830521 , 0.25364268, 0.26431122, 0.26609266, 0.2623072 ,\n",
       "       0.24247855, 0.31599626, 0.266008  , 0.25241718, 0.25159404,\n",
       "       0.25465158, 0.2381976 , 0.26936   , 0.2949132 , 0.22524305,\n",
       "       0.27760792, 0.27456635, 0.26015154, 0.2783044 , 0.28589192,\n",
       "       0.25260788, 0.29126576, 0.2745214 , 0.31174105, 0.25607827,\n",
       "       0.32937205, 0.25082582, 0.24957164, 0.27887645, 0.2519828 ,\n",
       "       0.31203738, 0.3072598 , 0.28016475, 0.26036236, 0.25688252,\n",
       "       0.2830533 , 0.2524959 , 0.2644796 , 0.28432667, 0.28638327,\n",
       "       0.26213753, 0.26806572, 0.3232631 , 0.33061832, 0.29632422,\n",
       "       0.2820536 , 0.26265258, 0.30279034, 0.26173815, 0.3282029 ,\n",
       "       0.2864587 , 0.42904487, 0.2815973 , 0.29689458, 0.29467326,\n",
       "       0.30724263, 0.28094035, 0.3637059 , 0.3184552 , 0.33295807,\n",
       "       0.33358172, 0.31642494, 0.33409432, 0.34634957, 0.33794174,\n",
       "       0.32045445, 0.29280528, 0.3427218 , 0.31327337, 0.3360047 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "federal-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0221, -0.0360,  0.0109,  ..., -0.0212,  0.0420,  0.0539],\n",
      "        [ 0.0665, -0.0150,  0.0654,  ..., -0.0893, -0.0182, -0.0902],\n",
      "        [-0.0695, -0.0110, -0.0557,  ...,  0.0798, -0.0765,  0.0625],\n",
      "        ...,\n",
      "        [ 0.0803, -0.0046,  0.0478,  ...,  0.0588, -0.0892,  0.0326],\n",
      "        [ 0.0360, -0.0322,  0.0301,  ..., -0.0137,  0.0948, -0.0082],\n",
      "        [-0.0652,  0.0347, -0.0570,  ...,  0.0282,  0.0767,  0.0577]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0222,  0.0851,  0.0923, -0.0091,  0.0839, -0.0649, -0.0209,  0.0281,\n",
      "        -0.0481,  0.0778,  0.0672, -0.0319, -0.0773, -0.0891, -0.0533,  0.0841,\n",
      "        -0.0069, -0.0616,  0.0576,  0.0581,  0.0954,  0.0769,  0.0092,  0.0759,\n",
      "        -0.0918,  0.0579,  0.0800,  0.0437,  0.0476, -0.0583, -0.0705, -0.0714,\n",
      "         0.0169, -0.0576, -0.0534,  0.0145, -0.0887,  0.0359,  0.0824,  0.0519,\n",
      "        -0.0280, -0.0057,  0.0309,  0.0348, -0.0592,  0.0321, -0.0852, -0.0133,\n",
      "         0.0979, -0.0963, -0.0224, -0.0611,  0.0155,  0.0056,  0.0318,  0.0551,\n",
      "         0.0506,  0.0152, -0.0219,  0.0947,  0.0988, -0.0040,  0.0595, -0.0066],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.6575e-02, -1.0808e-01,  4.6167e-02,  1.0485e-01, -3.0044e-02,\n",
      "          6.7750e-02,  9.5907e-02, -6.8777e-02,  6.6288e-03,  4.9151e-02,\n",
      "          1.0725e-01,  4.2803e-02,  7.0403e-02, -6.5237e-02, -6.1293e-02,\n",
      "         -8.7756e-02,  1.5457e-02,  3.4192e-02, -2.5773e-02,  4.4097e-02,\n",
      "         -1.1112e-01, -3.7409e-02, -1.0252e-01, -7.5074e-02, -8.3541e-02,\n",
      "          4.7447e-02, -7.0920e-02, -3.6282e-02, -1.1584e-02, -9.3290e-03,\n",
      "         -4.4752e-02,  8.3600e-02, -2.5280e-03, -1.1346e-01,  1.1906e-01,\n",
      "         -9.4776e-02,  6.9228e-02,  1.1617e-02,  6.6392e-02, -9.7941e-02,\n",
      "         -2.0744e-03,  8.9454e-02, -1.1041e-01,  4.6918e-02,  3.5047e-02,\n",
      "          6.8520e-02,  8.8885e-02, -4.5333e-03, -2.4838e-02,  4.3833e-02,\n",
      "         -4.7403e-02,  1.8122e-02, -6.8079e-02, -5.6318e-02,  6.9004e-02,\n",
      "          6.0737e-02,  5.5348e-02,  6.7002e-02, -9.2477e-02, -1.1033e-01,\n",
      "          3.1811e-02, -7.2289e-02, -4.2775e-02,  5.0864e-02],\n",
      "        [-1.1948e-01,  6.4471e-03, -6.3492e-02,  1.1945e-01,  1.8460e-02,\n",
      "          8.8857e-02, -6.1444e-02, -8.6840e-02, -4.8071e-02, -6.5189e-03,\n",
      "          7.3576e-02, -8.9497e-02,  1.1912e-02,  1.0299e-01,  3.4460e-02,\n",
      "          5.0041e-02,  9.8248e-02, -6.2688e-02,  3.4354e-03,  1.1167e-01,\n",
      "          1.0391e-01,  3.3293e-02,  9.6463e-02,  4.4208e-02,  3.9720e-02,\n",
      "          1.1367e-01,  6.1524e-02,  5.5389e-02,  1.4505e-02,  9.9189e-02,\n",
      "          7.1353e-03, -8.8291e-03,  1.2630e-02, -5.1931e-05,  4.1381e-03,\n",
      "         -1.1198e-01,  8.0621e-02,  1.2100e-01,  1.1428e-01,  8.0099e-02,\n",
      "         -1.8877e-02, -5.0360e-03,  5.0554e-02,  1.1514e-01,  1.1816e-01,\n",
      "         -5.0568e-02,  2.5400e-02, -1.5021e-02,  9.5656e-02,  1.1912e-01,\n",
      "         -5.0349e-02,  4.2794e-02, -7.4514e-02,  1.7114e-02, -4.2670e-02,\n",
      "         -9.8849e-02,  9.9601e-03, -5.2349e-02,  8.7421e-02,  1.1493e-02,\n",
      "         -2.7312e-02, -6.1639e-02,  9.1440e-02,  5.7263e-02],\n",
      "        [-8.4165e-02, -1.1078e-01, -2.4127e-02,  7.3987e-02,  8.5848e-02,\n",
      "          1.1301e-01, -4.4960e-02, -1.1557e-02,  1.7723e-02, -9.2291e-03,\n",
      "          1.0635e-02, -6.4254e-02, -5.3644e-02,  4.6602e-02, -1.0580e-01,\n",
      "          1.0097e-01,  4.2717e-03, -1.2272e-01, -2.4195e-02, -4.9948e-02,\n",
      "         -4.3465e-02,  1.0412e-01,  1.0372e-01,  7.4582e-02,  2.7625e-02,\n",
      "          1.0715e-01,  9.0963e-02,  6.7064e-02,  5.9771e-02,  1.2335e-01,\n",
      "          6.2224e-02,  5.0889e-02,  9.3285e-02, -5.7073e-02, -7.0932e-02,\n",
      "          1.1497e-01,  8.9755e-02,  1.2435e-01, -5.9816e-02, -8.8776e-02,\n",
      "          8.0313e-02, -2.4592e-02,  2.7828e-02, -4.4836e-02,  1.2291e-01,\n",
      "          1.0714e-01, -7.2275e-02,  1.2939e-02, -3.1522e-02,  6.9489e-02,\n",
      "         -1.4506e-02, -3.0159e-02, -1.0149e-02, -7.1991e-02,  6.3593e-02,\n",
      "         -9.6701e-02,  1.1115e-01, -8.5808e-02, -8.0561e-02, -1.1861e-01,\n",
      "         -1.0384e-01, -1.1562e-02, -3.7661e-02,  1.0981e-01],\n",
      "        [-1.5383e-02, -6.0077e-02,  6.2827e-02, -1.1918e-01,  9.5808e-02,\n",
      "         -1.0628e-01, -1.2620e-02, -7.4846e-02, -1.1238e-02, -9.6749e-02,\n",
      "          3.2847e-02, -2.3793e-02, -1.0687e-01,  1.4182e-02,  1.1673e-01,\n",
      "         -8.1835e-02, -5.1771e-02,  5.3018e-02,  1.1383e-01,  5.7844e-03,\n",
      "          2.8116e-02,  6.3615e-02,  8.6333e-02,  1.0175e-01, -5.2125e-02,\n",
      "          8.2585e-02,  8.9023e-04,  4.4201e-02, -4.8408e-03, -9.4088e-02,\n",
      "         -9.6292e-02,  4.9564e-02, -1.7625e-02, -3.2290e-02,  2.3012e-02,\n",
      "          9.6548e-02,  1.2050e-01,  1.0140e-01,  3.4147e-02, -5.0188e-02,\n",
      "          1.1150e-01,  2.6993e-02,  2.3323e-02, -1.0738e-01,  8.9899e-02,\n",
      "         -3.5148e-02, -1.1609e-01,  6.0460e-02,  1.1613e-01, -9.5348e-02,\n",
      "          3.0417e-02,  9.4739e-03,  4.2226e-02,  2.6851e-02,  7.3609e-02,\n",
      "          7.2526e-02, -1.1379e-01, -1.1908e-01,  6.5237e-02, -6.9752e-02,\n",
      "         -1.0313e-01, -1.2428e-01,  1.2654e-02, -4.3012e-02],\n",
      "        [-1.1060e-01, -5.0686e-02,  5.8844e-02,  5.2066e-02, -3.8401e-02,\n",
      "         -1.1786e-01,  9.4389e-02,  2.1601e-02, -1.0866e-01, -4.3821e-02,\n",
      "         -2.0057e-02, -9.2091e-02,  8.7735e-02, -5.2898e-02, -1.1147e-01,\n",
      "         -3.7936e-02,  1.1843e-01,  1.0011e-01,  6.8120e-02, -1.0521e-01,\n",
      "         -8.8204e-02, -4.0741e-02,  3.1808e-02, -2.4897e-02, -1.2047e-01,\n",
      "          1.4150e-02,  8.5818e-02, -8.0516e-02,  8.8159e-02,  8.3913e-02,\n",
      "         -9.4422e-02, -9.0994e-02, -2.9480e-02, -1.2484e-01,  7.8499e-02,\n",
      "          7.0725e-02,  1.0394e-01,  7.7222e-02, -7.6432e-02,  1.0958e-01,\n",
      "         -6.5804e-04,  2.7395e-02, -9.0804e-02, -1.0209e-01, -3.3178e-02,\n",
      "          7.1952e-02, -6.6066e-03,  1.2254e-01, -3.2641e-02, -9.2913e-02,\n",
      "         -3.0994e-02,  7.5836e-02, -2.0285e-02, -2.9888e-02,  3.3378e-02,\n",
      "          8.8497e-02, -6.9007e-03, -1.1423e-01, -1.0269e-01,  4.8789e-02,\n",
      "         -1.1113e-01, -4.2086e-03,  9.3572e-02,  6.1892e-02],\n",
      "        [-1.1672e-01, -7.5022e-02, -7.1532e-02,  6.3117e-02,  6.2968e-02,\n",
      "         -1.0232e-01,  1.1933e-01,  1.0419e-01,  3.4613e-02,  7.3627e-02,\n",
      "          5.6548e-02, -8.0318e-02, -8.8169e-02, -9.7636e-02, -4.3137e-02,\n",
      "         -3.4727e-02, -8.7409e-02, -1.1591e-01,  7.5787e-02,  6.9726e-02,\n",
      "         -7.2385e-02, -6.1877e-02,  4.0073e-02,  4.4572e-03,  9.9541e-03,\n",
      "         -9.1344e-02,  4.0480e-02, -3.5241e-02, -7.2469e-02, -2.6605e-02,\n",
      "          9.6135e-02, -6.5656e-02,  2.2220e-02,  4.3922e-02, -1.1571e-01,\n",
      "          1.1524e-01,  7.7184e-02,  1.5373e-02, -8.5523e-03, -1.0290e-01,\n",
      "          8.8076e-02,  1.1532e-02,  1.0051e-01, -1.0044e-01,  7.1008e-02,\n",
      "         -1.1092e-01,  1.1049e-01,  8.9371e-04,  6.9446e-02, -5.5711e-02,\n",
      "         -1.2431e-01,  1.0424e-01,  3.5708e-02, -7.5431e-02,  5.1651e-02,\n",
      "         -1.1870e-01, -8.1866e-02,  3.8427e-02, -8.5371e-02,  7.4192e-03,\n",
      "         -1.1672e-01,  6.5952e-02,  9.6255e-03,  7.4691e-02],\n",
      "        [ 9.0306e-02,  1.0764e-01,  5.3583e-02, -1.8727e-02, -1.9266e-02,\n",
      "          3.0659e-02,  3.1990e-02, -5.9507e-02, -1.0910e-01, -1.0027e-01,\n",
      "          1.2044e-01,  2.0924e-02, -1.0912e-01, -7.0726e-02, -9.9637e-02,\n",
      "         -3.1363e-02, -1.2451e-01, -1.6211e-02, -1.2218e-01, -1.3561e-02,\n",
      "          1.2056e-01, -6.9788e-02, -6.8101e-04, -5.7320e-02,  6.3618e-02,\n",
      "         -9.2986e-02,  2.7948e-02, -6.9975e-02, -9.4208e-02, -3.3570e-03,\n",
      "          8.1904e-02, -9.9990e-04, -3.1081e-02,  3.4172e-02,  1.2462e-01,\n",
      "          4.9244e-02,  6.9898e-02,  6.7886e-02,  9.3501e-02, -6.5014e-02,\n",
      "         -1.0155e-01,  5.8316e-03, -1.2283e-01, -7.7047e-02, -6.2988e-02,\n",
      "          8.8582e-02, -1.0955e-03,  1.2059e-01,  4.6639e-02,  3.3758e-02,\n",
      "          4.0184e-02, -1.1149e-01, -9.9887e-02, -3.0445e-02,  7.9572e-04,\n",
      "          1.3239e-02,  1.0717e-01, -7.1033e-02, -5.9157e-02,  9.9143e-02,\n",
      "          5.6433e-02,  3.7645e-02, -5.7630e-02, -4.5639e-02],\n",
      "        [ 1.7861e-02,  9.7753e-02, -6.9853e-02, -1.1093e-01, -1.2359e-02,\n",
      "         -1.0742e-01, -8.6719e-02, -1.1245e-01, -3.5756e-02, -3.9553e-02,\n",
      "          5.2202e-02,  3.2802e-02,  7.4693e-02, -5.0304e-02,  3.1501e-02,\n",
      "          1.5815e-02,  8.5956e-02, -8.2060e-02,  9.1052e-02, -1.2061e-01,\n",
      "          2.5521e-02,  9.3024e-02,  8.7305e-02,  6.8984e-03, -7.6672e-02,\n",
      "          3.1732e-02,  7.6643e-02, -8.9088e-02, -7.1529e-02,  6.1536e-02,\n",
      "         -1.1797e-01, -8.3982e-02,  6.3919e-02, -1.1150e-01,  9.5439e-02,\n",
      "          2.8763e-02, -1.1156e-01,  2.7775e-02,  1.2011e-01, -5.4394e-02,\n",
      "         -6.5370e-03,  1.0404e-01,  8.7194e-02,  3.0801e-02, -1.5908e-03,\n",
      "         -1.2289e-01,  1.1755e-01,  2.9020e-02, -6.2291e-02, -8.1925e-02,\n",
      "          8.0145e-02, -1.1661e-01,  4.5727e-02,  3.6067e-02, -6.0620e-02,\n",
      "         -5.4888e-02, -1.0206e-01,  3.7062e-02, -1.2101e-01,  8.9998e-02,\n",
      "         -4.0297e-02,  9.3349e-02,  9.3537e-02,  4.7313e-02],\n",
      "        [-1.0989e-01, -5.3193e-04, -3.5845e-02,  3.6406e-02, -3.8764e-02,\n",
      "         -4.6926e-02,  1.6343e-02, -7.7395e-02, -1.2332e-01,  2.2223e-02,\n",
      "          8.4255e-02, -2.3238e-02, -7.0092e-02, -5.9351e-02,  8.1196e-02,\n",
      "         -5.3003e-02,  1.1626e-02,  1.9048e-02,  3.0114e-02, -7.9192e-02,\n",
      "          3.7802e-02, -7.4874e-02,  5.6861e-02, -6.9898e-03,  5.4256e-02,\n",
      "          6.2185e-02,  1.9039e-02,  1.9571e-02, -2.8476e-02,  4.2168e-02,\n",
      "         -4.6004e-02,  7.6543e-02,  1.2367e-01, -4.5582e-02,  1.0827e-01,\n",
      "          9.1548e-02, -3.9329e-02, -1.0600e-01, -7.4555e-02, -9.3825e-02,\n",
      "          2.2446e-02,  1.1572e-02,  8.8195e-02, -3.3341e-02,  8.6269e-02,\n",
      "          1.6510e-02, -1.3533e-02,  1.0030e-01,  4.0669e-02,  1.1274e-01,\n",
      "         -8.0441e-02,  1.2383e-01, -6.8987e-02,  6.2171e-02,  5.6871e-02,\n",
      "          1.1893e-01, -1.2474e-01, -6.3474e-02, -7.8439e-02, -8.8227e-02,\n",
      "          7.3084e-02,  1.1437e-01, -4.0027e-02,  1.6350e-02],\n",
      "        [ 1.0063e-01,  8.1345e-02, -1.0599e-01, -7.3764e-02, -7.7358e-02,\n",
      "          6.3507e-02, -1.1153e-01, -1.0871e-01, -1.6447e-02,  6.6172e-02,\n",
      "         -8.2711e-02,  2.6162e-02,  9.3967e-02, -1.1600e-01, -6.2932e-02,\n",
      "         -2.4336e-02, -1.2135e-01, -1.1809e-01, -6.0606e-02,  2.1781e-02,\n",
      "          5.3734e-02, -5.1631e-02, -7.2325e-02,  1.9122e-02, -8.2675e-02,\n",
      "         -2.9403e-02, -1.0040e-01, -1.2249e-01,  2.5951e-02,  8.1800e-03,\n",
      "         -2.0597e-02,  2.7047e-02, -8.8632e-02, -1.2403e-01, -7.1038e-02,\n",
      "          2.1594e-03, -5.1528e-02,  4.1748e-03, -1.8347e-02, -4.5315e-03,\n",
      "          7.4442e-03, -1.0567e-01,  1.0100e-02,  9.4099e-03,  1.5426e-02,\n",
      "          1.2187e-01,  5.6186e-03,  4.1094e-02,  5.3347e-02, -1.3586e-02,\n",
      "         -6.2723e-02,  3.9311e-02, -1.1032e-01, -1.0072e-01, -9.9938e-02,\n",
      "          9.3261e-02, -5.4935e-02, -4.0135e-02,  6.7297e-02,  2.4751e-02,\n",
      "          1.1774e-01, -6.1260e-02, -6.0969e-02,  1.0739e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0482, -0.0455, -0.0785, -0.0174,  0.1220,  0.0614,  0.0053,  0.0143,\n",
      "        -0.0911, -0.1153], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.rand_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-triumph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-collection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-mistake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-water",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-voice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-leader",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-buffalo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-transport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-bishop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
